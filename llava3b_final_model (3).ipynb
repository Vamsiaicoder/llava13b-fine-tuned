{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ex7YQw2aPxyF",
        "outputId": "9b78cc25-1703-4bc4-a281-2d7782fc430d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.5.1+cu124\n",
            "CUDA available: True\n",
            "CUDA device count: 1\n",
            "Current CUDA device: 0\n",
            "GPU name: NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"CUDA device count:\", torch.cuda.device_count())\n",
        "print(\"Current CUDA device:\", torch.cuda.current_device() if torch.cuda.is_available() else \"No CUDA device\")\n",
        "print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "AiHAXztWFvjL"
      },
      "outputs": [],
      "source": [
        "# !pip install -U bitsandbytes\n",
        "# !pip install -U accelerate transformers\n",
        "# !pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjkLMdYeRW6Y"
      },
      "outputs": [],
      "source": [
        "# import zipfile\n",
        "# import os\n",
        "\n",
        "# zip_path = '/content/Compliance_model.zip'\n",
        "# extract_path = \"\"\n",
        "\n",
        "# with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "#     zip_ref.extractall(extract_path)\n",
        "\n",
        "# print(\"Extraction complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPCqXHfLuNMU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9OGrdwvuOg4"
      },
      "source": [
        "###LLava 13 b model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plUYMLqfuNPV"
      },
      "outputs": [],
      "source": [
        "hf_token ='huggin_face_token'\n",
        "#before preparing the data as per model cofiguratios we will read document and take note of how the data preparations should be done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 767,
          "referenced_widgets": [
            "d51451fbe0914d419149dfd01298721c",
            "ff3ce6ad11104d3b9461b4cfac7f86d9",
            "cc268da4acad43f6b4a02535be513add",
            "5f4f5122ace5499f81eae788ba274d5d",
            "b6f402ff54d640ccbb9672e66d9157c9",
            "1d7b384d43784b67b42b2f9bb91b0180",
            "57867682b2de4d3c93ed009fd6bedb20",
            "c19bd2431f9a451fb60e9d37657928c6",
            "38a3a87ec35747c4b724940d237b7e64",
            "4518b7a3d6c547dfa528a185699fa7f7",
            "1abef045e39b446eae3d8d680b8b992d",
            "6321dbf9e1f241289574079c489975f6",
            "a67f21f5806d4b69925acb8f7e70a95a",
            "60d6da517e0b42938ba953e630d7362c",
            "5ae22023fe3640579e37146171d9a36d",
            "5602428751f34d5ea08d167c05a7b360",
            "d6618ebb686b43ecaf8e62ec27ca928e",
            "0995c0c4a711465d8dcbaaa62642301a",
            "27dc915d53764745ae8550691a231b9c",
            "8f4ee815d68e407f9c68e5b76fc6c5c5",
            "06cd2cef14574ba191902916806d1595",
            "39b88f4102ba4222a3afd572629746e0",
            "698ca72845e247269a3e28f2d37fcdb6",
            "8e9b56e3dfeb4b78a78e9a2d47b4fb05",
            "b9b752b3e1284cdea140a891ca9adbe7",
            "880572259c5844b697a5018072d17fcc",
            "9b5a9169572e4a92beb20e2042f1f143",
            "943a02fa93e34fefab30f859dcef7e7d",
            "689ffbb060da46369dac1f7445738f04",
            "20cf22134a8f482a9e72fc5f4c02826b",
            "ed997990922a4e298203825ed094242b",
            "7f6e1b4b30424f9dbdab6d1e3693fdb3",
            "098653a393934263b5b390675488f0bf",
            "3ce27b9d396b44108550c798c1fe443e",
            "3f34d1e53ca24de0b3588220a1c494cb",
            "7775eea944c4417684d63e307b1620ab",
            "6b5ddce7563f41f0ae45c24b6c23d4e5",
            "6a0ac49b9f2e41a6ab7cdcc479df675b",
            "d6966f921e1a43a7b304368836e6a8b8",
            "30a441bee81f4073936fe007eac09590",
            "9e11ab1eab314fe0bc122f78bf68ff6d",
            "b9744ff5ef994155ad950bff292e6706",
            "68b05e1dfd37431cb02af1681d16e4fd",
            "0ab20ec7939342dea7ab3a9bd06a9065",
            "607e1cccc37844b98af4de125c2b14a9",
            "38fa4e98368948d793adb8f414c128de",
            "747f5b540cab4706b7e1ad22fc78fafd",
            "1f50f4c52e5543b7bb4b194e01abee9a",
            "02335270363a449b9ece2551b55d20ce",
            "a1c58f776af54a3d898130e9cd822747",
            "618151761499454490b916a0548b2efd",
            "39d8085120634454b8ac243c557df520",
            "19078e6a65a2444a9b7d1f6bd3e65275",
            "3b9643096e24497190978176855df1e8",
            "8a281f9cc30646839f7c5d9a9fd834e9",
            "a53089b534e244c2aa2af895557eea12",
            "dd40dd4193d249698c0dd69276cbee4c",
            "deb23f0814e146388128ecea0b5beda2",
            "3b9d5757879442829bdbf8c0c56f1f7b",
            "28359b6502c842b88a2c971195f4987e",
            "8279bbe0adab413b95fe5541259afdc3",
            "0161fe9b467641b79bc687b6d69c2712",
            "263cded35c0f40889b72fe2132e464b0",
            "70ba70053bb54a54912dfc38c29d2ee6",
            "fd2fa6d29b2940e2b370520b24821f6f",
            "18460aec28144a05a70456ba583218c1",
            "97c2a58c1e0744e19b9b15d080f1d4e0",
            "f02cb2da4d07442babb25ac66930c200",
            "cc86e318f3bb40a8a34164dbaf88aeef",
            "95fad93ab0a0484a9c223664f2deb1a8",
            "4ebfc388ebce4fd4af62eb43d5b29853",
            "52c7323d1c6f44e7972805133e8ca134",
            "da20685aa9b24d58ad41223f45485fe0",
            "ff5c7ac00b734cdaa0a44cbe1acff29e",
            "ab29260df001471d881a4e2752f5d0de",
            "3de4e73be3cd40449287d401b7ae86ac",
            "5d05f370e24d40a8b0c76275ce349ff7",
            "424ac53f6d194a4fb6eecfd4ae5e465e",
            "d8369888ab44410889405511abc8dfc5",
            "e8ee0c2c8ee2444a993b933f81f70b10",
            "c11aee1f2e7b42ae98dedc47e84555bd",
            "477ecf9b4f0e46ba9b7023c3457b2fa1",
            "ec7aae7775b6421699fb66139433e904",
            "f3dac6ff66324419a228cfea92cf9727",
            "8b7bc7aec1364625814ef5946683ffce",
            "e76afb5ab8924765827fa8b6e7d1e389",
            "2064462ac5334ae28dc3c438072aee37",
            "19d773f9d9064e839988f5d0cc914437",
            "548b13ed359745cfb6b9247189531c2b",
            "68f6409ae0784c05850737623a616688",
            "781e41864d484d999cd5e4ab5a54ac58",
            "e2ce474b812f460f9992bc1956e9c150",
            "36f6581c14914c5faf232ecbbf8c71a2",
            "7a14f1a7126a427cbc33659f32ce0a59",
            "7cf6a50961cf477bbd17b6922278dc4d",
            "4e3406e1984744a19a14896042e1240a",
            "0b8b021ad192444f9466713244ebe47f",
            "cbe15c57ea0446d38bbc33fae10264ca",
            "89e902f1912840389a1aa259c1c84252",
            "b9adcc96c2f04a1cbeccb079320e2abf",
            "a52e2c917b584bd6a128173f97e6f298",
            "171e14b083664289b68d58ebcadad314",
            "ca7bca7de2ee4952bef8cc4b03dfa9c7",
            "10e6b9f415bb48809b0c5a98dd96927b",
            "a98cdad10af043788497e0fb560366b8",
            "372654e39cf54716844fb0d8c8fc9615",
            "6b6d5be66dc24be69195579a35c1ee61",
            "f84166ee9d6b476a830081a41baf7e57",
            "1de32592a96d439093c5a6bd8ff5b777",
            "7ace79cfc887468eae93f5892f8c1998",
            "a0f822d36fcb42018165de4710bbf672",
            "2f521dad8eb54b0aa264437462a1dfd7",
            "a2a391aafcd9468ba69bf9d49600dd4e",
            "4538d6e6940f4ff8acbff03286bf503c",
            "be0118525d1544b6a4a135d0ce101369",
            "40e8147696954198b1f2ca20d9910cb1",
            "1c6a29bd1d0e4217b3f6dcaaca9fa300",
            "0c5471ba94334c849cce149045512b85",
            "4a6e400c58244f6c8021742bb8d00543",
            "f2bf1bb4b0054e00b384f04fbdbb86b3",
            "bd9c45911bec4055849aef7eecc4d6cf",
            "2f731602df604a4e8327fac2c395af99",
            "1bb4e74b6c1d4184b4a5d7722842e43d",
            "9a88ce7f2ac44bffa6a938b8ba60c863",
            "59458c7149844cada0be93f978269a88",
            "5e2d96d9393a4ba08af4f7118b2fb600",
            "4f0ddffd4b85454484649c10122d0f87",
            "29178358a54e4b8fbe0638066e464be9",
            "32a95e1db2464dbda32116ad5879e752",
            "874555ed1d3c478ca521608751492af8",
            "58b610bec47743a5b495b87f30a7bd55",
            "0cd8ade3c3f745768ebb06fcb4198d95",
            "56c2a436293a4a8cb8b62789f4c7cf09",
            "7255b2fd13984d7b81bad09a05daa1ec",
            "ab59bc7059af4075b94f2fac13ff33bb",
            "b36670b48c51417794c21ed195dd1c3b",
            "115036c2e2614fcdaebda2b78a988740",
            "9f12b26a1a464d9a8e3151627d0921d8",
            "d732d195663347818588b05febebc3a3",
            "a97d86e8dc3c47d2b53a046e26abdfc6",
            "228829dfb52d43edbcaf2f5332b58f42",
            "5a7f767393a9427d93486dd8356e8106",
            "a46baa939e8d42bbbfa3db1771f90bc3",
            "82b7050c4fe349e6a6fc49a0608d4ec8",
            "c3507c5985454c36aa4f83e2a661fb16",
            "d54ea90f246f4ef5a54c7e28a968e274",
            "703a9e14e32644278e1ed3b85728facb",
            "3020ee49c50f47109994c58631a3ee44",
            "224695c1308b421dad6890cf4126daae",
            "06093206fd8d4ef58c1e8f3741caf1ee",
            "8abbedc4dc0e4deab08b414ba9210266",
            "b4de01d0890845f9992dc90ebd3b410d",
            "50d9a1a00a9c465c945c3c1c928f9771",
            "3228561877b04c45ab7338abd0d02f13",
            "8764d39c88494cd89f20d207f79701fe",
            "cfb58c0dd80048f3ae641962af17a56c",
            "b66f5ce342ae4c0b8a6a7deb6c2ba912",
            "266e6b9d6ffd429ba4562d1d6b6fb36b",
            "87bd17d77a1d4caeb97b70234dbd7248",
            "13434db54bd04e208bd9ef033446d1e5",
            "bfe5351acf1647b5b0e93656e318dae1",
            "174f64acaeb74877b85527ab9ac60261",
            "82b545728c944f519ad1e12134426771",
            "5f8328d54e6b4556b8187220961ae740",
            "91fd22599ef6497584d144f032eb77b9",
            "ccd1ea6fc6d64a7fb4496369138aa6a9",
            "fbce96681f0d4e2f9076ccf85a9d97c0",
            "686b427a930042819fa618ad27bcfdba",
            "7381515a531d4cf78f5c5ecc80a2ca94",
            "cc1cc4bca9fe4646a420465acc20a6cd",
            "3a9d3bbb2eba47949fcbf3b112eb2c2a",
            "5436c8e4b1a2486dbdc58ff3a7ebeb13",
            "d0d5b29cdc4f411db2401e5860cc6a56",
            "b90279bc775445e8b1f69e94eeb3a710",
            "1020ba121d494d98b946de1abf6d2e7d",
            "91faf571a67447679f56043e6a353ee6",
            "b0f8aef051fe490c86c1ffc920bbf4a7",
            "3a25e247fc9b4c5f9ddac407e265c4be",
            "9553e75ebc83484ebb8dd651864c19c1",
            "ad4d9413e1d746c69722b1ea42d709d8",
            "ef44f44033324c1ea6699d90a9f4df9d",
            "d02da867109d48b998b92fdb4a1f9c99",
            "64ff73504e714597b5d115cea8ae43e8",
            "164dde685a3848999d25281206d1f10b",
            "2e83a991a043478ca5879ec3a6ff1fe3",
            "c237d95d55864f84b574d8a2cb59268d",
            "2046809f60e24821afcc8cee6d1db569",
            "6dc913be8ca24d0c917c480bf9b59d28",
            "37b1810d57464a9c8cda4c19c1d7fe22",
            "4e8b59feb2ac4be2884f409f4c57cff6",
            "11b265e2d9c54213b10848f90a74a320",
            "dd1355db04cd4c20b511a87e118afc6c",
            "bf32d9862cda431dbb5c7396a47daf3b",
            "10977be18b3144e7a98f741c78275399",
            "3856a259d0a541b8bf131a81a455bc6a",
            "d5a00820206348cba3a2dde939ed7823",
            "bf9748fd88514783b593ddb661b5e746",
            "7b09b70ecb854904856742c0182d31be",
            "105e12c3acc4499a86f059ef0e48b979",
            "5a15e5f700e74803a6b9a6e033aa2107",
            "76edb62358f943b8936a59d110d1dad4",
            "4461995efbee492ca3b14379119427c4",
            "98b9e4aa78964793abbd93b6f386d18e",
            "298dbd52ae6441749cc576189dc57dcc",
            "12bab7724d5a49868571c1572e0fa139",
            "3b8cc33626434244971000388c7ad476",
            "d3819e70fcf2496b8357fdb08da0ebf1",
            "b91d6730f3924be6a4b9bfe34a099457",
            "be5c6055ff8c4b06abe7c301701a2d49"
          ]
        },
        "collapsed": true,
        "id": "AjzqLSSxuNSN",
        "outputId": "190be93d-ea38-4004-bcd3-c39a1487f274"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA available: True\n",
            "Device name: NVIDIA A100-SXM4-40GB\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d51451fbe0914d419149dfd01298721c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "processor_config.json:   0%|          | 0.00/173 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6321dbf9e1f241289574079c489975f6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "chat_template.json:   0%|          | 0.00/701 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "698ca72845e247269a3e28f2d37fcdb6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/505 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3ce27b9d396b44108550c798c1fe443e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.45k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "607e1cccc37844b98af4de125c2b14a9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a53089b534e244c2aa2af895557eea12",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/3.62M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "97c2a58c1e0744e19b9b15d080f1d4e0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/41.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "424ac53f6d194a4fb6eecfd4ae5e465e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/552 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "548b13ed359745cfb6b9247189531c2b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/1.10k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b9adcc96c2f04a1cbeccb079320e2abf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/77.2k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a0f822d36fcb42018165de4710bbf672",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading shards:   0%|          | 0/6 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2f731602df604a4e8327fac2c395af99",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00006.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "56c2a436293a4a8cb8b62789f4c7cf09",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00006.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "82b7050c4fe349e6a6fc49a0608d4ec8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00003-of-00006.safetensors:   0%|          | 0.00/4.88G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8764d39c88494cd89f20d207f79701fe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00004-of-00006.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ccd1ea6fc6d64a7fb4496369138aa6a9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00005-of-00006.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b0f8aef051fe490c86c1ffc920bbf4a7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00006-of-00006.safetensors:   0%|          | 0.00/2.02G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6dc913be8ca24d0c917c480bf9b59d28",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "105e12c3acc4499a86f059ef0e48b979",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/141 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Model and processor loaded successfully!\n",
            "Model loaded on: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoProcessor, LlavaForConditionalGeneration, BitsAndBytesConfig\n",
        "\n",
        "# Check CUDA availability\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "# Model name\n",
        "model_name = \"llava-hf/llava-1.5-13b-hf\"\n",
        "\n",
        "# Load processor\n",
        "processor = AutoProcessor.from_pretrained(model_name, token='hf_token')\n",
        "\n",
        "# Load model and move to GPU\n",
        "try:\n",
        "\n",
        "    model = LlavaForConditionalGeneration.from_pretrained(\n",
        "        model_name,\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        llm_int8_enable_fp32_cpu_offload=True,\n",
        "        token=\"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\",\n",
        "        device_map=\"auto\",  # Automatically place on GPU\n",
        "        torch_dtype=torch.float16,\n",
        "        #torch_compile=False,\n",
        "    )\n",
        "\n",
        "    # Enable gradient checkpointing for memory efficiency\n",
        "    model.gradient_checkpointing_enable()\n",
        "\n",
        "    print(\"\\n‚úÖ Model and processor loaded successfully!\")\n",
        "    print(f\"Model loaded on: {device}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Error loading model: {e}\")\n",
        "    model = None\n",
        "    processor = None\n",
        "\n",
        "# Ensure processor is also set to use GPU\n",
        "if processor is not None:\n",
        "    processor.device = device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-oKXUyP1xByT"
      },
      "outputs": [],
      "source": [
        "# print(\"\\nüîç **Model Expected Input Signature** üîç\")\n",
        "# print(model.forward.__doc__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkVXMiB606CI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "class LLaVADataset(Dataset):\n",
        "    def __init__(self, image_folder, processor, max_length=2048, image_size=(336, 336), patch_size=14):\n",
        "        self.image_folder = image_folder\n",
        "        self.processor = processor\n",
        "        self.max_length = max_length\n",
        "        self.image_size = image_size\n",
        "        self.patch_size = patch_size\n",
        "        self.image_token_id = processor.tokenizer.convert_tokens_to_ids(\"<image>\")\n",
        "\n",
        "        # Gather .jpg files\n",
        "        self.image_files = [f for f in os.listdir(image_folder) if f.endswith(\".jpg\")]\n",
        "\n",
        "        if not self.image_files:\n",
        "            raise ValueError(f\"No images found in {image_folder}. Check dataset path!\")\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize(self.image_size),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "        self.patch_count_h = self.image_size[0] // self.patch_size\n",
        "        self.patch_count_w = self.image_size[1] // self.patch_size\n",
        "        self.num_patches = self.patch_count_h * self.patch_count_w\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_file = self.image_files[idx]\n",
        "        image_path = os.path.join(self.image_folder, image_file)\n",
        "\n",
        "        if not os.path.exists(image_path):\n",
        "            print(f\"‚ö†Ô∏è Missing image: {image_path}\")\n",
        "            return None  # Return None to be filtered later\n",
        "\n",
        "        try:\n",
        "            caption_text = os.path.splitext(image_file)[0].replace(\"_\", \" \")\n",
        "            text_prompt = f\"Here is an image: {caption_text}\\n\"\n",
        "            image = Image.open(image_path).convert(\"RGB\")\n",
        "            image_tensor = self.transform(image)\n",
        "\n",
        "            text_inputs = self.processor.tokenizer(\n",
        "                text_prompt,\n",
        "                padding=\"max_length\",\n",
        "                truncation=True,\n",
        "                max_length=max(0, self.max_length - self.num_patches),\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "            input_ids = text_inputs[\"input_ids\"].squeeze(0).to(torch.int64)\n",
        "            attention_mask = text_inputs[\"attention_mask\"].squeeze(0).to(torch.int64)\n",
        "\n",
        "            # Append 576 <image> tokens\n",
        "            image_tokens = torch.tensor([self.image_token_id] * self.num_patches, dtype=torch.int64)\n",
        "            input_ids = torch.cat([input_ids, image_tokens])\n",
        "            image_attn = torch.ones_like(image_tokens, dtype=torch.int64)\n",
        "            attention_mask = torch.cat([attention_mask, image_attn])\n",
        "\n",
        "            image_grid_thw = torch.tensor([1, self.patch_count_h, self.patch_count_w], dtype=torch.int64)\n",
        "\n",
        "            labels = input_ids.clone()\n",
        "            pad_id = self.processor.tokenizer.pad_token_id\n",
        "            if pad_id is not None:\n",
        "                labels[labels == pad_id] = -100\n",
        "\n",
        "            return {\n",
        "                \"input_ids\": input_ids,\n",
        "                \"attention_mask\": attention_mask,\n",
        "                \"pixel_values\": image_tensor.to(torch.float32),\n",
        "                \"image_grid_thw\": image_grid_thw,\n",
        "                \"labels\": labels,\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error processing {image_file}: {e}\")\n",
        "            return None  # Return None to be filtered\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KE23keoQ10-_"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch\n",
        "\n",
        "def collate_fn(batch):\n",
        "    batch = [b for b in batch if b is not None]\n",
        "\n",
        "    if not batch:\n",
        "        raise ValueError(\"Empty batch after filtering - check dataset or tokenization errors\")\n",
        "\n",
        "    input_ids = [item[\"input_ids\"].squeeze(0) for item in batch]\n",
        "    attention_mask = [item[\"attention_mask\"].squeeze(0) for item in batch]\n",
        "    pixel_values = [item[\"pixel_values\"] for item in batch]\n",
        "    image_grid_thw = [item[\"image_grid_thw\"] for item in batch]\n",
        "    labels = [item[\"labels\"].squeeze(0) for item in batch]\n",
        "\n",
        "    padded_input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
        "    padded_attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
        "    padded_labels = pad_sequence(labels, batch_first=True, padding_value=-100)\n",
        "\n",
        "    try:\n",
        "        pixel_values = torch.stack(pixel_values)\n",
        "        image_grid_thw = torch.stack(image_grid_thw)\n",
        "    except RuntimeError as e:\n",
        "        raise ValueError(f\"Image tensor shape mismatch: {[p.shape for p in pixel_values]}\") from e\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": padded_input_ids,\n",
        "        \"attention_mask\": padded_attention_mask,\n",
        "        \"pixel_values\": pixel_values,\n",
        "        \"image_grid_thw\": image_grid_thw,\n",
        "        \"labels\": padded_labels\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1Odcs_6ALnP"
      },
      "outputs": [],
      "source": [
        "# \"image_grid_thw\": image_grid_thw,\n",
        "# image_grid_thw = torch.stack(image_grid_thw)\n",
        "# image_grid_thw = [item[\"image_grid_thw\"] for item in batch]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ROzSgEDd13Wl",
        "outputId": "9dec7f62-bbb7-43e3-97dd-da3e1ed1ba9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 746/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 747/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 748/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 749/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 750/1460\n",
            "   input_ids: torch.Size([4, 607])\n",
            "   attention_mask: torch.Size([4, 607])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 607])\n",
            "\n",
            "üîπ Training Step 751/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "üîπ Training Step 752/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 753/1460\n",
            "   input_ids: torch.Size([4, 611])\n",
            "   attention_mask: torch.Size([4, 611])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 611])\n",
            "\n",
            "üîπ Training Step 754/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 755/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "üîπ Training Step 756/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 757/1460\n",
            "   input_ids: torch.Size([4, 612])\n",
            "   attention_mask: torch.Size([4, 612])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 612])\n",
            "\n",
            "üîπ Training Step 758/1460\n",
            "   input_ids: torch.Size([4, 601])\n",
            "   attention_mask: torch.Size([4, 601])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 601])\n",
            "\n",
            "üîπ Training Step 759/1460\n",
            "   input_ids: torch.Size([4, 604])\n",
            "   attention_mask: torch.Size([4, 604])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 604])\n",
            "\n",
            "üîπ Training Step 760/1460\n",
            "   input_ids: torch.Size([4, 611])\n",
            "   attention_mask: torch.Size([4, 611])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 611])\n",
            "\n",
            "üîπ Training Step 761/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 762/1460\n",
            "   input_ids: torch.Size([4, 611])\n",
            "   attention_mask: torch.Size([4, 611])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 611])\n",
            "\n",
            "üîπ Training Step 763/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 764/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 765/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 766/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 767/1460\n",
            "   input_ids: torch.Size([4, 610])\n",
            "   attention_mask: torch.Size([4, 610])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 610])\n",
            "\n",
            "üîπ Training Step 768/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 769/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "üîπ Training Step 770/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 771/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 772/1460\n",
            "   input_ids: torch.Size([4, 601])\n",
            "   attention_mask: torch.Size([4, 601])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 601])\n",
            "\n",
            "üîπ Training Step 773/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "üîπ Training Step 774/1460\n",
            "   input_ids: torch.Size([4, 605])\n",
            "   attention_mask: torch.Size([4, 605])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 605])\n",
            "\n",
            "üîπ Training Step 775/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 776/1460\n",
            "   input_ids: torch.Size([4, 595])\n",
            "   attention_mask: torch.Size([4, 595])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 595])\n",
            "\n",
            "üîπ Training Step 777/1460\n",
            "   input_ids: torch.Size([4, 601])\n",
            "   attention_mask: torch.Size([4, 601])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 601])\n",
            "\n",
            "üîπ Training Step 778/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "üîπ Training Step 779/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 780/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 781/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 782/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "üîπ Training Step 783/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 784/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 785/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 786/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 787/1460\n",
            "   input_ids: torch.Size([4, 607])\n",
            "   attention_mask: torch.Size([4, 607])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 607])\n",
            "\n",
            "üîπ Training Step 788/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 789/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 790/1460\n",
            "   input_ids: torch.Size([4, 606])\n",
            "   attention_mask: torch.Size([4, 606])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 606])\n",
            "\n",
            "üîπ Training Step 791/1460\n",
            "   input_ids: torch.Size([4, 607])\n",
            "   attention_mask: torch.Size([4, 607])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 607])\n",
            "\n",
            "üîπ Training Step 792/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 793/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 794/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 795/1460\n",
            "   input_ids: torch.Size([4, 616])\n",
            "   attention_mask: torch.Size([4, 616])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 616])\n",
            "\n",
            "üîπ Training Step 796/1460\n",
            "   input_ids: torch.Size([4, 602])\n",
            "   attention_mask: torch.Size([4, 602])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 602])\n",
            "\n",
            "üîπ Training Step 797/1460\n",
            "   input_ids: torch.Size([4, 595])\n",
            "   attention_mask: torch.Size([4, 595])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 595])\n",
            "\n",
            "üîπ Training Step 798/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 799/1460\n",
            "   input_ids: torch.Size([4, 601])\n",
            "   attention_mask: torch.Size([4, 601])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 601])\n",
            "\n",
            "üîπ Training Step 800/1460\n",
            "   input_ids: torch.Size([4, 614])\n",
            "   attention_mask: torch.Size([4, 614])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 614])\n",
            "\n",
            "üîπ Training Step 801/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 802/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 803/1460\n",
            "   input_ids: torch.Size([4, 613])\n",
            "   attention_mask: torch.Size([4, 613])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 613])\n",
            "\n",
            "üîπ Training Step 804/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 805/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "üîπ Training Step 806/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 807/1460\n",
            "   input_ids: torch.Size([4, 602])\n",
            "   attention_mask: torch.Size([4, 602])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 602])\n",
            "\n",
            "üîπ Training Step 808/1460\n",
            "   input_ids: torch.Size([4, 607])\n",
            "   attention_mask: torch.Size([4, 607])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 607])\n",
            "\n",
            "üîπ Training Step 809/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "üîπ Training Step 810/1460\n",
            "   input_ids: torch.Size([4, 606])\n",
            "   attention_mask: torch.Size([4, 606])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 606])\n",
            "\n",
            "üîπ Training Step 811/1460\n",
            "   input_ids: torch.Size([4, 612])\n",
            "   attention_mask: torch.Size([4, 612])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 612])\n",
            "\n",
            "üîπ Training Step 812/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 813/1460\n",
            "   input_ids: torch.Size([4, 604])\n",
            "   attention_mask: torch.Size([4, 604])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 604])\n",
            "\n",
            "üîπ Training Step 814/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 815/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 816/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 817/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 818/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 819/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 820/1460\n",
            "   input_ids: torch.Size([4, 605])\n",
            "   attention_mask: torch.Size([4, 605])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 605])\n",
            "\n",
            "üîπ Training Step 821/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 822/1460\n",
            "   input_ids: torch.Size([4, 605])\n",
            "   attention_mask: torch.Size([4, 605])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 605])\n",
            "\n",
            "üîπ Training Step 823/1460\n",
            "   input_ids: torch.Size([4, 608])\n",
            "   attention_mask: torch.Size([4, 608])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 608])\n",
            "\n",
            "üîπ Training Step 824/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 825/1460\n",
            "   input_ids: torch.Size([4, 603])\n",
            "   attention_mask: torch.Size([4, 603])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 603])\n",
            "\n",
            "üîπ Training Step 826/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 827/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 828/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 829/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 830/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 831/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 832/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 833/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 834/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 835/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 836/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 837/1460\n",
            "   input_ids: torch.Size([4, 608])\n",
            "   attention_mask: torch.Size([4, 608])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 608])\n",
            "\n",
            "üîπ Training Step 838/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 839/1460\n",
            "   input_ids: torch.Size([4, 609])\n",
            "   attention_mask: torch.Size([4, 609])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 609])\n",
            "\n",
            "üîπ Training Step 840/1460\n",
            "   input_ids: torch.Size([4, 603])\n",
            "   attention_mask: torch.Size([4, 603])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 603])\n",
            "\n",
            "üîπ Training Step 841/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 842/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 843/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 844/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 845/1460\n",
            "   input_ids: torch.Size([4, 608])\n",
            "   attention_mask: torch.Size([4, 608])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 608])\n",
            "\n",
            "üîπ Training Step 846/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 847/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 848/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 849/1460\n",
            "   input_ids: torch.Size([4, 607])\n",
            "   attention_mask: torch.Size([4, 607])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 607])\n",
            "\n",
            "üîπ Training Step 850/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "üîπ Training Step 851/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 852/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 853/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 854/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 855/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 856/1460\n",
            "   input_ids: torch.Size([4, 607])\n",
            "   attention_mask: torch.Size([4, 607])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 607])\n",
            "\n",
            "üîπ Training Step 857/1460\n",
            "   input_ids: torch.Size([4, 606])\n",
            "   attention_mask: torch.Size([4, 606])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 606])\n",
            "\n",
            "üîπ Training Step 858/1460\n",
            "   input_ids: torch.Size([4, 614])\n",
            "   attention_mask: torch.Size([4, 614])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 614])\n",
            "\n",
            "üîπ Training Step 859/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 860/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "üîπ Training Step 861/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 862/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "üîπ Training Step 863/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "üîπ Training Step 864/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "üîπ Training Step 865/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 866/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 867/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 868/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 869/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 870/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 871/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 872/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 873/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 874/1460\n",
            "   input_ids: torch.Size([4, 609])\n",
            "   attention_mask: torch.Size([4, 609])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 609])\n",
            "\n",
            "üîπ Training Step 875/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 876/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 877/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "üîπ Training Step 878/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 879/1460\n",
            "   input_ids: torch.Size([4, 611])\n",
            "   attention_mask: torch.Size([4, 611])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 611])\n",
            "\n",
            "üîπ Training Step 880/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 881/1460\n",
            "   input_ids: torch.Size([4, 606])\n",
            "   attention_mask: torch.Size([4, 606])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 606])\n",
            "\n",
            "üîπ Training Step 882/1460\n",
            "   input_ids: torch.Size([4, 606])\n",
            "   attention_mask: torch.Size([4, 606])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 606])\n",
            "\n",
            "üîπ Training Step 883/1460\n",
            "   input_ids: torch.Size([4, 595])\n",
            "   attention_mask: torch.Size([4, 595])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 595])\n",
            "\n",
            "üîπ Training Step 884/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 885/1460\n",
            "   input_ids: torch.Size([4, 608])\n",
            "   attention_mask: torch.Size([4, 608])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 608])\n",
            "\n",
            "üîπ Training Step 886/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 887/1460\n",
            "   input_ids: torch.Size([4, 611])\n",
            "   attention_mask: torch.Size([4, 611])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 611])\n",
            "\n",
            "üîπ Training Step 888/1460\n",
            "   input_ids: torch.Size([4, 608])\n",
            "   attention_mask: torch.Size([4, 608])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 608])\n",
            "\n",
            "üîπ Training Step 889/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 890/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 891/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "üîπ Training Step 892/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 893/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 894/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 895/1460\n",
            "   input_ids: torch.Size([4, 611])\n",
            "   attention_mask: torch.Size([4, 611])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 611])\n",
            "\n",
            "üîπ Training Step 896/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 897/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 898/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 899/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 900/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 901/1460\n",
            "   input_ids: torch.Size([4, 609])\n",
            "   attention_mask: torch.Size([4, 609])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 609])\n",
            "\n",
            "üîπ Training Step 902/1460\n",
            "   input_ids: torch.Size([4, 595])\n",
            "   attention_mask: torch.Size([4, 595])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 595])\n",
            "\n",
            "üîπ Training Step 903/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 904/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 905/1460\n",
            "   input_ids: torch.Size([4, 603])\n",
            "   attention_mask: torch.Size([4, 603])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 603])\n",
            "\n",
            "üîπ Training Step 906/1460\n",
            "   input_ids: torch.Size([4, 606])\n",
            "   attention_mask: torch.Size([4, 606])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 606])\n",
            "\n",
            "üîπ Training Step 907/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 908/1460\n",
            "   input_ids: torch.Size([4, 606])\n",
            "   attention_mask: torch.Size([4, 606])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 606])\n",
            "\n",
            "üîπ Training Step 909/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 910/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 911/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 912/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 913/1460\n",
            "   input_ids: torch.Size([4, 595])\n",
            "   attention_mask: torch.Size([4, 595])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 595])\n",
            "\n",
            "üîπ Training Step 914/1460\n",
            "   input_ids: torch.Size([4, 607])\n",
            "   attention_mask: torch.Size([4, 607])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 607])\n",
            "\n",
            "üîπ Training Step 915/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 916/1460\n",
            "   input_ids: torch.Size([4, 605])\n",
            "   attention_mask: torch.Size([4, 605])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 605])\n",
            "\n",
            "üîπ Training Step 917/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "üîπ Training Step 918/1460\n",
            "   input_ids: torch.Size([4, 603])\n",
            "   attention_mask: torch.Size([4, 603])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 603])\n",
            "\n",
            "üîπ Training Step 919/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 920/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "üîπ Training Step 921/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 922/1460\n",
            "   input_ids: torch.Size([4, 611])\n",
            "   attention_mask: torch.Size([4, 611])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 611])\n",
            "\n",
            "üîπ Training Step 923/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 924/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 925/1460\n",
            "   input_ids: torch.Size([4, 607])\n",
            "   attention_mask: torch.Size([4, 607])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 607])\n",
            "\n",
            "üîπ Training Step 926/1460\n",
            "   input_ids: torch.Size([4, 604])\n",
            "   attention_mask: torch.Size([4, 604])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 604])\n",
            "\n",
            "üîπ Training Step 927/1460\n",
            "   input_ids: torch.Size([4, 613])\n",
            "   attention_mask: torch.Size([4, 613])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 613])\n",
            "\n",
            "üîπ Training Step 928/1460\n",
            "   input_ids: torch.Size([4, 607])\n",
            "   attention_mask: torch.Size([4, 607])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 607])\n",
            "\n",
            "üîπ Training Step 929/1460\n",
            "   input_ids: torch.Size([4, 610])\n",
            "   attention_mask: torch.Size([4, 610])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 610])\n",
            "\n",
            "üîπ Training Step 930/1460\n",
            "   input_ids: torch.Size([4, 608])\n",
            "   attention_mask: torch.Size([4, 608])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 608])\n",
            "\n",
            "üîπ Training Step 931/1460\n",
            "   input_ids: torch.Size([4, 609])\n",
            "   attention_mask: torch.Size([4, 609])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 609])\n",
            "\n",
            "üîπ Training Step 932/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "üîπ Training Step 933/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 934/1460\n",
            "   input_ids: torch.Size([4, 602])\n",
            "   attention_mask: torch.Size([4, 602])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 602])\n",
            "\n",
            "üîπ Training Step 935/1460\n",
            "   input_ids: torch.Size([4, 608])\n",
            "   attention_mask: torch.Size([4, 608])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 608])\n",
            "\n",
            "üîπ Training Step 936/1460\n",
            "   input_ids: torch.Size([4, 605])\n",
            "   attention_mask: torch.Size([4, 605])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 605])\n",
            "\n",
            "üîπ Training Step 937/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 938/1460\n",
            "   input_ids: torch.Size([4, 610])\n",
            "   attention_mask: torch.Size([4, 610])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 610])\n",
            "\n",
            "üîπ Training Step 939/1460\n",
            "   input_ids: torch.Size([4, 612])\n",
            "   attention_mask: torch.Size([4, 612])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 612])\n",
            "\n",
            "üîπ Training Step 940/1460\n",
            "   input_ids: torch.Size([4, 606])\n",
            "   attention_mask: torch.Size([4, 606])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 606])\n",
            "\n",
            "üîπ Training Step 941/1460\n",
            "   input_ids: torch.Size([4, 606])\n",
            "   attention_mask: torch.Size([4, 606])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 606])\n",
            "\n",
            "üîπ Training Step 942/1460\n",
            "   input_ids: torch.Size([4, 611])\n",
            "   attention_mask: torch.Size([4, 611])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 611])\n",
            "\n",
            "üîπ Training Step 943/1460\n",
            "   input_ids: torch.Size([4, 613])\n",
            "   attention_mask: torch.Size([4, 613])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 613])\n",
            "\n",
            "üîπ Training Step 944/1460\n",
            "   input_ids: torch.Size([4, 601])\n",
            "   attention_mask: torch.Size([4, 601])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 601])\n",
            "\n",
            "üîπ Training Step 945/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 946/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 947/1460\n",
            "   input_ids: torch.Size([4, 606])\n",
            "   attention_mask: torch.Size([4, 606])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 606])\n",
            "\n",
            "üîπ Training Step 948/1460\n",
            "   input_ids: torch.Size([4, 616])\n",
            "   attention_mask: torch.Size([4, 616])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 616])\n",
            "\n",
            "üîπ Training Step 949/1460\n",
            "   input_ids: torch.Size([4, 601])\n",
            "   attention_mask: torch.Size([4, 601])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 601])\n",
            "\n",
            "üîπ Training Step 950/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 951/1460\n",
            "   input_ids: torch.Size([4, 610])\n",
            "   attention_mask: torch.Size([4, 610])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 610])\n",
            "\n",
            "üîπ Training Step 952/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 953/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "üîπ Training Step 954/1460\n",
            "   input_ids: torch.Size([4, 602])\n",
            "   attention_mask: torch.Size([4, 602])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 602])\n",
            "\n",
            "üîπ Training Step 955/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 956/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 957/1460\n",
            "   input_ids: torch.Size([4, 595])\n",
            "   attention_mask: torch.Size([4, 595])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 595])\n",
            "\n",
            "üîπ Training Step 958/1460\n",
            "   input_ids: torch.Size([4, 595])\n",
            "   attention_mask: torch.Size([4, 595])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 595])\n",
            "\n",
            "üîπ Training Step 959/1460\n",
            "   input_ids: torch.Size([4, 606])\n",
            "   attention_mask: torch.Size([4, 606])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 606])\n",
            "\n",
            "üîπ Training Step 960/1460\n",
            "   input_ids: torch.Size([4, 614])\n",
            "   attention_mask: torch.Size([4, 614])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 614])\n",
            "\n",
            "üîπ Training Step 961/1460\n",
            "   input_ids: torch.Size([4, 604])\n",
            "   attention_mask: torch.Size([4, 604])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 604])\n",
            "\n",
            "üîπ Training Step 962/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 963/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 964/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 965/1460\n",
            "   input_ids: torch.Size([4, 612])\n",
            "   attention_mask: torch.Size([4, 612])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 612])\n",
            "\n",
            "üîπ Training Step 966/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 967/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 968/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 969/1460\n",
            "   input_ids: torch.Size([4, 610])\n",
            "   attention_mask: torch.Size([4, 610])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 610])\n",
            "\n",
            "üîπ Training Step 970/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 971/1460\n",
            "   input_ids: torch.Size([4, 595])\n",
            "   attention_mask: torch.Size([4, 595])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 595])\n",
            "\n",
            "üîπ Training Step 972/1460\n",
            "   input_ids: torch.Size([4, 606])\n",
            "   attention_mask: torch.Size([4, 606])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 606])\n",
            "\n",
            "üîπ Training Step 973/1460\n",
            "   input_ids: torch.Size([4, 606])\n",
            "   attention_mask: torch.Size([4, 606])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 606])\n",
            "\n",
            "üîπ Training Step 974/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "üîπ Training Step 975/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 976/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 977/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 978/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 979/1460\n",
            "   input_ids: torch.Size([4, 607])\n",
            "   attention_mask: torch.Size([4, 607])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 607])\n",
            "\n",
            "üîπ Training Step 980/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 981/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 982/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 983/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 984/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 985/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 986/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 987/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 988/1460\n",
            "   input_ids: torch.Size([4, 604])\n",
            "   attention_mask: torch.Size([4, 604])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 604])\n",
            "\n",
            "üîπ Training Step 989/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 990/1460\n",
            "   input_ids: torch.Size([4, 604])\n",
            "   attention_mask: torch.Size([4, 604])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 604])\n",
            "\n",
            "üîπ Training Step 991/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 992/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 993/1460\n",
            "   input_ids: torch.Size([4, 609])\n",
            "   attention_mask: torch.Size([4, 609])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 609])\n",
            "\n",
            "üîπ Training Step 994/1460\n",
            "   input_ids: torch.Size([4, 616])\n",
            "   attention_mask: torch.Size([4, 616])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 616])\n",
            "\n",
            "üîπ Training Step 995/1460\n",
            "   input_ids: torch.Size([4, 610])\n",
            "   attention_mask: torch.Size([4, 610])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 610])\n",
            "\n",
            "üîπ Training Step 996/1460\n",
            "   input_ids: torch.Size([4, 609])\n",
            "   attention_mask: torch.Size([4, 609])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 609])\n",
            "\n",
            "üîπ Training Step 997/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "üîπ Training Step 998/1460\n",
            "   input_ids: torch.Size([4, 605])\n",
            "   attention_mask: torch.Size([4, 605])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 605])\n",
            "\n",
            "üîπ Training Step 999/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1000/1460\n",
            "   input_ids: torch.Size([4, 607])\n",
            "   attention_mask: torch.Size([4, 607])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 607])\n",
            "\n",
            "üîπ Training Step 1001/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 1002/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 1003/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1004/1460\n",
            "   input_ids: torch.Size([4, 595])\n",
            "   attention_mask: torch.Size([4, 595])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 595])\n",
            "\n",
            "üîπ Training Step 1005/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1006/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 1007/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 1008/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 1009/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "üîπ Training Step 1010/1460\n",
            "   input_ids: torch.Size([4, 594])\n",
            "   attention_mask: torch.Size([4, 594])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 594])\n",
            "\n",
            "üîπ Training Step 1011/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1012/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1013/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1014/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1015/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1016/1460\n",
            "   input_ids: torch.Size([4, 616])\n",
            "   attention_mask: torch.Size([4, 616])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 616])\n",
            "\n",
            "üîπ Training Step 1017/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1018/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1019/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "üîπ Training Step 1020/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1021/1460\n",
            "   input_ids: torch.Size([4, 616])\n",
            "   attention_mask: torch.Size([4, 616])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 616])\n",
            "\n",
            "üîπ Training Step 1022/1460\n",
            "   input_ids: torch.Size([4, 615])\n",
            "   attention_mask: torch.Size([4, 615])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 615])\n",
            "\n",
            "üîπ Training Step 1023/1460\n",
            "   input_ids: torch.Size([4, 601])\n",
            "   attention_mask: torch.Size([4, 601])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 601])\n",
            "\n",
            "üîπ Training Step 1024/1460\n",
            "   input_ids: torch.Size([4, 610])\n",
            "   attention_mask: torch.Size([4, 610])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 610])\n",
            "\n",
            "üîπ Training Step 1025/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 1026/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 1027/1460\n",
            "   input_ids: torch.Size([4, 607])\n",
            "   attention_mask: torch.Size([4, 607])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 607])\n",
            "\n",
            "üîπ Training Step 1028/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1029/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1030/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1031/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1032/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1033/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 1034/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1035/1460\n",
            "   input_ids: torch.Size([4, 609])\n",
            "   attention_mask: torch.Size([4, 609])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 609])\n",
            "\n",
            "üîπ Training Step 1036/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 1037/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1038/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1039/1460\n",
            "   input_ids: torch.Size([4, 595])\n",
            "   attention_mask: torch.Size([4, 595])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 595])\n",
            "\n",
            "üîπ Training Step 1040/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "üîπ Training Step 1041/1460\n",
            "   input_ids: torch.Size([4, 603])\n",
            "   attention_mask: torch.Size([4, 603])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 603])\n",
            "\n",
            "üîπ Training Step 1042/1460\n",
            "   input_ids: torch.Size([4, 617])\n",
            "   attention_mask: torch.Size([4, 617])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 617])\n",
            "\n",
            "üîπ Training Step 1043/1460\n",
            "   input_ids: torch.Size([4, 605])\n",
            "   attention_mask: torch.Size([4, 605])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 605])\n",
            "\n",
            "üîπ Training Step 1044/1460\n",
            "   input_ids: torch.Size([4, 611])\n",
            "   attention_mask: torch.Size([4, 611])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 611])\n",
            "\n",
            "üîπ Training Step 1045/1460\n",
            "   input_ids: torch.Size([4, 608])\n",
            "   attention_mask: torch.Size([4, 608])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 608])\n",
            "\n",
            "üîπ Training Step 1046/1460\n",
            "   input_ids: torch.Size([4, 607])\n",
            "   attention_mask: torch.Size([4, 607])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 607])\n",
            "\n",
            "üîπ Training Step 1047/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1048/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1049/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1050/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 1051/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1052/1460\n",
            "   input_ids: torch.Size([4, 611])\n",
            "   attention_mask: torch.Size([4, 611])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 611])\n",
            "\n",
            "üîπ Training Step 1053/1460\n",
            "   input_ids: torch.Size([4, 605])\n",
            "   attention_mask: torch.Size([4, 605])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 605])\n",
            "\n",
            "üîπ Training Step 1054/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "üîπ Training Step 1055/1460\n",
            "   input_ids: torch.Size([4, 602])\n",
            "   attention_mask: torch.Size([4, 602])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 602])\n",
            "\n",
            "üîπ Training Step 1056/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 1057/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1058/1460\n",
            "   input_ids: torch.Size([4, 614])\n",
            "   attention_mask: torch.Size([4, 614])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 614])\n",
            "\n",
            "üîπ Training Step 1059/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 1060/1460\n",
            "   input_ids: torch.Size([4, 595])\n",
            "   attention_mask: torch.Size([4, 595])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 595])\n",
            "\n",
            "üîπ Training Step 1061/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1062/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 1063/1460\n",
            "   input_ids: torch.Size([4, 612])\n",
            "   attention_mask: torch.Size([4, 612])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 612])\n",
            "\n",
            "üîπ Training Step 1064/1460\n",
            "   input_ids: torch.Size([4, 595])\n",
            "   attention_mask: torch.Size([4, 595])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 595])\n",
            "\n",
            "üîπ Training Step 1065/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1066/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1067/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1068/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 1069/1460\n",
            "   input_ids: torch.Size([4, 595])\n",
            "   attention_mask: torch.Size([4, 595])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 595])\n",
            "\n",
            "üîπ Training Step 1070/1460\n",
            "   input_ids: torch.Size([4, 609])\n",
            "   attention_mask: torch.Size([4, 609])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 609])\n",
            "\n",
            "üîπ Training Step 1071/1460\n",
            "   input_ids: torch.Size([4, 603])\n",
            "   attention_mask: torch.Size([4, 603])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 603])\n",
            "\n",
            "üîπ Training Step 1072/1460\n",
            "   input_ids: torch.Size([4, 603])\n",
            "   attention_mask: torch.Size([4, 603])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 603])\n",
            "\n",
            "üîπ Training Step 1073/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1074/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 1075/1460\n",
            "   input_ids: torch.Size([4, 601])\n",
            "   attention_mask: torch.Size([4, 601])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 601])\n",
            "\n",
            "üîπ Training Step 1076/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 1077/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1078/1460\n",
            "   input_ids: torch.Size([4, 607])\n",
            "   attention_mask: torch.Size([4, 607])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 607])\n",
            "\n",
            "üîπ Training Step 1079/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "üîπ Training Step 1080/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 1081/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1082/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 1083/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 1084/1460\n",
            "   input_ids: torch.Size([4, 604])\n",
            "   attention_mask: torch.Size([4, 604])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 604])\n",
            "\n",
            "üîπ Training Step 1085/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1086/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 1087/1460\n",
            "   input_ids: torch.Size([4, 612])\n",
            "   attention_mask: torch.Size([4, 612])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 612])\n",
            "\n",
            "üîπ Training Step 1088/1460\n",
            "   input_ids: torch.Size([4, 609])\n",
            "   attention_mask: torch.Size([4, 609])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 609])\n",
            "\n",
            "üîπ Training Step 1089/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1090/1460\n",
            "   input_ids: torch.Size([4, 610])\n",
            "   attention_mask: torch.Size([4, 610])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 610])\n",
            "\n",
            "üîπ Training Step 1091/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1092/1460\n",
            "   input_ids: torch.Size([4, 611])\n",
            "   attention_mask: torch.Size([4, 611])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 611])\n",
            "\n",
            "üîπ Training Step 1093/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 1094/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1095/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1096/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1097/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 1098/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "üîπ Training Step 1099/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1100/1460\n",
            "   input_ids: torch.Size([4, 606])\n",
            "   attention_mask: torch.Size([4, 606])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 606])\n",
            "\n",
            "üîπ Training Step 1101/1460\n",
            "   input_ids: torch.Size([4, 607])\n",
            "   attention_mask: torch.Size([4, 607])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 607])\n",
            "\n",
            "üîπ Training Step 1102/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "üîπ Training Step 1103/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 1104/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1105/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 1106/1460\n",
            "   input_ids: torch.Size([4, 595])\n",
            "   attention_mask: torch.Size([4, 595])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 595])\n",
            "\n",
            "üîπ Training Step 1107/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1108/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1109/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "üîπ Training Step 1110/1460\n",
            "   input_ids: torch.Size([4, 609])\n",
            "   attention_mask: torch.Size([4, 609])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 609])\n",
            "\n",
            "üîπ Training Step 1111/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1112/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1113/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1114/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 1115/1460\n",
            "   input_ids: torch.Size([4, 608])\n",
            "   attention_mask: torch.Size([4, 608])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 608])\n",
            "\n",
            "üîπ Training Step 1116/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 1117/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1118/1460\n",
            "   input_ids: torch.Size([4, 612])\n",
            "   attention_mask: torch.Size([4, 612])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 612])\n",
            "\n",
            "üîπ Training Step 1119/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1120/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 1121/1460\n",
            "   input_ids: torch.Size([4, 610])\n",
            "   attention_mask: torch.Size([4, 610])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 610])\n",
            "\n",
            "üîπ Training Step 1122/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 1123/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1124/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1125/1460\n",
            "   input_ids: torch.Size([4, 614])\n",
            "   attention_mask: torch.Size([4, 614])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 614])\n",
            "\n",
            "üîπ Training Step 1126/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 1127/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "üîπ Training Step 1128/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1129/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 1130/1460\n",
            "   input_ids: torch.Size([4, 601])\n",
            "   attention_mask: torch.Size([4, 601])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 601])\n",
            "\n",
            "üîπ Training Step 1131/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 1132/1460\n",
            "   input_ids: torch.Size([4, 601])\n",
            "   attention_mask: torch.Size([4, 601])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 601])\n",
            "\n",
            "üîπ Training Step 1133/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "üîπ Training Step 1134/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 1135/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 1136/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1137/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1138/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1139/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1140/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1141/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1142/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "üîπ Training Step 1143/1460\n",
            "   input_ids: torch.Size([4, 611])\n",
            "   attention_mask: torch.Size([4, 611])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 611])\n",
            "\n",
            "üîπ Training Step 1144/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "üîπ Training Step 1145/1460\n",
            "   input_ids: torch.Size([4, 595])\n",
            "   attention_mask: torch.Size([4, 595])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 595])\n",
            "\n",
            "üîπ Training Step 1146/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "üîπ Training Step 1147/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 1148/1460\n",
            "   input_ids: torch.Size([4, 613])\n",
            "   attention_mask: torch.Size([4, 613])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 613])\n",
            "\n",
            "üîπ Training Step 1149/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "üîπ Training Step 1150/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1151/1460\n",
            "   input_ids: torch.Size([4, 609])\n",
            "   attention_mask: torch.Size([4, 609])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 609])\n",
            "\n",
            "üîπ Training Step 1152/1460\n",
            "   input_ids: torch.Size([4, 606])\n",
            "   attention_mask: torch.Size([4, 606])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 606])\n",
            "\n",
            "üîπ Training Step 1153/1460\n",
            "   input_ids: torch.Size([4, 595])\n",
            "   attention_mask: torch.Size([4, 595])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 595])\n",
            "\n",
            "üîπ Training Step 1154/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 1155/1460\n",
            "   input_ids: torch.Size([4, 601])\n",
            "   attention_mask: torch.Size([4, 601])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 601])\n",
            "\n",
            "üîπ Training Step 1156/1460\n",
            "   input_ids: torch.Size([4, 601])\n",
            "   attention_mask: torch.Size([4, 601])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 601])\n",
            "\n",
            "üîπ Training Step 1157/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1158/1460\n",
            "   input_ids: torch.Size([4, 601])\n",
            "   attention_mask: torch.Size([4, 601])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 601])\n",
            "\n",
            "üîπ Training Step 1159/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1160/1460\n",
            "   input_ids: torch.Size([4, 601])\n",
            "   attention_mask: torch.Size([4, 601])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 601])\n",
            "\n",
            "üîπ Training Step 1161/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 1162/1460\n",
            "   input_ids: torch.Size([4, 608])\n",
            "   attention_mask: torch.Size([4, 608])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 608])\n",
            "\n",
            "üîπ Training Step 1163/1460\n",
            "   input_ids: torch.Size([4, 604])\n",
            "   attention_mask: torch.Size([4, 604])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 604])\n",
            "\n",
            "üîπ Training Step 1164/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1165/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "üîπ Training Step 1166/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 1167/1460\n",
            "   input_ids: torch.Size([4, 610])\n",
            "   attention_mask: torch.Size([4, 610])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 610])\n",
            "\n",
            "üîπ Training Step 1168/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 1169/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1170/1460\n",
            "   input_ids: torch.Size([4, 595])\n",
            "   attention_mask: torch.Size([4, 595])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 595])\n",
            "\n",
            "üîπ Training Step 1171/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1172/1460\n",
            "   input_ids: torch.Size([4, 601])\n",
            "   attention_mask: torch.Size([4, 601])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 601])\n",
            "\n",
            "üîπ Training Step 1173/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 1174/1460\n",
            "   input_ids: torch.Size([4, 607])\n",
            "   attention_mask: torch.Size([4, 607])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 607])\n",
            "\n",
            "üîπ Training Step 1175/1460\n",
            "   input_ids: torch.Size([4, 610])\n",
            "   attention_mask: torch.Size([4, 610])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 610])\n",
            "\n",
            "üîπ Training Step 1176/1460\n",
            "   input_ids: torch.Size([4, 605])\n",
            "   attention_mask: torch.Size([4, 605])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 605])\n",
            "\n",
            "üîπ Training Step 1177/1460\n",
            "   input_ids: torch.Size([4, 605])\n",
            "   attention_mask: torch.Size([4, 605])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 605])\n",
            "\n",
            "üîπ Training Step 1178/1460\n",
            "   input_ids: torch.Size([4, 602])\n",
            "   attention_mask: torch.Size([4, 602])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 602])\n",
            "\n",
            "üîπ Training Step 1179/1460\n",
            "   input_ids: torch.Size([4, 601])\n",
            "   attention_mask: torch.Size([4, 601])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 601])\n",
            "\n",
            "üîπ Training Step 1180/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 1181/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 1182/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1183/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1184/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1185/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 1186/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1187/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 1188/1460\n",
            "   input_ids: torch.Size([4, 614])\n",
            "   attention_mask: torch.Size([4, 614])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 614])\n",
            "\n",
            "üîπ Training Step 1189/1460\n",
            "   input_ids: torch.Size([4, 607])\n",
            "   attention_mask: torch.Size([4, 607])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 607])\n",
            "\n",
            "üîπ Training Step 1190/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1191/1460\n",
            "   input_ids: torch.Size([4, 607])\n",
            "   attention_mask: torch.Size([4, 607])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 607])\n",
            "\n",
            "üîπ Training Step 1192/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1193/1460\n",
            "   input_ids: torch.Size([4, 595])\n",
            "   attention_mask: torch.Size([4, 595])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 595])\n",
            "\n",
            "üîπ Training Step 1194/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1195/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1196/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1197/1460\n",
            "   input_ids: torch.Size([4, 609])\n",
            "   attention_mask: torch.Size([4, 609])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 609])\n",
            "\n",
            "üîπ Training Step 1198/1460\n",
            "   input_ids: torch.Size([4, 611])\n",
            "   attention_mask: torch.Size([4, 611])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 611])\n",
            "\n",
            "üîπ Training Step 1199/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 1200/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 1201/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "üîπ Training Step 1202/1460\n",
            "   input_ids: torch.Size([4, 601])\n",
            "   attention_mask: torch.Size([4, 601])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 601])\n",
            "\n",
            "üîπ Training Step 1203/1460\n",
            "   input_ids: torch.Size([4, 601])\n",
            "   attention_mask: torch.Size([4, 601])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 601])\n",
            "\n",
            "üîπ Training Step 1204/1460\n",
            "   input_ids: torch.Size([4, 602])\n",
            "   attention_mask: torch.Size([4, 602])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 602])\n",
            "\n",
            "üîπ Training Step 1205/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 1206/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1207/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 1208/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 1209/1460\n",
            "   input_ids: torch.Size([4, 595])\n",
            "   attention_mask: torch.Size([4, 595])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 595])\n",
            "\n",
            "üîπ Training Step 1210/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "üîπ Training Step 1211/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 1212/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 1213/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1214/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 1215/1460\n",
            "   input_ids: torch.Size([4, 607])\n",
            "   attention_mask: torch.Size([4, 607])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 607])\n",
            "\n",
            "üîπ Training Step 1216/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1217/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "üîπ Training Step 1218/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1219/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "üîπ Training Step 1220/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1221/1460\n",
            "   input_ids: torch.Size([4, 608])\n",
            "   attention_mask: torch.Size([4, 608])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 608])\n",
            "\n",
            "üîπ Training Step 1222/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 1223/1460\n",
            "   input_ids: torch.Size([4, 604])\n",
            "   attention_mask: torch.Size([4, 604])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 604])\n",
            "\n",
            "üîπ Training Step 1224/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1225/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 1226/1460\n",
            "   input_ids: torch.Size([4, 595])\n",
            "   attention_mask: torch.Size([4, 595])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 595])\n",
            "\n",
            "üîπ Training Step 1227/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 1228/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1229/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 1230/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 1231/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1232/1460\n",
            "   input_ids: torch.Size([4, 609])\n",
            "   attention_mask: torch.Size([4, 609])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 609])\n",
            "\n",
            "üîπ Training Step 1233/1460\n",
            "   input_ids: torch.Size([4, 610])\n",
            "   attention_mask: torch.Size([4, 610])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 610])\n",
            "\n",
            "üîπ Training Step 1234/1460\n",
            "   input_ids: torch.Size([4, 595])\n",
            "   attention_mask: torch.Size([4, 595])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 595])\n",
            "\n",
            "üîπ Training Step 1235/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1236/1460\n",
            "   input_ids: torch.Size([4, 616])\n",
            "   attention_mask: torch.Size([4, 616])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 616])\n",
            "\n",
            "üîπ Training Step 1237/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1238/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1239/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 1240/1460\n",
            "   input_ids: torch.Size([4, 607])\n",
            "   attention_mask: torch.Size([4, 607])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 607])\n",
            "\n",
            "üîπ Training Step 1241/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 1242/1460\n",
            "   input_ids: torch.Size([4, 605])\n",
            "   attention_mask: torch.Size([4, 605])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 605])\n",
            "\n",
            "üîπ Training Step 1243/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 1244/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 1245/1460\n",
            "   input_ids: torch.Size([4, 612])\n",
            "   attention_mask: torch.Size([4, 612])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 612])\n",
            "\n",
            "üîπ Training Step 1246/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1247/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "üîπ Training Step 1248/1460\n",
            "   input_ids: torch.Size([4, 617])\n",
            "   attention_mask: torch.Size([4, 617])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 617])\n",
            "\n",
            "üîπ Training Step 1249/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1250/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1251/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 1252/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1253/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1254/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 1255/1460\n",
            "   input_ids: torch.Size([4, 611])\n",
            "   attention_mask: torch.Size([4, 611])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 611])\n",
            "\n",
            "üîπ Training Step 1256/1460\n",
            "   input_ids: torch.Size([4, 608])\n",
            "   attention_mask: torch.Size([4, 608])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 608])\n",
            "\n",
            "üîπ Training Step 1257/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 1258/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1259/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 1260/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 1261/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1262/1460\n",
            "   input_ids: torch.Size([4, 613])\n",
            "   attention_mask: torch.Size([4, 613])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 613])\n",
            "\n",
            "üîπ Training Step 1263/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "üîπ Training Step 1264/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1265/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1266/1460\n",
            "   input_ids: torch.Size([4, 610])\n",
            "   attention_mask: torch.Size([4, 610])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 610])\n",
            "\n",
            "üîπ Training Step 1267/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "üîπ Training Step 1268/1460\n",
            "   input_ids: torch.Size([4, 614])\n",
            "   attention_mask: torch.Size([4, 614])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 614])\n",
            "\n",
            "üîπ Training Step 1269/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "üîπ Training Step 1270/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1271/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "üîπ Training Step 1272/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 1273/1460\n",
            "   input_ids: torch.Size([4, 606])\n",
            "   attention_mask: torch.Size([4, 606])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 606])\n",
            "\n",
            "üîπ Training Step 1274/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 1275/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 1276/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1277/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "üîπ Training Step 1278/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 1279/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1280/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1281/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1282/1460\n",
            "   input_ids: torch.Size([4, 616])\n",
            "   attention_mask: torch.Size([4, 616])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 616])\n",
            "\n",
            "üîπ Training Step 1283/1460\n",
            "   input_ids: torch.Size([4, 604])\n",
            "   attention_mask: torch.Size([4, 604])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 604])\n",
            "\n",
            "üîπ Training Step 1284/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1285/1460\n",
            "   input_ids: torch.Size([4, 604])\n",
            "   attention_mask: torch.Size([4, 604])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 604])\n",
            "\n",
            "üîπ Training Step 1286/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1287/1460\n",
            "   input_ids: torch.Size([4, 609])\n",
            "   attention_mask: torch.Size([4, 609])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 609])\n",
            "\n",
            "üîπ Training Step 1288/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 1289/1460\n",
            "   input_ids: torch.Size([4, 611])\n",
            "   attention_mask: torch.Size([4, 611])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 611])\n",
            "\n",
            "üîπ Training Step 1290/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1291/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 1292/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1293/1460\n",
            "   input_ids: torch.Size([4, 595])\n",
            "   attention_mask: torch.Size([4, 595])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 595])\n",
            "\n",
            "üîπ Training Step 1294/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 1295/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1296/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "üîπ Training Step 1297/1460\n",
            "   input_ids: torch.Size([4, 608])\n",
            "   attention_mask: torch.Size([4, 608])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 608])\n",
            "\n",
            "üîπ Training Step 1298/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1299/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 1300/1460\n",
            "   input_ids: torch.Size([4, 610])\n",
            "   attention_mask: torch.Size([4, 610])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 610])\n",
            "\n",
            "üîπ Training Step 1301/1460\n",
            "   input_ids: torch.Size([4, 601])\n",
            "   attention_mask: torch.Size([4, 601])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 601])\n",
            "\n",
            "üîπ Training Step 1302/1460\n",
            "   input_ids: torch.Size([4, 609])\n",
            "   attention_mask: torch.Size([4, 609])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 609])\n",
            "\n",
            "üîπ Training Step 1303/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 1304/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "üîπ Training Step 1305/1460\n",
            "   input_ids: torch.Size([4, 604])\n",
            "   attention_mask: torch.Size([4, 604])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 604])\n",
            "\n",
            "üîπ Training Step 1306/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 1307/1460\n",
            "   input_ids: torch.Size([4, 606])\n",
            "   attention_mask: torch.Size([4, 606])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 606])\n",
            "\n",
            "üîπ Training Step 1308/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1309/1460\n",
            "   input_ids: torch.Size([4, 608])\n",
            "   attention_mask: torch.Size([4, 608])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 608])\n",
            "\n",
            "üîπ Training Step 1310/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1311/1460\n",
            "   input_ids: torch.Size([4, 611])\n",
            "   attention_mask: torch.Size([4, 611])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 611])\n",
            "\n",
            "üîπ Training Step 1312/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 1313/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1314/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 1315/1460\n",
            "   input_ids: torch.Size([4, 595])\n",
            "   attention_mask: torch.Size([4, 595])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 595])\n",
            "\n",
            "üîπ Training Step 1316/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1317/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "üîπ Training Step 1318/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "üîπ Training Step 1319/1460\n",
            "   input_ids: torch.Size([4, 595])\n",
            "   attention_mask: torch.Size([4, 595])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 595])\n",
            "\n",
            "üîπ Training Step 1320/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "üîπ Training Step 1321/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1322/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 1323/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 1324/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 1325/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1326/1460\n",
            "   input_ids: torch.Size([4, 604])\n",
            "   attention_mask: torch.Size([4, 604])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 604])\n",
            "\n",
            "üîπ Training Step 1327/1460\n",
            "   input_ids: torch.Size([4, 610])\n",
            "   attention_mask: torch.Size([4, 610])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 610])\n",
            "\n",
            "üîπ Training Step 1328/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 1329/1460\n",
            "   input_ids: torch.Size([4, 602])\n",
            "   attention_mask: torch.Size([4, 602])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 602])\n",
            "\n",
            "üîπ Training Step 1330/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 1331/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 1332/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1333/1460\n",
            "   input_ids: torch.Size([4, 610])\n",
            "   attention_mask: torch.Size([4, 610])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 610])\n",
            "\n",
            "üîπ Training Step 1334/1460\n",
            "   input_ids: torch.Size([4, 608])\n",
            "   attention_mask: torch.Size([4, 608])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 608])\n",
            "\n",
            "üîπ Training Step 1335/1460\n",
            "   input_ids: torch.Size([4, 612])\n",
            "   attention_mask: torch.Size([4, 612])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 612])\n",
            "\n",
            "üîπ Training Step 1336/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1337/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1338/1460\n",
            "   input_ids: torch.Size([4, 614])\n",
            "   attention_mask: torch.Size([4, 614])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 614])\n",
            "\n",
            "üîπ Training Step 1339/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1340/1460\n",
            "   input_ids: torch.Size([4, 608])\n",
            "   attention_mask: torch.Size([4, 608])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 608])\n",
            "\n",
            "üîπ Training Step 1341/1460\n",
            "   input_ids: torch.Size([4, 611])\n",
            "   attention_mask: torch.Size([4, 611])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 611])\n",
            "\n",
            "üîπ Training Step 1342/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1343/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1344/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1345/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1346/1460\n",
            "   input_ids: torch.Size([4, 608])\n",
            "   attention_mask: torch.Size([4, 608])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 608])\n",
            "\n",
            "üîπ Training Step 1347/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1348/1460\n",
            "   input_ids: torch.Size([4, 613])\n",
            "   attention_mask: torch.Size([4, 613])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 613])\n",
            "\n",
            "üîπ Training Step 1349/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1350/1460\n",
            "   input_ids: torch.Size([4, 612])\n",
            "   attention_mask: torch.Size([4, 612])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 612])\n",
            "\n",
            "üîπ Training Step 1351/1460\n",
            "   input_ids: torch.Size([4, 608])\n",
            "   attention_mask: torch.Size([4, 608])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 608])\n",
            "\n",
            "üîπ Training Step 1352/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1353/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1354/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 1355/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 1356/1460\n",
            "   input_ids: torch.Size([4, 609])\n",
            "   attention_mask: torch.Size([4, 609])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 609])\n",
            "\n",
            "üîπ Training Step 1357/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 1358/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1359/1460\n",
            "   input_ids: torch.Size([4, 601])\n",
            "   attention_mask: torch.Size([4, 601])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 601])\n",
            "\n",
            "üîπ Training Step 1360/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1361/1460\n",
            "   input_ids: torch.Size([4, 618])\n",
            "   attention_mask: torch.Size([4, 618])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 618])\n",
            "\n",
            "üîπ Training Step 1362/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1363/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 1364/1460\n",
            "   input_ids: torch.Size([4, 605])\n",
            "   attention_mask: torch.Size([4, 605])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 605])\n",
            "\n",
            "üîπ Training Step 1365/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1366/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1367/1460\n",
            "   input_ids: torch.Size([4, 595])\n",
            "   attention_mask: torch.Size([4, 595])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 595])\n",
            "\n",
            "üîπ Training Step 1368/1460\n",
            "   input_ids: torch.Size([4, 607])\n",
            "   attention_mask: torch.Size([4, 607])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 607])\n",
            "\n",
            "üîπ Training Step 1369/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1370/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1371/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1372/1460\n",
            "   input_ids: torch.Size([4, 608])\n",
            "   attention_mask: torch.Size([4, 608])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 608])\n",
            "\n",
            "üîπ Training Step 1373/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1374/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 1375/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1376/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1377/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1378/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "üîπ Training Step 1379/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1380/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 1381/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 1382/1460\n",
            "   input_ids: torch.Size([4, 609])\n",
            "   attention_mask: torch.Size([4, 609])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 609])\n",
            "\n",
            "üîπ Training Step 1383/1460\n",
            "   input_ids: torch.Size([4, 603])\n",
            "   attention_mask: torch.Size([4, 603])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 603])\n",
            "\n",
            "üîπ Training Step 1384/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1385/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1386/1460\n",
            "   input_ids: torch.Size([4, 601])\n",
            "   attention_mask: torch.Size([4, 601])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 601])\n",
            "\n",
            "üîπ Training Step 1387/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "üîπ Training Step 1388/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1389/1460\n",
            "   input_ids: torch.Size([4, 610])\n",
            "   attention_mask: torch.Size([4, 610])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 610])\n",
            "\n",
            "üîπ Training Step 1390/1460\n",
            "   input_ids: torch.Size([4, 610])\n",
            "   attention_mask: torch.Size([4, 610])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 610])\n",
            "\n",
            "üîπ Training Step 1391/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 1392/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1393/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 1394/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1395/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 1396/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 1397/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "üîπ Training Step 1398/1460\n",
            "   input_ids: torch.Size([4, 602])\n",
            "   attention_mask: torch.Size([4, 602])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 602])\n",
            "\n",
            "üîπ Training Step 1399/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1400/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "üîπ Training Step 1401/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1402/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 1403/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1404/1460\n",
            "   input_ids: torch.Size([4, 595])\n",
            "   attention_mask: torch.Size([4, 595])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 595])\n",
            "\n",
            "üîπ Training Step 1405/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 1406/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 1407/1460\n",
            "   input_ids: torch.Size([4, 601])\n",
            "   attention_mask: torch.Size([4, 601])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 601])\n",
            "\n",
            "üîπ Training Step 1408/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1409/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 1410/1460\n",
            "   input_ids: torch.Size([4, 609])\n",
            "   attention_mask: torch.Size([4, 609])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 609])\n",
            "\n",
            "üîπ Training Step 1411/1460\n",
            "   input_ids: torch.Size([4, 601])\n",
            "   attention_mask: torch.Size([4, 601])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 601])\n",
            "\n",
            "üîπ Training Step 1412/1460\n",
            "   input_ids: torch.Size([4, 594])\n",
            "   attention_mask: torch.Size([4, 594])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 594])\n",
            "\n",
            "üîπ Training Step 1413/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "üîπ Training Step 1414/1460\n",
            "   input_ids: torch.Size([4, 610])\n",
            "   attention_mask: torch.Size([4, 610])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 610])\n",
            "\n",
            "üîπ Training Step 1415/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1416/1460\n",
            "   input_ids: torch.Size([4, 603])\n",
            "   attention_mask: torch.Size([4, 603])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 603])\n",
            "\n",
            "üîπ Training Step 1417/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1418/1460\n",
            "   input_ids: torch.Size([4, 601])\n",
            "   attention_mask: torch.Size([4, 601])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 601])\n",
            "\n",
            "üîπ Training Step 1419/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1420/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1421/1460\n",
            "   input_ids: torch.Size([4, 594])\n",
            "   attention_mask: torch.Size([4, 594])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 594])\n",
            "\n",
            "üîπ Training Step 1422/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1423/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1424/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1425/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 1426/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 1427/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1428/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 1429/1460\n",
            "   input_ids: torch.Size([4, 605])\n",
            "   attention_mask: torch.Size([4, 605])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 605])\n",
            "\n",
            "üîπ Training Step 1430/1460\n",
            "   input_ids: torch.Size([4, 603])\n",
            "   attention_mask: torch.Size([4, 603])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 603])\n",
            "\n",
            "üîπ Training Step 1431/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "üîπ Training Step 1432/1460\n",
            "   input_ids: torch.Size([4, 611])\n",
            "   attention_mask: torch.Size([4, 611])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 611])\n",
            "\n",
            "üîπ Training Step 1433/1460\n",
            "   input_ids: torch.Size([4, 594])\n",
            "   attention_mask: torch.Size([4, 594])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 594])\n",
            "\n",
            "üîπ Training Step 1434/1460\n",
            "   input_ids: torch.Size([4, 607])\n",
            "   attention_mask: torch.Size([4, 607])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 607])\n",
            "\n",
            "üîπ Training Step 1435/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1436/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1437/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 1438/1460\n",
            "   input_ids: torch.Size([4, 607])\n",
            "   attention_mask: torch.Size([4, 607])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 607])\n",
            "\n",
            "üîπ Training Step 1439/1460\n",
            "   input_ids: torch.Size([4, 611])\n",
            "   attention_mask: torch.Size([4, 611])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 611])\n",
            "\n",
            "üîπ Training Step 1440/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "üîπ Training Step 1441/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 1442/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1443/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1444/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1445/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 1446/1460\n",
            "   input_ids: torch.Size([4, 609])\n",
            "   attention_mask: torch.Size([4, 609])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 609])\n",
            "\n",
            "üîπ Training Step 1447/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 1448/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1449/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 1450/1460\n",
            "   input_ids: torch.Size([4, 595])\n",
            "   attention_mask: torch.Size([4, 595])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 595])\n",
            "\n",
            "üîπ Training Step 1451/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1452/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "üîπ Training Step 1453/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 1454/1460\n",
            "   input_ids: torch.Size([4, 611])\n",
            "   attention_mask: torch.Size([4, 611])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 611])\n",
            "\n",
            "üîπ Training Step 1455/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "üîπ Training Step 1456/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "üîπ Training Step 1457/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "üîπ Training Step 1458/1460\n",
            "   input_ids: torch.Size([4, 595])\n",
            "   attention_mask: torch.Size([4, 595])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 595])\n",
            "\n",
            "üîπ Training Step 1459/1460\n",
            "   input_ids: torch.Size([2, 595])\n",
            "   attention_mask: torch.Size([2, 595])\n",
            "   pixel_values: torch.Size([2, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([2, 3])\n",
            "   labels: torch.Size([2, 595])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from transformers import AutoProcessor\n",
        "\n",
        "# Define dataset path\n",
        "image_folder = \"/content/Compliance_model_data\"\n",
        "batch_size = 4\n",
        "max_length = 512\n",
        "image_size = (336, 336)\n",
        "\n",
        "# Initialize full dataset\n",
        "full_dataset = LLaVADataset(image_folder, processor, max_length=max_length, image_size=image_size)\n",
        "\n",
        "# **Split into Train and Validation Sets (80% Train, 20% Validation)**\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_dataset, eval_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# Initialize DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, pin_memory=False)\n",
        "eval_loader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, pin_memory=False)\n",
        "\n",
        "# Verify batch structure (Debugging)\n",
        "for step, batch in enumerate(train_loader):\n",
        "    if batch is None:\n",
        "        print(f\"‚ö†Ô∏è Skipping empty batch at step {step}\")\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nüîπ Training Step {step}/{len(train_loader)}\")\n",
        "    for key, value in batch.items():\n",
        "        print(f\"   {key}: {value.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "J52VNSx4-Kaq",
        "outputId": "9d706759-23c8-47ba-f5b1-fdb024d317d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç **Dataset Sample Keys**: dict_keys(['input_ids', 'attention_mask', 'pixel_values', 'image_grid_thw', 'labels'])\n",
            "üîπ input_ids: <class 'torch.Tensor'>, Shape: torch.Size([595])\n",
            "üîπ attention_mask: <class 'torch.Tensor'>, Shape: torch.Size([595])\n",
            "üîπ pixel_values: <class 'torch.Tensor'>, Shape: torch.Size([3, 336, 336])\n",
            "üîπ image_grid_thw: <class 'torch.Tensor'>, Shape: torch.Size([3])\n",
            "üîπ labels: <class 'torch.Tensor'>, Shape: torch.Size([595])\n"
          ]
        }
      ],
      "source": [
        "# Get a sample from the dataset\n",
        "sample = train_dataset[99]\n",
        "\n",
        "# Print available keys in the dataset sample\n",
        "print(\"üîç **Dataset Sample Keys**:\", sample.keys())\n",
        "\n",
        "# Print the shape and type of each key\n",
        "for key, value in sample.items():\n",
        "    print(f\"üîπ {key}: {type(value)}, Shape: {value.shape if isinstance(value, torch.Tensor) else 'N/A'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgU2Z64m6mrY",
        "outputId": "00f3353a-9ba5-453b-b842-c078e4e5cd87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Model and processor loaded successfully on GPU with LoRA!\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoProcessor, LlavaForConditionalGeneration, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "# Enable memory optimization\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=4, #previously its 8\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.enable_input_require_grads()\n",
        "model.config.use_cache = False  # Important for gradient checkpointing\n",
        "\n",
        "print(f\"\\n‚úÖ Model and processor loaded successfully on GPU with LoRA!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQZT3sfj_MVG"
      },
      "outputs": [],
      "source": [
        "# for name, param in model.named_parameters():\n",
        "#     if param.requires_grad:\n",
        "#         print(name)\n",
        "\n",
        "# for name, param in model.named_parameters():\n",
        "#     print(f\"{name}: requires_grad = {param.requires_grad}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruzaqzzWK0Z8"
      },
      "source": [
        "#free cuda before training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egQvLgLzK3KI",
        "outputId": "f793b4ca-f48b-4980-fb95-2400e42e3fa4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "80"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gc\n",
        "import torch\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnCbktK4BLx2"
      },
      "source": [
        "#Original training code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "KM3ZvqlD7F5n",
        "outputId": "7e8d3771-932b-422e-86fe-d71ca5cb0d70"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-15-7083241d0146>:100: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10/10 03:34, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>76.865400</td>\n",
              "      <td>9.517768</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model and necessary components saved at /content/Fine_tuned_llava_model\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    EarlyStoppingCallback,\n",
        ")\n",
        "\n",
        "# 1) Define TrainingArguments with Early Stopping\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/Fine_tuned_llava_model\",\n",
        "    per_device_train_batch_size=1,  # Adjust based on available GPU memory\n",
        "    gradient_accumulation_steps=8,\n",
        "    max_grad_norm=1.0,\n",
        "    gradient_checkpointing=True,\n",
        "    logging_strategy=\"steps\",\n",
        "    max_steps=10,\n",
        "    logging_steps=10,  # Logs every 10 steps\n",
        "    evaluation_strategy=\"epoch\",  # Evaluate at the end of each epoch\n",
        "    save_strategy=\"epoch\",\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    fp16=True,  # Enables mixed precision for speed\n",
        "    bf16=False,\n",
        "    push_to_hub=False,\n",
        "    logging_dir=\"/content/logs\",\n",
        "    remove_unused_columns=False,\n",
        "    label_names=[\"labels\"],\n",
        "    dataloader_num_workers=0,\n",
        "    save_total_limit=1,  # Keeps only the last 2 checkpoints\n",
        "    load_best_model_at_end=True,  # Loads best model after training\n",
        "    metric_for_best_model=\"loss\"\n",
        ")\n",
        "\n",
        "# 2) Create a Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=processor.tokenizer,\n",
        "    data_collator=collate_fn,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
        ")\n",
        "\n",
        "# 3) Train model\n",
        "trainer.train()\n",
        "\n",
        "# 4) Save all components for reloading\n",
        "save_path = \"/content/Fine_tuned_llava_model\"\n",
        "trainer.save_model(save_path)\n",
        "processor.tokenizer.save_pretrained(save_path)\n",
        "processor.save_pretrained(save_path)\n",
        "torch.save(training_args, f\"{save_path}/training_args.bin\")\n",
        "torch.save(trainer.state.optimizer.state_dict(), f\"{save_path}/optimizer_state.pt\")\n",
        "\n",
        "print(f\"Model and necessary components saved at {save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "cbb0310da632478a9dad8cdeafe909fe",
            "2f19f4f8835a4b3b98c546875d1b6d39",
            "8c86469fb635495bb3d157dfb67616e6",
            "bd0db7d34f334506833116528043d6a5",
            "1d360b6ed5394177bfd89c7805e1a9d3",
            "ac21d37ec70548e385d1b6466e532dee",
            "7237fb6d1ff049fca43045a0a6b3c496",
            "374236e0ef7d4e9888ed3e60972fded7",
            "c30f654164c0437f993d7ea2bbaa77fa",
            "c665d8ec1c4d490488696f2348e6faf8",
            "cf91abe9cb2b482e86a2c494d08eb82d"
          ]
        },
        "id": "pUFSz7_dPK7K",
        "outputId": "e592a0de-4986-43ac-de49-31b32f9bb8e5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cbb0310da632478a9dad8cdeafe909fe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import LlavaForConditionalGeneration, AutoProcessor\n",
        "\n",
        "# Define the path where the fine-tuned model is saved\n",
        "save_path = \"/content/Fine_tuned_llava_model\"\n",
        "\n",
        "# Load the fine-tuned LLaVA-13B model\n",
        "model_reloaded = LlavaForConditionalGeneration.from_pretrained(save_path)\n",
        "processor_reloaded = AutoProcessor.from_pretrained(save_path)\n",
        "tokenizer_reloaded = processor_reloaded.tokenizer  # Tokenizer is part of the processor\n",
        "\n",
        "print(\"Fine-tuned LLaVA-13B model reloaded successfully!\")\n",
        "\n",
        "# ---- Load the Image ----\n",
        "image_path = \"/content/violant_image.jpg\"  # Change this to your actual image file path\n",
        "image = Image.open(image_path).convert(\"RGB\")  # Convert to RGB format\n",
        "print(\"Image:\",image)\n",
        "\n",
        "\n",
        "# ---- Process the Image ----\n",
        "inputs = processor_reloaded(image, return_tensors=\"pt\")\n",
        "\n",
        "# Move to GPU if available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_reloaded.to(device)\n",
        "inputs = {key: val.to(device) for key, val in inputs.items()}\n",
        "\n",
        "# ---- Generate Description ----\n",
        "with torch.no_grad():\n",
        "    generated_ids = model_reloaded.generate(**inputs, max_length=50)\n",
        "\n",
        "# Decode the generated output\n",
        "generated_text = tokenizer_reloaded.decode(generated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Generated Description:\", generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVQjH1N0i3OD",
        "outputId": "1c391c0a-0681-4f07-8d22-69f16de1476f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It is Returning\n",
            "\n",
            "üîç Extracted Required Image Tokens: 577\n",
            "577\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "def extract_required_image_tokens(model, dataset, device=\"cuda\"):\n",
        "    \"\"\"Extract image features from the model to determine required `<image>` tokens.\"\"\"\n",
        "\n",
        "    # Get a single sample from the dataset\n",
        "    sample = dataset[0]\n",
        "    pixel_values = sample[\"pixel_values\"].unsqueeze(0).to(device)  # Add batch dimension\n",
        "\n",
        "    # Ensure no text input is given\n",
        "    with torch.no_grad():\n",
        "        image_features = model.vision_tower(pixel_values)\n",
        "\n",
        "    # Handle the output structure of vision_tower\n",
        "    if isinstance(image_features, torch.Tensor):\n",
        "        num_image_features = image_features.shape[1] * image_features.shape[2]\n",
        "    elif hasattr(image_features, \"last_hidden_state\"):\n",
        "        num_image_features = image_features.last_hidden_state.shape[1]\n",
        "    else:\n",
        "        raise ValueError(\"‚ùå Could not extract valid image features. Debug model output.\")\n",
        "\n",
        "    print(f\"\\nüîç Extracted Required Image Tokens: {num_image_features}\")\n",
        "    return num_image_features\n",
        "\n",
        "# Run feature extraction\n",
        "num_required_image_tokens = extract_required_image_tokens(model, dataset)\n",
        "print(num_required_image_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "1b053694608b467882f797ce1332da34",
            "48a12a36c77b47b3a962fb7e3f0eede5",
            "db4afb94aa304bf197ceb9518fc175e5",
            "124acec5653c40e89b4687bc4d6caa5f",
            "af421f24a5a84bb082a3f2e2d02d91b6",
            "94a010bf063146b3b11e1f03daab5d70",
            "f02e47abf8514c6781562c6e1c335525",
            "74c89dbfa8254809be3ae0119ee4dad7",
            "097ae431e07946b98dce00faffb3ec9a",
            "fd479b74ae6e4bcd8cdd2e3cd83be7f9",
            "b02a1abae8cf44b6ac907d2bed3d468c"
          ]
        },
        "id": "hgeFJRQf4xGA",
        "outputId": "aa79a746-e8f2-4e3a-d507-928b03fd863d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1b053694608b467882f797ce1332da34",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   0%|          | 0/1825 [00:00<?, ?it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   0%|          | 1/1825 [00:00<06:57,  4.37it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   0%|          | 3/1825 [00:00<03:18,  9.17it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   0%|          | 5/1825 [00:00<02:39, 11.43it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   0%|          | 7/1825 [00:00<02:21, 12.88it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   0%|          | 9/1825 [00:00<02:12, 13.71it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   1%|          | 11/1825 [00:00<02:13, 13.62it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   1%|          | 13/1825 [00:01<02:12, 13.72it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   1%|          | 15/1825 [00:01<02:08, 14.09it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   1%|          | 17/1825 [00:01<02:09, 13.93it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   1%|          | 19/1825 [00:01<02:05, 14.42it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   1%|          | 21/1825 [00:01<02:02, 14.75it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   1%|‚ñè         | 23/1825 [00:01<02:00, 14.89it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   1%|‚ñè         | 25/1825 [00:01<01:59, 15.05it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   1%|‚ñè         | 27/1825 [00:01<01:58, 15.14it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   2%|‚ñè         | 29/1825 [00:02<01:58, 15.22it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   2%|‚ñè         | 31/1825 [00:02<01:58, 15.16it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   2%|‚ñè         | 33/1825 [00:02<02:01, 14.76it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   2%|‚ñè         | 35/1825 [00:02<02:00, 14.90it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   2%|‚ñè         | 37/1825 [00:02<01:59, 14.99it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   2%|‚ñè         | 39/1825 [00:02<01:59, 15.00it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   2%|‚ñè         | 41/1825 [00:02<02:00, 14.76it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   2%|‚ñè         | 43/1825 [00:03<02:00, 14.80it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   2%|‚ñè         | 45/1825 [00:03<01:59, 14.94it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   3%|‚ñé         | 47/1825 [00:03<02:00, 14.79it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   3%|‚ñé         | 49/1825 [00:03<02:00, 14.80it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   3%|‚ñé         | 51/1825 [00:03<01:58, 14.94it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   3%|‚ñé         | 53/1825 [00:03<01:58, 14.97it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   3%|‚ñé         | 55/1825 [00:03<01:58, 14.88it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   3%|‚ñé         | 57/1825 [00:04<02:02, 14.46it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   3%|‚ñé         | 59/1825 [00:04<02:02, 14.43it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   3%|‚ñé         | 61/1825 [00:04<01:59, 14.79it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   3%|‚ñé         | 63/1825 [00:04<02:01, 14.47it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   4%|‚ñé         | 65/1825 [00:04<02:07, 13.78it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   4%|‚ñé         | 67/1825 [00:04<02:11, 13.38it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   4%|‚ñç         | 69/1825 [00:04<02:11, 13.35it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   4%|‚ñç         | 71/1825 [00:05<02:12, 13.26it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   4%|‚ñç         | 73/1825 [00:05<02:08, 13.65it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   4%|‚ñç         | 75/1825 [00:05<02:05, 13.96it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   4%|‚ñç         | 77/1825 [00:05<02:03, 14.13it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   4%|‚ñç         | 79/1825 [00:05<02:06, 13.82it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   4%|‚ñç         | 81/1825 [00:05<02:03, 14.07it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   5%|‚ñç         | 83/1825 [00:05<02:05, 13.85it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   5%|‚ñç         | 85/1825 [00:06<02:04, 14.02it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   5%|‚ñç         | 87/1825 [00:06<02:04, 13.95it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   5%|‚ñç         | 89/1825 [00:06<02:04, 13.90it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   5%|‚ñç         | 91/1825 [00:06<02:04, 13.90it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   5%|‚ñå         | 93/1825 [00:06<02:02, 14.15it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   5%|‚ñå         | 95/1825 [00:06<02:04, 13.85it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   5%|‚ñå         | 97/1825 [00:06<02:06, 13.66it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   5%|‚ñå         | 99/1825 [00:07<02:06, 13.67it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   6%|‚ñå         | 101/1825 [00:07<02:05, 13.71it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   6%|‚ñå         | 103/1825 [00:07<02:03, 13.96it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   6%|‚ñå         | 105/1825 [00:07<02:02, 14.03it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   6%|‚ñå         | 107/1825 [00:07<02:01, 14.13it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   6%|‚ñå         | 109/1825 [00:07<02:01, 14.18it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   6%|‚ñå         | 111/1825 [00:07<02:01, 14.12it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   6%|‚ñå         | 113/1825 [00:08<02:03, 13.89it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   6%|‚ñã         | 115/1825 [00:08<01:59, 14.34it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   6%|‚ñã         | 117/1825 [00:08<01:58, 14.43it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   7%|‚ñã         | 119/1825 [00:08<01:57, 14.49it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   7%|‚ñã         | 121/1825 [00:08<02:01, 14.07it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   7%|‚ñã         | 123/1825 [00:08<02:00, 14.09it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   7%|‚ñã         | 125/1825 [00:08<02:03, 13.78it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   7%|‚ñã         | 127/1825 [00:09<02:05, 13.56it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   7%|‚ñã         | 129/1825 [00:09<02:03, 13.77it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   7%|‚ñã         | 131/1825 [00:09<02:02, 13.86it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   7%|‚ñã         | 133/1825 [00:09<01:58, 14.28it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   7%|‚ñã         | 135/1825 [00:09<01:55, 14.68it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   8%|‚ñä         | 137/1825 [00:09<01:52, 15.00it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   8%|‚ñä         | 139/1825 [00:09<01:51, 15.12it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   8%|‚ñä         | 141/1825 [00:09<01:50, 15.26it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   8%|‚ñä         | 143/1825 [00:10<01:53, 14.84it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   8%|‚ñä         | 145/1825 [00:10<01:52, 14.91it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   8%|‚ñä         | 147/1825 [00:10<01:52, 14.87it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   8%|‚ñä         | 149/1825 [00:10<01:52, 14.95it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   8%|‚ñä         | 151/1825 [00:10<01:51, 15.06it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   8%|‚ñä         | 153/1825 [00:10<01:50, 15.17it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   8%|‚ñä         | 155/1825 [00:10<01:49, 15.26it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   9%|‚ñä         | 157/1825 [00:11<01:51, 15.00it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   9%|‚ñä         | 159/1825 [00:11<01:53, 14.71it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   9%|‚ñâ         | 161/1825 [00:11<01:52, 14.81it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   9%|‚ñâ         | 163/1825 [00:11<01:54, 14.57it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   9%|‚ñâ         | 165/1825 [00:11<01:57, 14.08it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   9%|‚ñâ         | 167/1825 [00:11<01:57, 14.11it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   9%|‚ñâ         | 169/1825 [00:11<01:55, 14.40it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   9%|‚ñâ         | 171/1825 [00:12<01:53, 14.57it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   9%|‚ñâ         | 173/1825 [00:12<01:51, 14.78it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  10%|‚ñâ         | 175/1825 [00:12<01:53, 14.49it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  10%|‚ñâ         | 177/1825 [00:12<01:52, 14.71it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  10%|‚ñâ         | 179/1825 [00:12<01:51, 14.76it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  10%|‚ñâ         | 181/1825 [00:12<01:50, 14.87it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  10%|‚ñà         | 183/1825 [00:12<01:49, 14.99it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  10%|‚ñà         | 185/1825 [00:12<01:48, 15.13it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  10%|‚ñà         | 187/1825 [00:13<01:47, 15.20it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  10%|‚ñà         | 189/1825 [00:13<01:50, 14.75it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  10%|‚ñà         | 191/1825 [00:13<01:49, 14.90it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  11%|‚ñà         | 193/1825 [00:13<01:48, 15.09it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  11%|‚ñà         | 195/1825 [00:13<01:46, 15.24it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  11%|‚ñà         | 197/1825 [00:13<01:46, 15.22it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  11%|‚ñà         | 199/1825 [00:13<01:45, 15.34it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  11%|‚ñà         | 201/1825 [00:13<01:45, 15.44it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  11%|‚ñà         | 203/1825 [00:14<01:44, 15.50it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  11%|‚ñà         | 205/1825 [00:14<01:44, 15.54it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  11%|‚ñà‚ñè        | 207/1825 [00:14<01:44, 15.49it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  11%|‚ñà‚ñè        | 209/1825 [00:14<01:43, 15.57it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  12%|‚ñà‚ñè        | 211/1825 [00:14<01:44, 15.45it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  12%|‚ñà‚ñè        | 213/1825 [00:14<01:44, 15.42it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  12%|‚ñà‚ñè        | 215/1825 [00:14<01:47, 15.02it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  12%|‚ñà‚ñè        | 217/1825 [00:15<01:50, 14.50it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  12%|‚ñà‚ñè        | 219/1825 [00:15<01:50, 14.58it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  12%|‚ñà‚ñè        | 221/1825 [00:15<01:53, 14.11it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  12%|‚ñà‚ñè        | 223/1825 [00:15<01:51, 14.34it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  12%|‚ñà‚ñè        | 225/1825 [00:15<01:50, 14.51it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  12%|‚ñà‚ñè        | 227/1825 [00:15<01:48, 14.66it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  13%|‚ñà‚ñé        | 229/1825 [00:15<01:47, 14.81it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  13%|‚ñà‚ñé        | 231/1825 [00:16<01:47, 14.84it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  13%|‚ñà‚ñé        | 233/1825 [00:16<01:46, 14.91it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  13%|‚ñà‚ñé        | 235/1825 [00:16<01:46, 14.99it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  13%|‚ñà‚ñé        | 237/1825 [00:16<01:45, 15.07it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  13%|‚ñà‚ñé        | 239/1825 [00:16<01:47, 14.72it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  13%|‚ñà‚ñé        | 241/1825 [00:16<01:46, 14.90it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  13%|‚ñà‚ñé        | 243/1825 [00:16<01:45, 14.98it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  13%|‚ñà‚ñé        | 245/1825 [00:16<01:45, 15.00it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  14%|‚ñà‚ñé        | 247/1825 [00:17<01:45, 14.96it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  14%|‚ñà‚ñé        | 249/1825 [00:17<01:45, 14.89it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  14%|‚ñà‚ñç        | 251/1825 [00:17<01:46, 14.77it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  14%|‚ñà‚ñç        | 253/1825 [00:17<01:45, 14.92it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  14%|‚ñà‚ñç        | 255/1825 [00:17<01:44, 15.09it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  14%|‚ñà‚ñç        | 257/1825 [00:17<01:43, 15.12it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  14%|‚ñà‚ñç        | 259/1825 [00:17<01:44, 15.05it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  14%|‚ñà‚ñç        | 261/1825 [00:18<01:46, 14.67it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  14%|‚ñà‚ñç        | 263/1825 [00:18<01:46, 14.73it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  15%|‚ñà‚ñç        | 265/1825 [00:18<01:45, 14.79it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  15%|‚ñà‚ñç        | 267/1825 [00:18<01:46, 14.57it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  15%|‚ñà‚ñç        | 269/1825 [00:18<01:47, 14.47it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  15%|‚ñà‚ñç        | 271/1825 [00:18<01:47, 14.49it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  15%|‚ñà‚ñç        | 273/1825 [00:18<01:45, 14.73it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  15%|‚ñà‚ñå        | 275/1825 [00:18<01:46, 14.62it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  15%|‚ñà‚ñå        | 277/1825 [00:19<01:44, 14.84it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  15%|‚ñà‚ñå        | 279/1825 [00:19<01:44, 14.79it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  15%|‚ñà‚ñå        | 281/1825 [00:19<01:48, 14.23it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  16%|‚ñà‚ñå        | 283/1825 [00:19<01:46, 14.41it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  16%|‚ñà‚ñå        | 285/1825 [00:19<01:46, 14.47it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  16%|‚ñà‚ñå        | 287/1825 [00:19<01:45, 14.61it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  16%|‚ñà‚ñå        | 289/1825 [00:19<01:44, 14.77it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  16%|‚ñà‚ñå        | 291/1825 [00:20<01:42, 14.94it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  16%|‚ñà‚ñå        | 293/1825 [00:20<01:42, 14.95it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  16%|‚ñà‚ñå        | 295/1825 [00:20<01:41, 15.03it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  16%|‚ñà‚ñã        | 297/1825 [00:20<01:41, 15.08it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  16%|‚ñà‚ñã        | 299/1825 [00:20<01:40, 15.14it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  16%|‚ñà‚ñã        | 301/1825 [00:20<01:40, 15.19it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  17%|‚ñà‚ñã        | 303/1825 [00:20<01:40, 15.12it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  17%|‚ñà‚ñã        | 305/1825 [00:21<01:41, 14.97it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  17%|‚ñà‚ñã        | 307/1825 [00:21<01:41, 15.02it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  17%|‚ñà‚ñã        | 309/1825 [00:21<01:40, 15.04it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  17%|‚ñà‚ñã        | 311/1825 [00:21<01:43, 14.65it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  17%|‚ñà‚ñã        | 313/1825 [00:21<01:45, 14.38it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  17%|‚ñà‚ñã        | 315/1825 [00:21<01:46, 14.21it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  17%|‚ñà‚ñã        | 317/1825 [00:21<01:45, 14.30it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  17%|‚ñà‚ñã        | 319/1825 [00:21<01:42, 14.65it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  18%|‚ñà‚ñä        | 321/1825 [00:22<01:40, 14.91it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  18%|‚ñà‚ñä        | 323/1825 [00:22<01:39, 15.11it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  18%|‚ñà‚ñä        | 325/1825 [00:22<01:38, 15.23it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  18%|‚ñà‚ñä        | 327/1825 [00:22<01:39, 15.07it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  18%|‚ñà‚ñä        | 329/1825 [00:22<01:39, 15.09it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  18%|‚ñà‚ñä        | 331/1825 [00:22<01:38, 15.17it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  18%|‚ñà‚ñä        | 333/1825 [00:22<01:42, 14.54it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  18%|‚ñà‚ñä        | 335/1825 [00:23<01:41, 14.66it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  18%|‚ñà‚ñä        | 337/1825 [00:23<01:39, 14.91it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  19%|‚ñà‚ñä        | 339/1825 [00:23<01:40, 14.73it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  19%|‚ñà‚ñä        | 341/1825 [00:23<01:40, 14.70it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  19%|‚ñà‚ñâ        | 343/1825 [00:23<01:39, 14.85it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  19%|‚ñà‚ñâ        | 345/1825 [00:23<01:39, 14.90it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  19%|‚ñà‚ñâ        | 347/1825 [00:23<01:41, 14.61it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  19%|‚ñà‚ñâ        | 349/1825 [00:23<01:40, 14.67it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  19%|‚ñà‚ñâ        | 351/1825 [00:24<01:39, 14.81it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  19%|‚ñà‚ñâ        | 353/1825 [00:24<01:38, 14.97it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  19%|‚ñà‚ñâ        | 355/1825 [00:24<01:38, 14.96it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  20%|‚ñà‚ñâ        | 357/1825 [00:24<01:37, 15.04it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  20%|‚ñà‚ñâ        | 359/1825 [00:24<01:37, 15.11it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  20%|‚ñà‚ñâ        | 361/1825 [00:24<01:38, 14.82it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  20%|‚ñà‚ñâ        | 363/1825 [00:24<01:40, 14.48it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  20%|‚ñà‚ñà        | 365/1825 [00:25<01:39, 14.64it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  20%|‚ñà‚ñà        | 367/1825 [00:25<01:38, 14.79it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  20%|‚ñà‚ñà        | 369/1825 [00:25<01:37, 14.93it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  20%|‚ñà‚ñà        | 371/1825 [00:25<01:37, 14.87it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  20%|‚ñà‚ñà        | 373/1825 [00:25<01:39, 14.65it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  21%|‚ñà‚ñà        | 375/1825 [00:25<01:37, 14.83it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  21%|‚ñà‚ñà        | 377/1825 [00:25<01:37, 14.91it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  21%|‚ñà‚ñà        | 379/1825 [00:26<01:36, 14.97it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  21%|‚ñà‚ñà        | 381/1825 [00:26<01:35, 15.09it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  21%|‚ñà‚ñà        | 383/1825 [00:26<01:35, 15.16it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  21%|‚ñà‚ñà        | 385/1825 [00:26<01:34, 15.19it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  21%|‚ñà‚ñà        | 387/1825 [00:26<01:34, 15.16it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  21%|‚ñà‚ñà‚ñè       | 389/1825 [00:26<01:35, 15.02it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  21%|‚ñà‚ñà‚ñè       | 391/1825 [00:26<01:34, 15.15it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  22%|‚ñà‚ñà‚ñè       | 393/1825 [00:26<01:34, 15.08it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  22%|‚ñà‚ñà‚ñè       | 395/1825 [00:27<01:36, 14.85it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  22%|‚ñà‚ñà‚ñè       | 397/1825 [00:27<01:37, 14.60it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  22%|‚ñà‚ñà‚ñè       | 399/1825 [00:27<01:42, 13.94it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  22%|‚ñà‚ñà‚ñè       | 401/1825 [00:27<01:42, 13.95it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  22%|‚ñà‚ñà‚ñè       | 403/1825 [00:27<01:39, 14.34it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  22%|‚ñà‚ñà‚ñè       | 405/1825 [00:27<01:42, 13.80it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  22%|‚ñà‚ñà‚ñè       | 407/1825 [00:27<01:45, 13.46it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  22%|‚ñà‚ñà‚ñè       | 409/1825 [00:28<01:43, 13.70it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  23%|‚ñà‚ñà‚ñé       | 411/1825 [00:28<01:39, 14.21it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  23%|‚ñà‚ñà‚ñé       | 413/1825 [00:28<01:38, 14.33it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  23%|‚ñà‚ñà‚ñé       | 415/1825 [00:28<01:36, 14.59it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  23%|‚ñà‚ñà‚ñé       | 417/1825 [00:28<01:39, 14.21it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  23%|‚ñà‚ñà‚ñé       | 419/1825 [00:28<01:40, 13.98it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  23%|‚ñà‚ñà‚ñé       | 421/1825 [00:28<01:40, 14.02it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  23%|‚ñà‚ñà‚ñé       | 423/1825 [00:29<01:37, 14.37it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  23%|‚ñà‚ñà‚ñé       | 425/1825 [00:29<01:36, 14.47it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  23%|‚ñà‚ñà‚ñé       | 427/1825 [00:29<01:37, 14.28it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  24%|‚ñà‚ñà‚ñé       | 429/1825 [00:29<01:40, 13.95it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  24%|‚ñà‚ñà‚ñé       | 431/1825 [00:29<01:39, 13.95it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  24%|‚ñà‚ñà‚ñé       | 433/1825 [00:29<01:37, 14.34it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  24%|‚ñà‚ñà‚ñç       | 435/1825 [00:29<01:36, 14.37it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  24%|‚ñà‚ñà‚ñç       | 437/1825 [00:30<01:37, 14.24it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  24%|‚ñà‚ñà‚ñç       | 439/1825 [00:30<01:38, 14.07it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  24%|‚ñà‚ñà‚ñç       | 441/1825 [00:30<01:35, 14.44it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  24%|‚ñà‚ñà‚ñç       | 443/1825 [00:30<01:35, 14.53it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  24%|‚ñà‚ñà‚ñç       | 445/1825 [00:30<01:34, 14.67it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  24%|‚ñà‚ñà‚ñç       | 447/1825 [00:30<01:33, 14.79it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  25%|‚ñà‚ñà‚ñç       | 449/1825 [00:30<01:32, 14.85it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  25%|‚ñà‚ñà‚ñç       | 451/1825 [00:30<01:33, 14.73it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  25%|‚ñà‚ñà‚ñç       | 453/1825 [00:31<01:33, 14.69it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  25%|‚ñà‚ñà‚ñç       | 455/1825 [00:31<01:34, 14.57it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  25%|‚ñà‚ñà‚ñå       | 457/1825 [00:31<01:32, 14.76it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  25%|‚ñà‚ñà‚ñå       | 459/1825 [00:31<01:34, 14.40it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  25%|‚ñà‚ñà‚ñå       | 461/1825 [00:31<01:35, 14.29it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  25%|‚ñà‚ñà‚ñå       | 463/1825 [00:31<01:34, 14.45it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  25%|‚ñà‚ñà‚ñå       | 465/1825 [00:31<01:32, 14.65it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  26%|‚ñà‚ñà‚ñå       | 467/1825 [00:32<01:32, 14.74it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  26%|‚ñà‚ñà‚ñå       | 469/1825 [00:32<01:30, 14.97it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  26%|‚ñà‚ñà‚ñå       | 471/1825 [00:32<01:34, 14.37it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  26%|‚ñà‚ñà‚ñå       | 473/1825 [00:32<01:33, 14.41it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  26%|‚ñà‚ñà‚ñå       | 475/1825 [00:32<01:32, 14.67it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  26%|‚ñà‚ñà‚ñå       | 477/1825 [00:32<01:31, 14.77it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  26%|‚ñà‚ñà‚ñå       | 479/1825 [00:32<01:30, 14.93it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  26%|‚ñà‚ñà‚ñã       | 481/1825 [00:33<01:30, 14.77it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  26%|‚ñà‚ñà‚ñã       | 483/1825 [00:33<01:32, 14.48it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  27%|‚ñà‚ñà‚ñã       | 485/1825 [00:33<01:33, 14.34it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  27%|‚ñà‚ñà‚ñã       | 487/1825 [00:33<01:30, 14.73it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  27%|‚ñà‚ñà‚ñã       | 489/1825 [00:33<01:29, 14.89it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  27%|‚ñà‚ñà‚ñã       | 491/1825 [00:33<01:29, 14.98it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  27%|‚ñà‚ñà‚ñã       | 493/1825 [00:33<01:28, 15.01it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  27%|‚ñà‚ñà‚ñã       | 495/1825 [00:33<01:29, 14.81it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  27%|‚ñà‚ñà‚ñã       | 497/1825 [00:34<01:29, 14.78it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  27%|‚ñà‚ñà‚ñã       | 499/1825 [00:34<01:28, 14.98it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  27%|‚ñà‚ñà‚ñã       | 501/1825 [00:34<01:27, 15.08it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  28%|‚ñà‚ñà‚ñä       | 503/1825 [00:34<01:28, 15.01it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  28%|‚ñà‚ñà‚ñä       | 505/1825 [00:34<01:27, 15.04it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  28%|‚ñà‚ñà‚ñä       | 507/1825 [00:34<01:27, 15.02it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  28%|‚ñà‚ñà‚ñä       | 509/1825 [00:34<01:29, 14.75it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  28%|‚ñà‚ñà‚ñä       | 511/1825 [00:35<01:28, 14.80it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  28%|‚ñà‚ñà‚ñä       | 513/1825 [00:35<01:28, 14.82it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  28%|‚ñà‚ñà‚ñä       | 515/1825 [00:35<01:29, 14.66it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  28%|‚ñà‚ñà‚ñä       | 517/1825 [00:35<01:29, 14.68it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  28%|‚ñà‚ñà‚ñä       | 519/1825 [00:35<01:27, 14.91it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  29%|‚ñà‚ñà‚ñä       | 521/1825 [00:35<01:26, 15.02it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  29%|‚ñà‚ñà‚ñä       | 523/1825 [00:35<01:26, 15.14it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  29%|‚ñà‚ñà‚ñâ       | 525/1825 [00:35<01:25, 15.19it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  29%|‚ñà‚ñà‚ñâ       | 527/1825 [00:36<01:25, 15.20it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  29%|‚ñà‚ñà‚ñâ       | 529/1825 [00:36<01:25, 15.16it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  29%|‚ñà‚ñà‚ñâ       | 531/1825 [00:36<01:27, 14.72it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  29%|‚ñà‚ñà‚ñâ       | 533/1825 [00:36<01:26, 14.90it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  29%|‚ñà‚ñà‚ñâ       | 535/1825 [00:36<01:25, 15.01it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  29%|‚ñà‚ñà‚ñâ       | 537/1825 [00:36<01:25, 15.06it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  30%|‚ñà‚ñà‚ñâ       | 539/1825 [00:36<01:24, 15.19it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  30%|‚ñà‚ñà‚ñâ       | 541/1825 [00:37<01:24, 15.16it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  30%|‚ñà‚ñà‚ñâ       | 543/1825 [00:37<01:24, 15.25it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  30%|‚ñà‚ñà‚ñâ       | 545/1825 [00:37<01:24, 15.14it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  30%|‚ñà‚ñà‚ñâ       | 547/1825 [00:37<01:25, 15.01it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  30%|‚ñà‚ñà‚ñà       | 549/1825 [00:37<01:23, 15.23it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  30%|‚ñà‚ñà‚ñà       | 551/1825 [00:37<01:25, 14.91it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  30%|‚ñà‚ñà‚ñà       | 553/1825 [00:37<01:26, 14.78it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  30%|‚ñà‚ñà‚ñà       | 555/1825 [00:38<01:26, 14.70it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  31%|‚ñà‚ñà‚ñà       | 557/1825 [00:38<01:24, 14.93it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  31%|‚ñà‚ñà‚ñà       | 559/1825 [00:38<01:24, 14.93it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  31%|‚ñà‚ñà‚ñà       | 561/1825 [00:38<01:24, 14.92it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  31%|‚ñà‚ñà‚ñà       | 563/1825 [00:38<01:23, 15.04it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  31%|‚ñà‚ñà‚ñà       | 565/1825 [00:38<01:23, 15.10it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  31%|‚ñà‚ñà‚ñà       | 567/1825 [00:38<01:23, 15.12it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  31%|‚ñà‚ñà‚ñà       | 569/1825 [00:38<01:23, 15.13it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  31%|‚ñà‚ñà‚ñà‚ñè      | 571/1825 [00:39<01:25, 14.73it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  31%|‚ñà‚ñà‚ñà‚ñè      | 573/1825 [00:39<01:23, 14.98it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  32%|‚ñà‚ñà‚ñà‚ñè      | 575/1825 [00:39<01:25, 14.62it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  32%|‚ñà‚ñà‚ñà‚ñè      | 577/1825 [00:39<01:24, 14.75it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  32%|‚ñà‚ñà‚ñà‚ñè      | 579/1825 [00:39<01:27, 14.26it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  32%|‚ñà‚ñà‚ñà‚ñè      | 581/1825 [00:39<01:26, 14.43it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  32%|‚ñà‚ñà‚ñà‚ñè      | 583/1825 [00:39<01:25, 14.49it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  32%|‚ñà‚ñà‚ñà‚ñè      | 585/1825 [00:40<01:24, 14.65it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  32%|‚ñà‚ñà‚ñà‚ñè      | 587/1825 [00:40<01:25, 14.44it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  32%|‚ñà‚ñà‚ñà‚ñè      | 589/1825 [00:40<01:25, 14.48it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  32%|‚ñà‚ñà‚ñà‚ñè      | 591/1825 [00:40<01:25, 14.35it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  32%|‚ñà‚ñà‚ñà‚ñè      | 593/1825 [00:40<01:26, 14.20it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  33%|‚ñà‚ñà‚ñà‚ñé      | 595/1825 [00:40<01:25, 14.34it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  33%|‚ñà‚ñà‚ñà‚ñé      | 597/1825 [00:40<01:25, 14.42it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  33%|‚ñà‚ñà‚ñà‚ñé      | 599/1825 [00:41<01:25, 14.33it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  33%|‚ñà‚ñà‚ñà‚ñé      | 601/1825 [00:41<01:24, 14.53it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  33%|‚ñà‚ñà‚ñà‚ñé      | 603/1825 [00:41<01:25, 14.37it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  33%|‚ñà‚ñà‚ñà‚ñé      | 605/1825 [00:41<01:25, 14.20it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  33%|‚ñà‚ñà‚ñà‚ñé      | 607/1825 [00:41<01:26, 14.14it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  33%|‚ñà‚ñà‚ñà‚ñé      | 609/1825 [00:41<01:26, 14.12it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  33%|‚ñà‚ñà‚ñà‚ñé      | 611/1825 [00:41<01:30, 13.34it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  34%|‚ñà‚ñà‚ñà‚ñé      | 613/1825 [00:42<01:32, 13.04it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  34%|‚ñà‚ñà‚ñà‚ñé      | 615/1825 [00:42<01:32, 13.08it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  34%|‚ñà‚ñà‚ñà‚ñç      | 617/1825 [00:42<01:31, 13.22it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  34%|‚ñà‚ñà‚ñà‚ñç      | 619/1825 [00:42<01:28, 13.58it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  34%|‚ñà‚ñà‚ñà‚ñç      | 621/1825 [00:42<01:25, 14.03it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  34%|‚ñà‚ñà‚ñà‚ñç      | 623/1825 [00:42<01:26, 13.93it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  34%|‚ñà‚ñà‚ñà‚ñç      | 625/1825 [00:42<01:28, 13.61it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  34%|‚ñà‚ñà‚ñà‚ñç      | 627/1825 [00:43<01:33, 12.83it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  34%|‚ñà‚ñà‚ñà‚ñç      | 629/1825 [00:43<01:34, 12.71it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  35%|‚ñà‚ñà‚ñà‚ñç      | 631/1825 [00:43<01:30, 13.18it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  35%|‚ñà‚ñà‚ñà‚ñç      | 633/1825 [00:43<01:28, 13.43it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  35%|‚ñà‚ñà‚ñà‚ñç      | 635/1825 [00:43<01:25, 13.88it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  35%|‚ñà‚ñà‚ñà‚ñç      | 637/1825 [00:43<01:25, 13.91it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  35%|‚ñà‚ñà‚ñà‚ñå      | 639/1825 [00:43<01:25, 13.79it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  35%|‚ñà‚ñà‚ñà‚ñå      | 641/1825 [00:44<01:29, 13.24it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  35%|‚ñà‚ñà‚ñà‚ñå      | 643/1825 [00:44<01:29, 13.24it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  35%|‚ñà‚ñà‚ñà‚ñå      | 645/1825 [00:44<01:27, 13.44it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  35%|‚ñà‚ñà‚ñà‚ñå      | 647/1825 [00:44<01:28, 13.30it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  36%|‚ñà‚ñà‚ñà‚ñå      | 649/1825 [00:44<01:26, 13.66it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  36%|‚ñà‚ñà‚ñà‚ñå      | 651/1825 [00:44<01:23, 14.08it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  36%|‚ñà‚ñà‚ñà‚ñå      | 653/1825 [00:44<01:21, 14.40it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  36%|‚ñà‚ñà‚ñà‚ñå      | 655/1825 [00:45<01:21, 14.41it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  36%|‚ñà‚ñà‚ñà‚ñå      | 657/1825 [00:45<01:19, 14.64it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  36%|‚ñà‚ñà‚ñà‚ñå      | 659/1825 [00:45<01:18, 14.80it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  36%|‚ñà‚ñà‚ñà‚ñå      | 661/1825 [00:45<01:18, 14.74it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  36%|‚ñà‚ñà‚ñà‚ñã      | 663/1825 [00:45<01:18, 14.81it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  36%|‚ñà‚ñà‚ñà‚ñã      | 665/1825 [00:45<01:17, 14.93it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  37%|‚ñà‚ñà‚ñà‚ñã      | 667/1825 [00:45<01:16, 15.13it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  37%|‚ñà‚ñà‚ñà‚ñã      | 669/1825 [00:46<01:15, 15.30it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  37%|‚ñà‚ñà‚ñà‚ñã      | 671/1825 [00:46<01:19, 14.57it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  37%|‚ñà‚ñà‚ñà‚ñã      | 673/1825 [00:46<01:17, 14.86it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  37%|‚ñà‚ñà‚ñà‚ñã      | 675/1825 [00:46<01:16, 15.05it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  37%|‚ñà‚ñà‚ñà‚ñã      | 677/1825 [00:46<01:18, 14.66it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  37%|‚ñà‚ñà‚ñà‚ñã      | 679/1825 [00:46<01:16, 14.90it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  37%|‚ñà‚ñà‚ñà‚ñã      | 681/1825 [00:46<01:15, 15.07it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  37%|‚ñà‚ñà‚ñà‚ñã      | 683/1825 [00:46<01:15, 15.12it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  38%|‚ñà‚ñà‚ñà‚ñä      | 685/1825 [00:47<01:15, 15.18it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  38%|‚ñà‚ñà‚ñà‚ñä      | 687/1825 [00:47<01:14, 15.20it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  38%|‚ñà‚ñà‚ñà‚ñä      | 689/1825 [00:47<01:16, 14.83it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  38%|‚ñà‚ñà‚ñà‚ñä      | 691/1825 [00:47<01:17, 14.59it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  38%|‚ñà‚ñà‚ñà‚ñä      | 693/1825 [00:47<01:18, 14.51it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  38%|‚ñà‚ñà‚ñà‚ñä      | 695/1825 [00:47<01:17, 14.53it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  38%|‚ñà‚ñà‚ñà‚ñä      | 697/1825 [00:47<01:16, 14.74it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  38%|‚ñà‚ñà‚ñà‚ñä      | 699/1825 [00:48<01:15, 14.94it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  38%|‚ñà‚ñà‚ñà‚ñä      | 701/1825 [00:48<01:14, 15.03it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  39%|‚ñà‚ñà‚ñà‚ñä      | 703/1825 [00:48<01:14, 14.99it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  39%|‚ñà‚ñà‚ñà‚ñä      | 705/1825 [00:48<01:15, 14.77it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  39%|‚ñà‚ñà‚ñà‚ñä      | 707/1825 [00:48<01:16, 14.63it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  39%|‚ñà‚ñà‚ñà‚ñâ      | 709/1825 [00:48<01:16, 14.64it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  39%|‚ñà‚ñà‚ñà‚ñâ      | 711/1825 [00:48<01:16, 14.64it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  39%|‚ñà‚ñà‚ñà‚ñâ      | 713/1825 [00:49<01:16, 14.54it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  39%|‚ñà‚ñà‚ñà‚ñâ      | 715/1825 [00:49<01:16, 14.52it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  39%|‚ñà‚ñà‚ñà‚ñâ      | 717/1825 [00:49<01:16, 14.41it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  39%|‚ñà‚ñà‚ñà‚ñâ      | 719/1825 [00:49<01:15, 14.60it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  40%|‚ñà‚ñà‚ñà‚ñâ      | 721/1825 [00:49<01:14, 14.82it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  40%|‚ñà‚ñà‚ñà‚ñâ      | 723/1825 [00:49<01:13, 15.00it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  40%|‚ñà‚ñà‚ñà‚ñâ      | 725/1825 [00:49<01:12, 15.15it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  40%|‚ñà‚ñà‚ñà‚ñâ      | 727/1825 [00:49<01:13, 15.02it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  40%|‚ñà‚ñà‚ñà‚ñâ      | 729/1825 [00:50<01:13, 14.97it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  40%|‚ñà‚ñà‚ñà‚ñà      | 731/1825 [00:50<01:14, 14.59it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  40%|‚ñà‚ñà‚ñà‚ñà      | 733/1825 [00:50<01:14, 14.63it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  40%|‚ñà‚ñà‚ñà‚ñà      | 735/1825 [00:50<01:14, 14.71it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  40%|‚ñà‚ñà‚ñà‚ñà      | 737/1825 [00:50<01:13, 14.78it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  40%|‚ñà‚ñà‚ñà‚ñà      | 739/1825 [00:50<01:12, 14.92it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  41%|‚ñà‚ñà‚ñà‚ñà      | 741/1825 [00:50<01:13, 14.74it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  41%|‚ñà‚ñà‚ñà‚ñà      | 743/1825 [00:51<01:12, 14.86it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  41%|‚ñà‚ñà‚ñà‚ñà      | 745/1825 [00:51<01:12, 14.90it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  41%|‚ñà‚ñà‚ñà‚ñà      | 747/1825 [00:51<01:13, 14.65it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  41%|‚ñà‚ñà‚ñà‚ñà      | 749/1825 [00:51<01:14, 14.45it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  41%|‚ñà‚ñà‚ñà‚ñà      | 751/1825 [00:51<01:13, 14.71it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 753/1825 [00:51<01:12, 14.86it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 755/1825 [00:51<01:11, 15.02it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 757/1825 [00:51<01:11, 14.86it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 759/1825 [00:52<01:13, 14.58it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 761/1825 [00:52<01:11, 14.79it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 763/1825 [00:52<01:11, 14.92it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 765/1825 [00:52<01:11, 14.90it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 767/1825 [00:52<01:11, 14.86it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 769/1825 [00:52<01:10, 14.91it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 771/1825 [00:52<01:10, 14.97it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 773/1825 [00:53<01:10, 15.01it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 775/1825 [00:53<01:09, 15.14it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 777/1825 [00:53<01:28, 11.85it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 779/1825 [00:53<01:22, 12.67it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 781/1825 [00:53<01:18, 13.30it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 783/1825 [00:53<01:15, 13.72it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 785/1825 [00:53<01:14, 13.89it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 787/1825 [00:54<01:12, 14.31it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 789/1825 [00:54<01:13, 14.09it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 791/1825 [00:54<01:12, 14.32it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 793/1825 [00:54<01:10, 14.62it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 795/1825 [00:54<01:09, 14.86it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 797/1825 [00:54<01:09, 14.70it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 799/1825 [00:54<01:09, 14.73it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 801/1825 [00:55<01:08, 14.96it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 803/1825 [00:55<01:09, 14.79it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 805/1825 [00:55<01:07, 15.01it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 807/1825 [00:55<01:08, 14.81it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 809/1825 [00:55<01:09, 14.66it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 811/1825 [00:55<01:08, 14.81it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 813/1825 [00:55<01:06, 15.17it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 815/1825 [00:55<01:05, 15.37it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 817/1825 [00:56<01:04, 15.51it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 819/1825 [00:56<01:05, 15.33it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 821/1825 [00:56<01:05, 15.33it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 823/1825 [00:56<01:05, 15.33it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 825/1825 [00:56<01:05, 15.36it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 827/1825 [00:56<01:05, 15.32it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 829/1825 [00:56<01:05, 15.18it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 831/1825 [00:57<01:04, 15.34it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 833/1825 [00:57<01:04, 15.40it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 835/1825 [00:57<01:05, 15.08it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 837/1825 [00:57<01:05, 15.16it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 839/1825 [00:57<01:04, 15.30it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 841/1825 [00:57<01:04, 15.37it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 843/1825 [00:57<01:03, 15.36it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 845/1825 [00:57<01:03, 15.37it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 847/1825 [00:58<01:04, 15.09it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 849/1825 [00:58<01:04, 15.20it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 851/1825 [00:58<01:05, 14.98it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 853/1825 [00:58<01:04, 15.05it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 855/1825 [00:58<01:07, 14.37it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 857/1825 [00:58<01:06, 14.59it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 859/1825 [00:58<01:04, 14.91it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 861/1825 [00:59<01:03, 15.10it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 863/1825 [00:59<01:03, 15.14it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 865/1825 [00:59<01:03, 15.02it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 867/1825 [00:59<01:04, 14.96it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 869/1825 [00:59<01:03, 15.13it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 871/1825 [00:59<01:02, 15.25it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 873/1825 [00:59<01:03, 14.92it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 875/1825 [00:59<01:02, 15.13it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 877/1825 [01:00<01:04, 14.80it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 879/1825 [01:00<01:02, 15.03it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 881/1825 [01:00<01:05, 14.47it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 883/1825 [01:00<01:05, 14.49it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 885/1825 [01:00<01:04, 14.49it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 887/1825 [01:00<01:03, 14.79it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 889/1825 [01:00<01:03, 14.65it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 891/1825 [01:01<01:03, 14.67it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 893/1825 [01:01<01:02, 14.83it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 895/1825 [01:01<01:03, 14.59it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 897/1825 [01:01<01:03, 14.58it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 899/1825 [01:01<01:05, 14.05it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 901/1825 [01:01<01:03, 14.54it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 903/1825 [01:01<01:02, 14.86it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 905/1825 [01:02<01:00, 15.10it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 907/1825 [01:02<01:00, 15.26it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 909/1825 [01:02<00:59, 15.38it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 911/1825 [01:02<00:58, 15.52it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 913/1825 [01:02<00:59, 15.36it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 915/1825 [01:02<00:58, 15.46it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 917/1825 [01:02<00:58, 15.50it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 919/1825 [01:02<00:58, 15.54it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 921/1825 [01:03<00:58, 15.55it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 923/1825 [01:03<00:57, 15.59it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 925/1825 [01:03<00:57, 15.62it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 927/1825 [01:03<00:57, 15.64it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 929/1825 [01:03<00:57, 15.63it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 931/1825 [01:03<00:57, 15.52it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 933/1825 [01:03<00:58, 15.31it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 935/1825 [01:03<00:58, 15.18it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 937/1825 [01:04<00:58, 15.20it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 939/1825 [01:04<00:58, 15.27it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 941/1825 [01:04<00:58, 15.14it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 943/1825 [01:04<00:58, 15.17it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 945/1825 [01:04<00:58, 15.15it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 947/1825 [01:04<00:58, 15.08it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 949/1825 [01:04<00:59, 14.77it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 951/1825 [01:05<01:02, 14.10it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 953/1825 [01:05<01:00, 14.39it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 955/1825 [01:05<01:00, 14.48it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 957/1825 [01:05<00:58, 14.77it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 959/1825 [01:05<00:57, 14.94it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 961/1825 [01:05<00:57, 15.14it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 963/1825 [01:05<00:56, 15.16it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 965/1825 [01:05<00:58, 14.62it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 967/1825 [01:06<00:57, 14.83it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 969/1825 [01:06<00:56, 15.03it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 971/1825 [01:06<00:57, 14.91it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 973/1825 [01:06<00:56, 15.08it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 975/1825 [01:06<00:56, 14.97it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 977/1825 [01:06<00:56, 15.07it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 979/1825 [01:06<00:57, 14.81it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 981/1825 [01:07<00:56, 14.89it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 983/1825 [01:07<00:57, 14.65it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 985/1825 [01:07<00:58, 14.32it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 987/1825 [01:07<00:57, 14.55it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 989/1825 [01:07<00:56, 14.70it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 991/1825 [01:07<00:55, 14.92it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 993/1825 [01:07<00:56, 14.81it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 995/1825 [01:07<00:55, 14.99it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 997/1825 [01:08<00:55, 15.03it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 999/1825 [01:08<00:55, 14.98it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1001/1825 [01:08<00:56, 14.46it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1003/1825 [01:08<00:56, 14.55it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1005/1825 [01:08<00:55, 14.84it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1007/1825 [01:08<00:55, 14.74it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1009/1825 [01:08<00:55, 14.80it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1011/1825 [01:09<00:54, 15.00it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1013/1825 [01:09<00:54, 14.97it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1015/1825 [01:09<00:53, 15.12it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1017/1825 [01:09<00:54, 14.96it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1019/1825 [01:09<00:53, 15.17it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1021/1825 [01:09<00:54, 14.78it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1023/1825 [01:09<00:53, 15.07it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1025/1825 [01:09<00:52, 15.27it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1027/1825 [01:10<00:53, 14.86it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1029/1825 [01:10<00:53, 14.95it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1031/1825 [01:10<00:53, 14.91it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1033/1825 [01:10<00:52, 14.95it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1035/1825 [01:10<00:52, 15.11it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1037/1825 [01:10<00:51, 15.31it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1039/1825 [01:10<00:52, 14.84it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1041/1825 [01:11<00:52, 14.99it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1043/1825 [01:11<00:51, 15.07it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1045/1825 [01:11<00:51, 15.25it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1047/1825 [01:11<00:52, 14.69it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1049/1825 [01:11<00:51, 15.09it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1051/1825 [01:11<00:50, 15.19it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1053/1825 [01:11<00:50, 15.29it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1055/1825 [01:11<00:50, 15.20it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1057/1825 [01:12<00:51, 14.86it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1059/1825 [01:12<00:51, 14.86it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1061/1825 [01:12<00:50, 15.03it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1063/1825 [01:12<00:51, 14.78it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1065/1825 [01:12<00:50, 14.98it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1067/1825 [01:12<00:49, 15.23it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1069/1825 [01:12<00:50, 14.89it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1071/1825 [01:13<00:51, 14.72it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1073/1825 [01:13<00:50, 15.03it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1075/1825 [01:13<00:50, 14.91it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1077/1825 [01:13<00:51, 14.48it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1079/1825 [01:13<00:50, 14.85it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1081/1825 [01:13<00:49, 15.09it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1083/1825 [01:13<00:49, 14.86it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1085/1825 [01:14<00:52, 14.06it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1087/1825 [01:14<00:50, 14.50it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1089/1825 [01:14<00:50, 14.59it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1091/1825 [01:14<00:50, 14.56it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1093/1825 [01:14<00:49, 14.81it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1095/1825 [01:14<00:48, 15.12it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1097/1825 [01:14<00:47, 15.20it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1099/1825 [01:14<00:47, 15.17it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1101/1825 [01:15<00:48, 14.92it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1103/1825 [01:15<00:47, 15.05it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1105/1825 [01:15<00:48, 14.95it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1107/1825 [01:15<00:47, 15.15it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1109/1825 [01:15<00:46, 15.38it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1111/1825 [01:15<00:46, 15.47it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1113/1825 [01:15<00:48, 14.82it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1115/1825 [01:16<00:47, 14.85it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1117/1825 [01:16<00:47, 14.77it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 1119/1825 [01:16<00:47, 14.85it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 1121/1825 [01:16<00:46, 15.00it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 1123/1825 [01:16<00:45, 15.27it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 1125/1825 [01:16<00:45, 15.38it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 1127/1825 [01:16<00:45, 15.31it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 1129/1825 [01:16<00:45, 15.21it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 1131/1825 [01:17<00:45, 15.30it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 1133/1825 [01:17<00:45, 15.25it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 1135/1825 [01:17<00:44, 15.40it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 1137/1825 [01:17<00:44, 15.49it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 1139/1825 [01:17<00:44, 15.50it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 1141/1825 [01:17<00:44, 15.50it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 1143/1825 [01:17<00:45, 15.13it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 1145/1825 [01:17<00:46, 14.77it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 1147/1825 [01:18<00:45, 14.92it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 1149/1825 [01:18<00:45, 14.88it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 1151/1825 [01:18<00:44, 15.02it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 1153/1825 [01:18<00:45, 14.88it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 1155/1825 [01:18<00:44, 15.04it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 1157/1825 [01:18<00:44, 15.04it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 1159/1825 [01:18<00:43, 15.16it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 1161/1825 [01:19<00:43, 15.24it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 1163/1825 [01:19<00:44, 15.01it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 1165/1825 [01:19<00:43, 15.06it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 1167/1825 [01:19<00:43, 15.19it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 1169/1825 [01:19<00:43, 15.07it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 1171/1825 [01:19<00:43, 14.98it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 1173/1825 [01:19<00:44, 14.69it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 1175/1825 [01:20<00:44, 14.51it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 1177/1825 [01:20<00:45, 14.35it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 1179/1825 [01:20<00:44, 14.58it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 1181/1825 [01:20<00:43, 14.86it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 1183/1825 [01:20<00:42, 15.01it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 1185/1825 [01:20<00:42, 15.21it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 1187/1825 [01:20<00:42, 15.15it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 1189/1825 [01:20<00:41, 15.33it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 1191/1825 [01:21<00:41, 15.21it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 1193/1825 [01:21<00:41, 15.08it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 1195/1825 [01:21<00:41, 15.26it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 1197/1825 [01:21<00:41, 15.28it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 1199/1825 [01:21<00:40, 15.37it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 1201/1825 [01:21<00:40, 15.50it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 1203/1825 [01:21<00:40, 15.46it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 1205/1825 [01:21<00:40, 15.29it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 1207/1825 [01:22<00:40, 15.30it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 1209/1825 [01:22<00:40, 15.36it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 1211/1825 [01:22<00:40, 14.99it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 1213/1825 [01:22<00:41, 14.78it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 1215/1825 [01:22<00:41, 14.82it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 1217/1825 [01:22<00:40, 15.01it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 1219/1825 [01:22<00:40, 14.98it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 1221/1825 [01:23<00:39, 15.14it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 1223/1825 [01:23<00:39, 15.28it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 1225/1825 [01:23<00:39, 15.35it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 1227/1825 [01:23<00:39, 15.26it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 1229/1825 [01:23<00:38, 15.41it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 1231/1825 [01:23<00:38, 15.56it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 1233/1825 [01:23<00:38, 15.36it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 1235/1825 [01:23<00:38, 15.32it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 1237/1825 [01:24<00:38, 15.47it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 1239/1825 [01:24<00:37, 15.55it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 1241/1825 [01:24<00:37, 15.61it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 1243/1825 [01:24<00:37, 15.52it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 1245/1825 [01:24<00:37, 15.57it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 1247/1825 [01:24<00:37, 15.59it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 1249/1825 [01:24<00:38, 15.00it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 1251/1825 [01:24<00:37, 15.14it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 1253/1825 [01:25<00:37, 15.27it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 1255/1825 [01:25<00:37, 15.22it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 1257/1825 [01:25<00:37, 15.03it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 1259/1825 [01:25<00:37, 14.99it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 1261/1825 [01:25<00:37, 15.12it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 1263/1825 [01:25<00:37, 15.17it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 1265/1825 [01:25<00:37, 14.93it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 1267/1825 [01:26<00:37, 15.01it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 1269/1825 [01:26<00:36, 15.16it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 1271/1825 [01:26<00:36, 15.17it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 1273/1825 [01:26<00:37, 14.66it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 1275/1825 [01:26<00:37, 14.81it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 1277/1825 [01:26<00:36, 14.99it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 1279/1825 [01:26<00:36, 15.06it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 1281/1825 [01:26<00:35, 15.18it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 1283/1825 [01:27<00:35, 15.29it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 1285/1825 [01:27<00:35, 15.19it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 1287/1825 [01:27<00:35, 15.24it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 1289/1825 [01:27<00:35, 15.26it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 1291/1825 [01:27<00:34, 15.36it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 1293/1825 [01:27<00:35, 14.97it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 1295/1825 [01:27<00:34, 15.24it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 1297/1825 [01:28<00:36, 14.56it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 1299/1825 [01:28<00:35, 14.62it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 1301/1825 [01:28<00:35, 14.81it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 1303/1825 [01:28<00:35, 14.65it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 1305/1825 [01:28<00:35, 14.65it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 1307/1825 [01:28<00:35, 14.53it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 1309/1825 [01:28<00:35, 14.66it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 1311/1825 [01:28<00:34, 15.00it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 1313/1825 [01:29<00:35, 14.52it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 1315/1825 [01:29<00:34, 14.72it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 1317/1825 [01:29<00:34, 14.75it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 1319/1825 [01:29<00:33, 14.95it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 1321/1825 [01:29<00:33, 15.13it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 1323/1825 [01:29<00:33, 14.80it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 1325/1825 [01:29<00:34, 14.50it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 1327/1825 [01:30<00:34, 14.62it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 1329/1825 [01:30<00:33, 14.72it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 1331/1825 [01:30<00:33, 14.85it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 1333/1825 [01:30<00:33, 14.78it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 1335/1825 [01:30<00:32, 15.09it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 1337/1825 [01:30<00:32, 15.22it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 1339/1825 [01:30<00:31, 15.38it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 1341/1825 [01:31<00:32, 14.98it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 1343/1825 [01:31<00:32, 14.95it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 1345/1825 [01:31<00:31, 15.01it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 1347/1825 [01:31<00:31, 14.95it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 1349/1825 [01:31<00:31, 15.07it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 1351/1825 [01:31<00:31, 15.04it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 1353/1825 [01:31<00:31, 15.08it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 1355/1825 [01:31<00:32, 14.65it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 1357/1825 [01:32<00:32, 14.61it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 1359/1825 [01:32<00:31, 14.85it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 1361/1825 [01:32<00:30, 14.98it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 1363/1825 [01:32<00:31, 14.83it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 1365/1825 [01:32<00:32, 14.10it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 1367/1825 [01:32<00:32, 14.10it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 1369/1825 [01:32<00:32, 14.20it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 1371/1825 [01:33<00:31, 14.60it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 1373/1825 [01:33<00:30, 14.95it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 1375/1825 [01:33<00:30, 14.66it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 1377/1825 [01:33<00:30, 14.78it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 1379/1825 [01:33<00:29, 14.96it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 1381/1825 [01:33<00:29, 15.07it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 1383/1825 [01:33<00:29, 15.20it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 1385/1825 [01:33<00:28, 15.21it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 1387/1825 [01:34<00:28, 15.37it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 1389/1825 [01:34<00:28, 15.51it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 1391/1825 [01:34<00:28, 15.39it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 1393/1825 [01:34<00:28, 15.26it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 1395/1825 [01:34<00:27, 15.38it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 1397/1825 [01:34<00:28, 15.13it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 1399/1825 [01:34<00:28, 15.07it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 1401/1825 [01:35<00:27, 15.21it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 1403/1825 [01:35<00:27, 15.41it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 1405/1825 [01:35<00:27, 15.02it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 1407/1825 [01:35<00:28, 14.77it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 1409/1825 [01:35<00:28, 14.69it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 1411/1825 [01:35<00:27, 14.97it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 1413/1825 [01:35<00:27, 15.21it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 1415/1825 [01:35<00:27, 14.66it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 1417/1825 [01:36<00:27, 14.88it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 1419/1825 [01:36<00:27, 14.98it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 1421/1825 [01:36<00:26, 15.24it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 1423/1825 [01:36<00:26, 15.18it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 1425/1825 [01:36<00:26, 15.38it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 1427/1825 [01:36<00:25, 15.37it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 1429/1825 [01:36<00:25, 15.30it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 1431/1825 [01:37<00:26, 14.73it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 1433/1825 [01:37<00:26, 15.01it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 1435/1825 [01:37<00:26, 14.82it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 1437/1825 [01:37<00:26, 14.57it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 1439/1825 [01:37<00:26, 14.67it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 1441/1825 [01:37<00:25, 14.81it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 1443/1825 [01:37<00:25, 15.00it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 1445/1825 [01:37<00:25, 14.81it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 1447/1825 [01:38<00:25, 14.79it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 1449/1825 [01:38<00:25, 14.96it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 1451/1825 [01:38<00:24, 15.07it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 1453/1825 [01:38<00:24, 15.22it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 1455/1825 [01:38<00:24, 15.13it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 1457/1825 [01:38<00:24, 14.83it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 1459/1825 [01:38<00:25, 14.28it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 1461/1825 [01:39<00:25, 14.31it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 1463/1825 [01:39<00:24, 14.50it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 1465/1825 [01:39<00:24, 14.71it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 1467/1825 [01:39<00:24, 14.42it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 1469/1825 [01:39<00:24, 14.52it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 1471/1825 [01:39<00:24, 14.36it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 1473/1825 [01:39<00:24, 14.48it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 1475/1825 [01:40<00:24, 14.01it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 1477/1825 [01:40<00:24, 14.39it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 1479/1825 [01:40<00:24, 14.28it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 1481/1825 [01:40<00:23, 14.49it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 1483/1825 [01:40<00:23, 14.75it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 1485/1825 [01:40<00:23, 14.67it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 1487/1825 [01:40<00:22, 14.90it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 1489/1825 [01:40<00:22, 15.04it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 1491/1825 [01:41<00:21, 15.25it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 1493/1825 [01:41<00:21, 15.32it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 1495/1825 [01:41<00:21, 15.27it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 1497/1825 [01:41<00:21, 15.27it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 1499/1825 [01:41<00:21, 15.03it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 1501/1825 [01:41<00:21, 15.15it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 1503/1825 [01:41<00:21, 15.09it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 1505/1825 [01:42<00:21, 15.12it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 1507/1825 [01:42<00:21, 14.67it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 1509/1825 [01:42<00:21, 14.54it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 1511/1825 [01:42<00:21, 14.71it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 1513/1825 [01:42<00:21, 14.36it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 1515/1825 [01:42<00:21, 14.60it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 1517/1825 [01:42<00:21, 14.58it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 1519/1825 [01:42<00:20, 14.74it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 1521/1825 [01:43<00:20, 14.91it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 1523/1825 [01:43<00:20, 14.75it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 1525/1825 [01:43<00:21, 14.20it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 1527/1825 [01:43<00:20, 14.51it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 1529/1825 [01:43<00:20, 14.29it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 1531/1825 [01:43<00:20, 14.41it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 1533/1825 [01:43<00:20, 14.60it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 1535/1825 [01:44<00:19, 14.95it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 1537/1825 [01:44<00:18, 15.16it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 1539/1825 [01:44<00:18, 15.27it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 1541/1825 [01:44<00:18, 15.30it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 1543/1825 [01:44<00:18, 15.48it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 1545/1825 [01:44<00:18, 15.41it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 1547/1825 [01:44<00:18, 15.37it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 1549/1825 [01:44<00:17, 15.34it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 1551/1825 [01:45<00:17, 15.33it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 1553/1825 [01:45<00:17, 15.33it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 1555/1825 [01:45<00:17, 15.18it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 1557/1825 [01:45<00:17, 15.19it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 1559/1825 [01:45<00:17, 15.08it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 1561/1825 [01:45<00:17, 15.01it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 1563/1825 [01:45<00:17, 15.07it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 1565/1825 [01:46<00:17, 15.15it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 1567/1825 [01:46<00:17, 15.15it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 1569/1825 [01:46<00:16, 15.08it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 1571/1825 [01:46<00:16, 14.98it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 1573/1825 [01:46<00:16, 15.13it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 1575/1825 [01:46<00:16, 15.32it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 1577/1825 [01:46<00:16, 15.42it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 1579/1825 [01:46<00:16, 15.32it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 1581/1825 [01:47<00:16, 15.06it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 1583/1825 [01:47<00:15, 15.20it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 1585/1825 [01:47<00:16, 14.89it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 1587/1825 [01:47<00:16, 14.79it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 1589/1825 [01:47<00:15, 14.98it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 1591/1825 [01:47<00:15, 14.99it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 1593/1825 [01:47<00:15, 14.82it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 1595/1825 [01:48<00:15, 14.91it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 1597/1825 [01:48<00:15, 14.87it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 1599/1825 [01:48<00:15, 14.88it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 1601/1825 [01:48<00:15, 14.80it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 1603/1825 [01:48<00:15, 14.62it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 1605/1825 [01:48<00:14, 14.92it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 1607/1825 [01:48<00:14, 15.06it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 1609/1825 [01:48<00:14, 15.18it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 1611/1825 [01:49<00:14, 14.96it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 1613/1825 [01:49<00:14, 14.75it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 1615/1825 [01:49<00:14, 14.85it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 1617/1825 [01:49<00:13, 15.10it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 1619/1825 [01:49<00:13, 15.20it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 1621/1825 [01:49<00:13, 15.31it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 1623/1825 [01:49<00:13, 15.41it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 1625/1825 [01:50<00:12, 15.47it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 1627/1825 [01:50<00:12, 15.45it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 1629/1825 [01:50<00:13, 14.79it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 1631/1825 [01:50<00:13, 14.73it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 1633/1825 [01:50<00:12, 14.98it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 1635/1825 [01:50<00:12, 14.89it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 1637/1825 [01:50<00:12, 15.01it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 1639/1825 [01:50<00:12, 14.77it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 1641/1825 [01:51<00:12, 14.46it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 1643/1825 [01:51<00:12, 14.75it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 1645/1825 [01:51<00:11, 15.01it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 1647/1825 [01:51<00:11, 15.03it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 1649/1825 [01:51<00:11, 15.14it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 1651/1825 [01:51<00:11, 14.84it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 1653/1825 [01:51<00:11, 14.62it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 1655/1825 [01:52<00:11, 14.58it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 1657/1825 [01:52<00:11, 14.63it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 1659/1825 [01:52<00:11, 14.69it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 1661/1825 [01:52<00:11, 14.82it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 1663/1825 [01:52<00:10, 14.94it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 1665/1825 [01:52<00:10, 14.91it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 1667/1825 [01:52<00:10, 14.98it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 1669/1825 [01:52<00:10, 15.13it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 1671/1825 [01:53<00:10, 14.48it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 1673/1825 [01:53<00:10, 14.32it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 1675/1825 [01:53<00:10, 14.34it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 1677/1825 [01:53<00:10, 14.75it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 1679/1825 [01:53<00:09, 14.84it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 1681/1825 [01:53<00:09, 14.76it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 1683/1825 [01:53<00:09, 14.81it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 1685/1825 [01:54<00:09, 14.98it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 1687/1825 [01:54<00:09, 14.76it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 1689/1825 [01:54<00:09, 14.72it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 1691/1825 [01:54<00:09, 14.89it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 1693/1825 [01:54<00:08, 14.76it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 1695/1825 [01:54<00:10, 12.94it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 1697/1825 [01:54<00:09, 13.57it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 1699/1825 [01:55<00:09, 13.75it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 1701/1825 [01:55<00:08, 14.12it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 1703/1825 [01:55<00:08, 13.84it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 1705/1825 [01:55<00:08, 13.96it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 1707/1825 [01:55<00:08, 13.85it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 1709/1825 [01:55<00:08, 14.29it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 1711/1825 [01:55<00:07, 14.65it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 1713/1825 [01:56<00:07, 14.99it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 1715/1825 [01:56<00:07, 15.03it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 1717/1825 [01:56<00:07, 14.45it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 1719/1825 [01:56<00:07, 14.57it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 1721/1825 [01:56<00:07, 14.46it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 1723/1825 [01:56<00:06, 14.66it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 1725/1825 [01:56<00:06, 14.57it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 1727/1825 [01:57<00:06, 14.72it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 1729/1825 [01:57<00:06, 14.90it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 1731/1825 [01:57<00:06, 14.72it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 1733/1825 [01:57<00:06, 14.92it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 1735/1825 [01:57<00:05, 15.06it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 1737/1825 [01:57<00:05, 15.17it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 1739/1825 [01:57<00:05, 14.95it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 1741/1825 [01:57<00:05, 15.07it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 1743/1825 [01:58<00:05, 14.63it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 1745/1825 [01:58<00:05, 14.77it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 1747/1825 [01:58<00:05, 14.73it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 1749/1825 [01:58<00:05, 14.80it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 1751/1825 [01:58<00:04, 14.97it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 1753/1825 [01:58<00:04, 14.89it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 1755/1825 [01:58<00:04, 14.58it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 1757/1825 [01:59<00:04, 14.77it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 1759/1825 [01:59<00:04, 14.87it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 1761/1825 [01:59<00:04, 14.53it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 1763/1825 [01:59<00:04, 14.37it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 1765/1825 [01:59<00:04, 14.74it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 1767/1825 [01:59<00:04, 14.42it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 1769/1825 [01:59<00:03, 14.19it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 1771/1825 [02:00<00:03, 14.07it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 1773/1825 [02:00<00:03, 14.19it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 1775/1825 [02:00<00:03, 13.83it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 1777/1825 [02:00<00:03, 14.22it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 1779/1825 [02:00<00:03, 14.45it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 1781/1825 [02:00<00:03, 14.62it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 1783/1825 [02:00<00:02, 14.09it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 1785/1825 [02:01<00:02, 14.23it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 1787/1825 [02:01<00:02, 14.40it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 1789/1825 [02:01<00:02, 14.38it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 1791/1825 [02:01<00:02, 14.47it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 1793/1825 [02:01<00:02, 14.60it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 1795/1825 [02:01<00:02, 14.58it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 1797/1825 [02:01<00:01, 14.13it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 1799/1825 [02:01<00:01, 14.24it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 1801/1825 [02:02<00:01, 13.88it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 1803/1825 [02:02<00:01, 14.32it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 1805/1825 [02:02<00:01, 14.22it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 1807/1825 [02:02<00:01, 14.50it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 1809/1825 [02:02<00:01, 14.90it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 1811/1825 [02:02<00:00, 14.98it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 1813/1825 [02:02<00:00, 14.50it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 1815/1825 [02:03<00:00, 14.43it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 1817/1825 [02:03<00:00, 14.55it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 1819/1825 [02:03<00:00, 14.29it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 1821/1825 [02:03<00:00, 14.59it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 1823/1825 [02:03<00:00, 14.52it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 2, features 1152\n",
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1825/1825 [02:03<00:00, 14.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model is saved.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoProcessor, LlavaForConditionalGeneration, BitsAndBytesConfig\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from PIL import Image\n",
        "import os\n",
        "import torchvision.transforms as transforms\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class LLaVADataset(Dataset):\n",
        "    def __init__(self, image_folder, processor, max_length=512, image_size=(336, 336)):\n",
        "        self.image_folder = image_folder\n",
        "        self.processor = processor\n",
        "        self.max_length = max_length\n",
        "        self.image_size = image_size\n",
        "        self.image_token = processor.tokenizer.convert_tokens_to_ids(\"<image>\")\n",
        "\n",
        "        # Validate and load image files\n",
        "        self.image_files = [f for f in os.listdir(image_folder)\n",
        "                          if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "        if not self.image_files:\n",
        "            raise ValueError(f\"No valid image files found in {image_folder}\")\n",
        "\n",
        "        logger.info(f\"Found {len(self.image_files)} valid image files\")\n",
        "\n",
        "        # Improved image transformation pipeline\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize(image_size, interpolation=transforms.InterpolationMode.BILINEAR),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                               std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_file = self.image_files[idx]\n",
        "        image_path = os.path.join(self.image_folder, image_file)\n",
        "\n",
        "        # Extract caption and preprocess\n",
        "        caption = os.path.splitext(image_file)[0].replace(\"_\", \" \").strip()\n",
        "        # Important: Do not add image token in the text - it's handled by the model\n",
        "        caption = caption\n",
        "\n",
        "        try:\n",
        "            # Image processing with error handling\n",
        "            image = Image.open(image_path).convert(\"RGB\")\n",
        "            if image.mode != \"RGB\":\n",
        "                image = image.convert(\"RGB\")\n",
        "            image_tensor = self.transform(image)\n",
        "\n",
        "            # Text processing with proper token handling\n",
        "            text_inputs = self.processor.tokenizer(\n",
        "                caption,\n",
        "                padding=\"max_length\",\n",
        "                truncation=True,\n",
        "                max_length=self.max_length - 1,  # Reserve space for image token\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "            input_ids = text_inputs[\"input_ids\"].squeeze(0)\n",
        "            attention_mask = text_inputs[\"attention_mask\"].squeeze(0)\n",
        "\n",
        "            # Append image token\n",
        "            input_ids = torch.cat([input_ids, torch.tensor([self.image_token])])\n",
        "            attention_mask = torch.cat([attention_mask, torch.tensor([1])])\n",
        "\n",
        "            return {\n",
        "                \"input_ids\": input_ids,\n",
        "                \"attention_mask\": attention_mask,\n",
        "                \"pixel_values\": image_tensor,\n",
        "                \"image_grid_thw\": torch.tensor([[1, 1, 1]], dtype=torch.int64)\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing {image_file}: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "def create_collate_fn(pad_token_id=0):\n",
        "    def collate_fn(batch):\n",
        "        # Remove None values from failed samples\n",
        "        batch = [b for b in batch if b is not None]\n",
        "        if not batch:\n",
        "            return None\n",
        "\n",
        "        # Prepare tensors\n",
        "        input_ids = pad_sequence([item[\"input_ids\"] for item in batch],\n",
        "                               batch_first=True,\n",
        "                               padding_value=pad_token_id)\n",
        "        attention_mask = pad_sequence([item[\"attention_mask\"] for item in batch],\n",
        "                                    batch_first=True,\n",
        "                                    padding_value=0)\n",
        "        pixel_values = torch.stack([item[\"pixel_values\"] for item in batch])\n",
        "        image_grid_thw = torch.stack([item[\"image_grid_thw\"] for item in batch])\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"pixel_values\": pixel_values,\n",
        "            \"image_grid_thw\": image_grid_thw\n",
        "        }\n",
        "    return collate_fn\n",
        "\n",
        "def setup_model_and_processor(model_name, token):\n",
        "    \"\"\"Setup model with optimized configuration\"\"\"\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\"  # Using nested float 4 for better precision\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        processor = AutoProcessor.from_pretrained(model_name, token=token)\n",
        "        model = LlavaForConditionalGeneration.from_pretrained(\n",
        "            model_name,\n",
        "            token=token,\n",
        "            quantization_config=bnb_config,\n",
        "            device_map=\"auto\",\n",
        "            torch_dtype=torch.float16\n",
        "        )\n",
        "        model.gradient_checkpointing_enable()\n",
        "\n",
        "        logger.info(\"Model and processor loaded successfully\")\n",
        "        return model, processor\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading model: {str(e)}\")\n",
        "        return None, None\n",
        "\n",
        "def main():\n",
        "    # Configuration\n",
        "    MODEL_NAME = \"llava-hf/llava-1.5-13b-hf\"\n",
        "    TOKEN = \"hf_token\"\n",
        "    IMAGE_FOLDER = \"/content/content/sample_data\"\n",
        "    BATCH_SIZE = 4\n",
        "    MAX_LENGTH = 512\n",
        "    IMAGE_SIZE = (336, 336)  # Make sure this matches the model's expected size\n",
        "\n",
        "    # Add PEFT/LoRA configuration\n",
        "    lora_config = LoraConfig(\n",
        "        r=16,\n",
        "        lora_alpha=32,\n",
        "        target_modules=[\"q_proj\", \"v_proj\"],\n",
        "        lora_dropout=0.1,\n",
        "        bias=\"none\",\n",
        "    )\n",
        "\n",
        "    # Setup\n",
        "    model, processor = setup_model_and_processor(MODEL_NAME, TOKEN)\n",
        "    if model is None or processor is None:\n",
        "        return\n",
        "\n",
        "    # Initialize dataset and dataloader\n",
        "    dataset = LLaVADataset(IMAGE_FOLDER, processor, MAX_LENGTH, IMAGE_SIZE)\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        collate_fn=create_collate_fn(processor.tokenizer.pad_token_id),\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    # Training loop example\n",
        "    model.train()\n",
        "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
        "        if batch is None:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Move batch to device\n",
        "            batch = {k: v.cuda() for k, v in batch.items()}\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss\n",
        "\n",
        "            # Backward pass and optimization steps would go here\n",
        "            # loss.backward()\n",
        "            # optimizer.step()\n",
        "            # optimizer.zero_grad()\n",
        "\n",
        "            logger.info(f\"Loss: {loss.item():.4f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in training loop: {str(e)}\")\n",
        "            continue\n",
        "    model.save_pretrained(\"/content/llava-finetuned\")\n",
        "    print(\"model is saved.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVLPVDNt8qTn",
        "outputId": "b17273c5-9315-44b6-a94c-e7d7a1f5f51e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìù Generated Description:  Describe the scene in detail.\n"
          ]
        }
      ],
      "source": [
        "# Generate a description\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        **inputs,\n",
        "        max_length=1024,\n",
        "        do_sample=True,  # Allow sampling for diverse outputs\n",
        "        temperature=0.7,  # Adjust randomness\n",
        "        top_p=0.9  # Enable nucleus sampling\n",
        "    )\n",
        "\n",
        "# Decode and print the result\n",
        "description = processor.tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(\"\\nüìù Generated Description:\", description)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "bc3b8a41a31448a5b8cada089bbbf428",
            "4ffb79decbb1495e8b3170997535371f",
            "ebe48e28147e4068813f5e88c64eeb98",
            "8b956e7a497644d294d246530db33a91",
            "f359454a80be4630a31f22156046037e",
            "477bd4ffe2384ca78900ac18a7e07c66",
            "413edeff0876446d9d0e6de4c5d4286f",
            "6d64747b6cf94a6781dde91b87f593a4",
            "1f999bfdc60c489a9d38736ee0ddd2ba",
            "154e97b19e7c4cc2971157c239ebce20",
            "d4db570bdf614c2aa85786674a97db37"
          ]
        },
        "id": "r0AamRhY83FU",
        "outputId": "13ec8f80-c537-4edd-f7f5-9a8d3534cc6b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bc3b8a41a31448a5b8cada089bbbf428",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìù Pretrained Model Description:  Describe the scene in detail.\n"
          ]
        }
      ],
      "source": [
        "pretrained_model = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/llava-1.5-13b-hf\", torch_dtype=torch.float16, device_map=\"auto\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = pretrained_model.generate(**inputs, max_length=1024)\n",
        "\n",
        "description = processor.tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(\"\\nüìù Pretrained Model Description:\", description)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "817af4ee28114961b8deec547b9d224a",
            "8cc8105b95544171ac67f752a42dcaaa",
            "133b8af5f5944565a5db3c68c302d806",
            "d2c2b1cfdd664e299e631cd29ade0104",
            "c7c2d2b92ddc4d218989ae3d78877fd2",
            "e312eb0a596749fbb9768a58c1d834b7",
            "1be5ea723a3f415eb6f01976a27203a7",
            "b3afd4668b204263a8ecbcef303206a3",
            "b46afe5aeab64e7b898766268f1bcaa9",
            "81fe15871bce4f23aadbeed7a2d52c71",
            "2f02d8fabe5845119148e48e3af3cdaa"
          ]
        },
        "id": "LWOb6TfuRair",
        "outputId": "30e6d5dc-7e47-431f-f046-bef799497cbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading base LLaVA model...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "817af4ee28114961b8deec547b9d224a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "LlavaForConditionalGeneration(\n",
              "  (vision_tower): CLIPVisionModel(\n",
              "    (vision_model): CLIPVisionTransformer(\n",
              "      (embeddings): CLIPVisionEmbeddings(\n",
              "        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
              "        (position_embedding): Embedding(577, 1024)\n",
              "      )\n",
              "      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (encoder): CLIPEncoder(\n",
              "        (layers): ModuleList(\n",
              "          (0-23): 24 x CLIPEncoderLayer(\n",
              "            (self_attn): CLIPSdpaAttention(\n",
              "              (k_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
              "              (v_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
              "              (q_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
              "              (out_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
              "            )\n",
              "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): CLIPMLP(\n",
              "              (activation_fn): QuickGELUActivation()\n",
              "              (fc1): Linear4bit(in_features=1024, out_features=4096, bias=True)\n",
              "              (fc2): Linear4bit(in_features=4096, out_features=1024, bias=True)\n",
              "            )\n",
              "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (multi_modal_projector): LlavaMultiModalProjector(\n",
              "    (linear_1): Linear4bit(in_features=1024, out_features=5120, bias=True)\n",
              "    (act): GELUActivation()\n",
              "    (linear_2): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
              "  )\n",
              "  (language_model): LlamaForCausalLM(\n",
              "    (model): LlamaModel(\n",
              "      (embed_tokens): Embedding(32064, 5120)\n",
              "      (layers): ModuleList(\n",
              "        (0-39): 40 x LlamaDecoderLayer(\n",
              "          (self_attn): LlamaAttention(\n",
              "            (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
              "            (k_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
              "            (v_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
              "            (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
              "          )\n",
              "          (mlp): LlamaMLP(\n",
              "            (gate_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
              "            (up_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
              "            (down_proj): Linear4bit(in_features=13824, out_features=5120, bias=False)\n",
              "            (act_fn): SiLU()\n",
              "          )\n",
              "          (input_layernorm): LlamaRMSNorm((5120,), eps=1e-05)\n",
              "          (post_attention_layernorm): LlamaRMSNorm((5120,), eps=1e-05)\n",
              "        )\n",
              "      )\n",
              "      (norm): LlamaRMSNorm((5120,), eps=1e-05)\n",
              "      (rotary_emb): LlamaRotaryEmbedding()\n",
              "    )\n",
              "    (lm_head): Linear(in_features=5120, out_features=32064, bias=False)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import json, re\n",
        "from transformers import AutoProcessor, LlavaForConditionalGeneration, BitsAndBytesConfig\n",
        "\n",
        "# Configure 4-bit quantization if needed\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# Load base LLaVA model (load only once)\n",
        "model_name = \"llava-hf/llava-1.5-13b-hf\"\n",
        "processor = AutoProcessor.from_pretrained(\n",
        "    model_name,\n",
        "    token=\"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\"\n",
        ")\n",
        "\n",
        "print(\"Loading base LLaVA model...\")\n",
        "model = LlavaForConditionalGeneration.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config,\n",
        ")\n",
        "model.eval()\n",
        "model.to(\"cuda\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "BpeE4JXNAEBK",
        "outputId": "e472869f-1442-4103-8677-3751ae123fd8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ngraph TD\\n    A[Input Image] --> B(LLaVA Visual Analysis)\\n    B --> C[Textual Description]\\n    C --> D{Keyword Extraction}\\n    D --> E[Base Score Calculation]\\n    C --> F[Contextual Understanding]\\n    F --> G[Context Score]\\n    E --> H[Score Aggregator]\\n    G --> H\\n    H --> I((Final Compliance Score))\\n    I --> J[Moderation Action]\\n\\n'"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "graph TD\n",
        "    A[Input Image] --> B(LLaVA Visual Analysis)\n",
        "    B --> C[Textual Description]\n",
        "    C --> D{Keyword Extraction}\n",
        "    D --> E[Base Score Calculation]\n",
        "    C --> F[Contextual Understanding]\n",
        "    F --> G[Context Score]\n",
        "    E --> H[Score Aggregator]\n",
        "    G --> H\n",
        "    H --> I((Final Compliance Score))\n",
        "    I --> J[Moderation Action]\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgzmyW2EAdRP"
      },
      "outputs": [],
      "source": [
        "from scipy.special import expit as sigmoid\n",
        "\n",
        "def calculate_compliance_score(description, category_rules):\n",
        "    \"\"\"Calculate dynamic compliance score using sigmoid thresholding\"\"\"\n",
        "    base_score = sum(\n",
        "        category['weights'].get(word, 0) * count\n",
        "        for word, count in analyze_keywords(description).items()\n",
        "        for category in category_rules.values()\n",
        "    )\n",
        "\n",
        "    context_score = llava_context_analysis(description)  # 0-1 scale\n",
        "    adjusted_score = base_score * sigmoid(5*(context_score - 0.65))  # Œ±=5, œÑ=0.65\n",
        "    return min(100, max(0, int(adjusted_score * 100)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmSyt7AhAfU1"
      },
      "outputs": [],
      "source": [
        "def load_compliance_context(json_path):\n",
        "    with open(json_path) as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    categories = {}\n",
        "    for cat in data['subcategories']:\n",
        "        total_weight = sum(tag['weight'] for tag in cat['tags'])\n",
        "        categories[cat['name']] = {\n",
        "            'tags': {tag['name']: tag['weight']/total_weight for tag in cat['tags']},\n",
        "            'threshold': cat.get('threshold', 0.5)\n",
        "        }\n",
        "\n",
        "    return categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAP8LubcRiyk",
        "outputId": "b8b6a14c-9565-451f-e235-d9489ddd9a86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Compliance Context Summary:\n",
            "Content Compliance Framework:\n",
            "\n",
            "Generated Description:\n",
            " ### Instruction:\n",
            "You are a vision+text assistant.\n",
            "1) Describe the image in detail: objects, arrangement, color, usage, background details.\n",
            "2) Check for potential policy concerns (sensitive content, hate/violence, illegal, graphic) based on these guidelines: Content Compliance Framework:\n",
            "3) Important: End your response with a separate line exactly like: \"Compliance Score: XX/100\"\n",
            "   (where XX is a number from 0 to 100, with 0 meaning entirely safe and 100 extremely unsafe).\n",
            "\n",
            "### Input:\n",
            " \n",
            "\n",
            "### Response:\n",
            "\n",
            "In the image, a man is running down a street during a protest, with the smoke and tear gas filling the air. Several other people are scattered throughout the scene, some of them appearing to be protesters, and others seemingly observers. The man in the red and blue clothing appears to be in a hurry, possibly trying to escape the chaos happening around him.\n",
            "\n",
            "The presence of tear gas and smoke indicates that the situation is tense and potentially dangerous. The protesters may be exercising their right to express their opinions, but the potential for violence and unrest are evident in the scene.\n",
            "\n",
            "Compliance Score: 0/100\n",
            "\n",
            "Extracted Compliance Score: 0/100\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "def load_compliance_context(json_path):\n",
        "    # Open and load the JSON file from the provided path\n",
        "    with open(json_path, 'r') as f:\n",
        "        context = json.load(f)\n",
        "\n",
        "    # Extract the top-level name (e.g., \"Content Restrictions\")\n",
        "    top_name = context.get(\"name\", \"Compliance Guidelines\")\n",
        "\n",
        "    # Process each subcategory, listing its name and a sample of its tags\n",
        "    subcategories = context.get(\"subcategories\", [])\n",
        "    summary_parts = [f\"{top_name}:\"]\n",
        "\n",
        "    for subcat in subcategories:\n",
        "        cat_name = subcat.get(\"name\", \"Unnamed Category\")\n",
        "        # Include a few sample tags (first 3) for context; adjust as needed\n",
        "        sample_tags = \", \".join(subcat.get(\"tags\", [])[:3])\n",
        "        summary_parts.append(f\"{cat_name} (e.g., {sample_tags})\")\n",
        "\n",
        "    # Join the parts with a separator; you can use a comma or pipe depending on your preference\n",
        "    return \" | \".join(summary_parts)\n",
        "\n",
        "# Example usage:\n",
        "compliance_context = load_compliance_context(\"/content/compliance_rules.json\")\n",
        "print(\"Compliance Context Summary:\")\n",
        "print(compliance_context)\n",
        "\n",
        "# Define the function to generate the description\n",
        "def generate_image_description(image, compliance_context):\n",
        "    prompt = f\"\"\"### Instruction:\n",
        "You are a vision+text assistant.\n",
        "1) Describe the image in detail: objects, arrangement, color, usage, background details.\n",
        "2) Check for potential policy concerns (sensitive content, hate/violence, illegal, graphic) based on these guidelines: {compliance_context}\n",
        "3) Important: End your response with a separate line exactly like: \"Compliance Score: XX/100\"\n",
        "   (where XX is a number from 0 to 100, with 0 meaning entirely safe and 100 extremely unsafe).\n",
        "\n",
        "### Input:\n",
        "<image>\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "    inputs = processor(\n",
        "        images=image,\n",
        "        text=prompt,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generated_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=512,\n",
        "            do_sample=True,\n",
        "            temperature=0.7\n",
        "        )\n",
        "    output = processor.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "    return output\n",
        "\n",
        "# Post-processing: extract the compliance score from the generated description\n",
        "def extract_compliance_score(text):\n",
        "    match = re.search(r'Compliance Score:\\s*(\\d{1,3})/100', text)\n",
        "    if match:\n",
        "        return int(match.group(1))\n",
        "    return None\n",
        "\n",
        "# Example usage:\n",
        "image_path = \"/content/violant_image.jpg\"\n",
        "image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "description = generate_image_description(image, compliance_context)\n",
        "print(\"\\nGenerated Description:\\n\", description)\n",
        "\n",
        "score = extract_compliance_score(description)\n",
        "if score is not None:\n",
        "    print(f\"\\nExtracted Compliance Score: {score}/100\")\n",
        "else:\n",
        "    print(\"\\nCompliance Score not found in the generated output.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjFEWTXpJnIg"
      },
      "outputs": [],
      "source": [
        "# %%time\n",
        "# !pip install -q transformers accelerate torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105,
          "referenced_widgets": [
            "0768b0d7dd864bfabe5f12418dfcfe76",
            "ffa0fb1b67e3465884ad3f60f18ac13f",
            "ef3cc101cde64d5792edff060d849558",
            "e35686d576ff451ab13c236df9e2ba97",
            "384e1eaa7eac487f8c15a79137cebe54",
            "e6ba135bd754484d963f46368f9e7b6b",
            "d2ebf8f6dcc245b7b8a5e264ce17903c",
            "351a8622db5a40be9b01e1676f133405",
            "171ada5217eb4514b4f7b6342a06ebae",
            "5ee1eda0eff54491a024a5fcea46c87b",
            "623a38668f574f83953fde06cf584508"
          ]
        },
        "id": "xK1v7zcaT0qm",
        "outputId": "2778fb5d-25bd-49ac-8873-8c76d08a7f7f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading base LLaVA model...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0768b0d7dd864bfabe5f12418dfcfe76",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import LlavaForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\n",
        "\n",
        "def load_models():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # Configure 4-bit quantization\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "    )\n",
        "\n",
        "    model_name = \"llava-hf/llava-1.5-13b-hf\"\n",
        "    processor = AutoProcessor.from_pretrained(\n",
        "        model_name,\n",
        "        token=\"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\"\n",
        "    )\n",
        "\n",
        "    print(\"Loading base LLaVA model...\")\n",
        "    model = LlavaForConditionalGeneration.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=bnb_config,\n",
        "    )\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    return model, processor, device\n",
        "\n",
        "# Run once per session\n",
        "model, processor, device = load_models()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzbVUzi9BcNC"
      },
      "outputs": [],
      "source": [
        "# Content Moderation System Implementation\n",
        "import json\n",
        "import re\n",
        "from PIL import Image\n",
        "from scipy.special import expit as sigmoid\n",
        "from collections import defaultdict\n",
        "from functools import lru_cache\n",
        "import torch\n",
        "\n",
        "class ContentModerator:\n",
        "    def __init__(self, model, processor, rules_path=\"/content/compliance_rules.json\"):\n",
        "        \"\"\"Initialize Content Moderator with model, processor, and compliance rules.\"\"\"\n",
        "        self.model = model\n",
        "        self.processor = processor\n",
        "        print(f\"üîÑ Loading compliance rules from: {rules_path} (Type: {type(rules_path)})\")  # Debug\n",
        "        self.categories = self._load_rules(rules_path)\n",
        "        self.alpha = 3.0  # Reduced alpha to soften sigmoid scaling\n",
        "        self.base_threshold = 0.65\n",
        "        print(f\"‚úÖ Initialization model and processor (Type: {type(model)})\")  # Debug\n",
        "        print(f\"‚úÖ Initialization Complete (Alpha: {self.alpha}, Base Threshold: {self.base_threshold})\")  # Debug\n",
        "\n",
        "    def _load_rules(self, json_path):\n",
        "        \"\"\"Load compliance categories and normalize weights.\"\"\"\n",
        "        with open(json_path) as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        print(f\"üìÇ JSON Data Loaded (Type: {type(data)})\")  # Debug\n",
        "        print(f\"üõ† Categories Found: {len(data['categories'])} (Type: {type(data['categories'])})\")  # Debug\n",
        "\n",
        "        categories = {}\n",
        "        for cat in data[\"categories\"]:\n",
        "            print(f\"‚û° Processing Category: {cat['name']} (Type: {type(cat)})\")  # Debug\n",
        "            total = sum(t[\"weight\"] for t in cat[\"tags\"])  # Sum of all tag weights\n",
        "\n",
        "            print(f\"üìä Total Tag Weight for {cat['name']}: {total} (Type: {type(total)})\")  # Debug\n",
        "\n",
        "            categories[cat[\"name\"]] = {\n",
        "                \"tags\": {t[\"name\"]: t[\"weight\"] / total for t in cat[\"tags\"]},\n",
        "                \"threshold\": cat.get(\"threshold\", 0.5),\n",
        "                \"severity_scale\": cat.get(\"severity_scale\", 1.0)  # Apply severity scaling\n",
        "            }\n",
        "\n",
        "            print(f\"‚úî Normalized Weights for {cat['name']}: {categories[cat['name']]['tags']} (Type: {type(categories[cat['name']]['tags'])})\")  # Debug\n",
        "\n",
        "        print(f\"üîç Final Categories Structure: {categories} (Type: {type(categories)})\")  # Debug\n",
        "        return categories\n",
        "\n",
        "    @lru_cache(maxsize=1000)\n",
        "    def _analyze_text(self, text):\n",
        "        \"\"\"Analyze text for keyword matches and compute severity scores.\"\"\"\n",
        "        print(f\"üìù Text to Analyze: {text} (Type: {type(text)})\")  # Debug\n",
        "        text = text.lower()\n",
        "        matches = defaultdict(float)\n",
        "\n",
        "        print(f\"üìä Available Categories: {list(self.categories.keys())} (Type: {type(self.categories)})\")  # Debug\n",
        "\n",
        "        for cat, data in self.categories.items():\n",
        "            print(f\"üîé Checking Category: {cat} (Type: {type(cat)})\")  # Debug\n",
        "            for kw, wt in data[\"tags\"].items():\n",
        "                if re.search(rf'\\b{re.escape(kw)}s?\\b', text):\n",
        "                    print(f\"‚úÖ Matched Keyword: {kw} (Type: {type(kw)}) in Category: {cat}\")  # Debug\n",
        "                    matches[cat] += wt\n",
        "                    print(f\"üìà Updated Match Score for {cat}: {matches[cat]} (Type: {type(matches[cat])})\")  # Debug\n",
        "\n",
        "        print(f\"üìå Final Match Scores: {matches} (Type: {type(matches)})\")  # Debug\n",
        "        return dict(matches)\n",
        "\n",
        "    def _get_context_score(self, description):\n",
        "        \"\"\"Extract compliance score from LLaVA model output or estimate based on keyword density.\"\"\"\n",
        "        print(f\"üìú Model Output Description: {description} (Type: {type(description)})\")  # Debug\n",
        "        match = re.search(r'\\[Score:\\s*(\\d+)/100\\]', description)\n",
        "\n",
        "        if match:\n",
        "            score = int(match.group(1)) / 100\n",
        "            print(f\"üéØ Extracted Context Score: {score} (Type: {type(score)})\")  # Debug\n",
        "            return score\n",
        "\n",
        "        # If no explicit score, estimate based on keyword density\n",
        "        print(\"‚ö†Ô∏è No explicit score found in description. Estimating context score...\")\n",
        "        word_count = len(description.split())\n",
        "        keyword_density = len(self._analyze_text(description)) / max(word_count, 1)\n",
        "        estimated_score = min(1.0, max(0.5, keyword_density * 2))\n",
        "        print(f\"üìä Estimated Context Score: {estimated_score}\")\n",
        "        return estimated_score\n",
        "\n",
        "    def analyze(self, image_path):\n",
        "        \"\"\"Analyze an image for compliance scoring.\"\"\"\n",
        "        print(f\"üñºÔ∏è Processing Image: {image_path} (Type: {type(image_path)})\")  # Debug\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "        print(f\"‚úÖ Image Loaded Successfully (Mode: {image.mode}, Size: {image.size})\")  # Debug\n",
        "\n",
        "        prompt = \"\"\" <image> Describe this image in detail. Include:\n",
        "- Objects present\n",
        "- Actions being performed\n",
        "- Clothing, expressions, and gestures\n",
        "- Background and scene description\n",
        "- Any elements that might relate to compliance rules \"\"\"\n",
        "\n",
        "        print(f\"üìù Generated Prompt: {prompt} (Type: {type(prompt)})\")  # Debug\n",
        "\n",
        "\n",
        "        inputs = self.processor(\n",
        "        images=image,\n",
        "        text=prompt,\n",
        "        return_tensors=\"pt\"\n",
        "        ).to(\"cuda\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            generated_ids = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=512,\n",
        "                do_sample=True,\n",
        "                temperature=0.7\n",
        "              )\n",
        "        desc = processor.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "\n",
        "        # # Generate description\n",
        "        # inputs = self.processor(\n",
        "        #     text=prompt,\n",
        "        #     images=image,\n",
        "        #     return_tensors=\"pt\"\n",
        "        # ).to(device, torch.float16)\n",
        "\n",
        "        # print(f\"üõ† Model Input Prepared (Keys: {inputs.keys()}, Type: {type(inputs)})\")  # Debug\n",
        "\n",
        "        # with torch.inference_mode():\n",
        "        #     out = self.model.generate(**inputs, max_new_tokens=512)\n",
        "        #     print(f\"üöÄ Model Generated Output: {out} (Type: {type(out)}\")\n",
        "\n",
        "        # desc = self.processor.decode(out[0], skip_special_tokens=True)\n",
        "        #desc = processor.tokenizer.batch_decode(inputs, skip_special_tokens=True)[0]\n",
        "\n",
        "        print(f\"üìú Generated Description: {desc} (Type: {type(desc)})\")  # Debug\n",
        "\n",
        "        # Calculate compliance scores\n",
        "        ctx_score = self._get_context_score(desc)\n",
        "        matches = self._analyze_text(desc.lower())\n",
        "\n",
        "        print(f\"üìä Context Score: {ctx_score} (Type: {type(ctx_score)})\")  # Debug\n",
        "        print(f\"üìå Matched Keywords & Scores: {matches} (Type: {type(matches)})\")  # Debug\n",
        "\n",
        "        final_scores = {}\n",
        "        for cat, score in matches.items():\n",
        "            severity_scale = self.categories[cat][\"severity_scale\"]\n",
        "            print(f\"üîç Category: {cat}, Score: {score}, Severity Scale: {severity_scale}\")  # Debug\n",
        "            adj = (score * severity_scale) * sigmoid(self.alpha * (ctx_score - self.base_threshold))\n",
        "            print(f\"üìä Adjusted Score for adj {cat}: {adj} (Type: {type(adj)}) {min(100, int(adj * 1000))}\")  # Debug\n",
        "            final_scores[cat] = min(100, int(adj * 1000))\n",
        "            print(f\"üìà Adjusted Score for vah {cat}: {final_scores[cat]} (Type: {type(final_scores[cat])})\")  # Debug\n",
        "\n",
        "        print(f\"üìä Final Scores: {final_scores} (Type: {type(final_scores)})\")  # Debug\n",
        "        overall_score = max(final_scores.values(), default=0)A\n",
        "        print(f\"üö® Final Compliance Scores: {final_scores} (Type: {type(final_scores)})\")  # Debug\n",
        "        print(f\"üèÜ Overall Risk Score: {overall_score} (Type: {type(overall_score)})\")  # Debug\n",
        "\n",
        "        return {\n",
        "            \"description\": desc,\n",
        "            \"scores\": final_scores,\n",
        "            \"context_score\": int(ctx_score * 100),\n",
        "            \"overall\": overall_score\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjjI3010BcQa",
        "outputId": "78c99367-928c-48d1-aa1a-dd31548aade5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Loading compliance rules from: /content/compliance_rules.json (Type: <class 'str'>)\n",
            "üìÇ JSON Data Loaded (Type: <class 'dict'>)\n",
            "üõ† Categories Found: 2 (Type: <class 'list'>)\n",
            "‚û° Processing Category: Sensitive Content (Type: <class 'dict'>)\n",
            "üìä Total Tag Weight for Sensitive Content: 53.83 (Type: <class 'float'>)\n",
            "‚úî Normalized Weights for Sensitive Content: {'actions': 0.009288500835965075, 'acts': 0.009288500835965075, 'adult': 0.009288500835965075, 'alluring': 0.009288500835965075, 'analysis': 0.009288500835965075, 'anatomical': 0.009288500835965075, 'area': 0.009288500835965075, 'attitudes': 0.009288500835965075, 'bare': 0.009288500835965075, 'beach': 0.009288500835965075, 'body': 0.009288500835965075, 'boobs': 0.009288500835965075, 'connection': 0.009288500835965075, 'content': 0.009288500835965075, 'copulation': 0.009288500835965075, 'crude': 0.009288500835965075, 'cues': 0.009288500835965075, 'cultural': 0.009288500835965075, 'culturally': 0.009288500835965075, 'depictions': 0.009288500835965075, 'detection': 0.009288500835965075, 'emblems': 0.009288500835965075, 'entertainment': 0.009288500835965075, 'enticing': 0.009288500835965075, 'erotic': 0.009288500835965075, 'explicit': 0.016719301504737136, 'exposed': 0.009288500835965075, 'expressions': 0.009288500835965075, 'faith-based': 0.009288500835965075, 'feature': 0.009288500835965075, 'female': 0.009288500835965075, 'flirtatious': 0.009288500835965075, 'genital': 0.009288500835965075, 'genitalia': 0.016347761471298534, 'genitals': 0.009288500835965075, 'gestures': 0.009288500835965075, 'graphic': 0.009288500835965075, 'heritage': 0.009288500835965075, 'human': 0.009288500835965075, 'icons': 0.009288500835965075, 'identification': 0.009288500835965075, 'illustrations': 0.009288500835965075, 'imagery': 0.009288500835965075, 'insignias': 0.009288500835965075, 'intercourse': 0.009288500835965075, 'intimacy': 0.009288500835965075, 'intimate': 0.009288500835965075, 'language': 0.009288500835965075, 'logos': 0.009288500835965075, 'lovemaking': 0.009288500835965075, 'male': 0.009288500835965075, 'man': 0.009288500835965075, 'marks': 0.009288500835965075, 'mature': 0.009288500835965075, 'motions': 0.009288500835965075, 'movements': 0.009288500835965075, 'naked': 0.009288500835965075, 'nipple': 0.009288500835965075, 'nipples': 0.009288500835965075, 'nude': 0.015790451421140626, 'nudity': 0.009288500835965075, 'obscene': 0.009288500835965075, 'offensive': 0.009288500835965075, 'part': 0.009288500835965075, 'partial': 0.009288500835965075, 'penis': 0.009288500835965075, 'physical': 0.009288500835965075, 'porn': 0.009288500835965075, 'pornography': 0.017648151588333643, 'portrayals': 0.009288500835965075, 'poses': 0.009288500835965075, 'positions': 0.009288500835965075, 'postures': 0.009288500835965075, 'private': 0.009288500835965075, 'profane': 0.009288500835965075, 'provocative': 0.009288500835965075, 'recognition': 0.009288500835965075, 'region': 0.009288500835965075, 'remarks': 0.009288500835965075, 'representations': 0.009288500835965075, 'sacred': 0.009288500835965075, 'seductive': 0.009288500835965075, 'segmentation': 0.009288500835965075, 'sensual': 0.009288500835965075, 'sex': 0.009288500835965075, 'sexual': 0.009288500835965075, 'sexuality': 0.009288500835965075, 'signals': 0.009288500835965075, 'significant': 0.009288500835965075, 'speech': 0.009288500835965075, 'spiritual': 0.009288500835965075, 'stances': 0.009288500835965075, 'structure': 0.009288500835965075, 'suggestive': 0.009288500835965075, 'symbols': 0.009288500835965075, 'tempting': 0.009288500835965075, 'terminology': 0.009288500835965075, 'topless': 0.013932751253947613, 'tradition-sensitive': 0.009288500835965075, 'usage': 0.009288500835965075, 'visuals': 0.009288500835965075, 'vivid': 0.009288500835965075, 'vulgar': 0.009288500835965075, 'women': 0.009288500835965075} (Type: <class 'dict'>)\n",
            "‚û° Processing Category: Hate Speech & Violence (Type: <class 'dict'>)\n",
            "üìä Total Tag Weight for Hate Speech & Violence: 39.36 (Type: <class 'float'>)\n",
            "‚úî Normalized Weights for Hate Speech & Violence: {'abduction': 0.012703252032520325, 'abuse': 0.012703252032520325, 'aggressive': 0.012703252032520325, 'armed': 0.012703252032520325, 'assassination': 0.012703252032520325, 'assault': 0.012703252032520325, 'attack': 0.012703252032520325, 'battle': 0.012703252032520325, 'bigotry': 0.012703252032520325, 'bodily': 0.012703252032520325, 'bully': 0.012703252032520325, 'capturing': 0.012703252032520325, 'coercive': 0.012703252032520325, 'combat': 0.012703252032520325, 'confinement': 0.012703252032520325, 'conflict': 0.012703252032520325, 'confrontation': 0.012703252032520325, 'corporal': 0.012703252032520325, 'cutting': 0.012703252032520325, 'damage': 0.012703252032520325, 'declarations': 0.012703252032520325, 'discrimination': 0.012703252032520325, 'execution': 0.012703252032520325, 'extremism': 0.012703252032520325, 'fight': 0.012703252032520325, 'forcible': 0.012703252032520325, 'guerrilla': 0.012703252032520325, 'harass': 0.012703252032520325, 'harassment': 0.012703252032520325, 'harm': 0.012703252032520325, 'hate': 0.012703252032520325, 'homicide': 0.012703252032520325, 'homophobia': 0.012703252032520325, 'hostage-taking': 0.012703252032520325, 'hostile': 0.012703252032520325, 'hostilities': 0.012703252032520325, 'hurt': 0.012703252032520325, 'injury': 0.012703252032520325, 'insurgency': 0.012703252032520325, 'intimidate': 0.012703252032520325, 'intimidating': 0.012703252032520325, 'intimidation': 0.012703252032520325, 'kidnapping': 0.012703252032520325, 'killing': 0.012703252032520325, 'manslaughter': 0.012703252032520325, 'menacing': 0.012703252032520325, 'messages': 0.012703252032520325, 'militancy': 0.012703252032520325, 'military': 0.012703252032520325, 'misogyny': 0.012703252032520325, 'murder': 0.02464430894308943, 'physical': 0.012703252032520325, 'political': 0.012703252032520325, 'racism': 0.012703252032520325, 'radicalism': 0.012703252032520325, 'remarks': 0.012703252032520325, 'seizure': 0.012703252032520325, 'self_harm': 0.012703252032520325, 'severing': 0.012703252032520325, 'skirmish': 0.012703252032520325, 'slaying': 0.012703252032520325, 'slicing': 0.012703252032520325, 'snatching': 0.012703252032520325, 'statements': 0.012703252032520325, 'suicide': 0.012703252032520325, 'terrorism': 0.025152439024390245, 'threatening': 0.012703252032520325, 'threats': 0.012703252032520325, 'transphobia': 0.012703252032520325, 'trauma': 0.012703252032520325, 'violence': 0.012703252032520325, 'violent': 0.012703252032520325, 'warfare': 0.012703252032520325, 'warnings': 0.012703252032520325, 'wounding': 0.012703252032520325, 'xenophobia': 0.022865853658536585} (Type: <class 'dict'>)\n",
            "üîç Final Categories Structure: {'Sensitive Content': {'tags': {'actions': 0.009288500835965075, 'acts': 0.009288500835965075, 'adult': 0.009288500835965075, 'alluring': 0.009288500835965075, 'analysis': 0.009288500835965075, 'anatomical': 0.009288500835965075, 'area': 0.009288500835965075, 'attitudes': 0.009288500835965075, 'bare': 0.009288500835965075, 'beach': 0.009288500835965075, 'body': 0.009288500835965075, 'boobs': 0.009288500835965075, 'connection': 0.009288500835965075, 'content': 0.009288500835965075, 'copulation': 0.009288500835965075, 'crude': 0.009288500835965075, 'cues': 0.009288500835965075, 'cultural': 0.009288500835965075, 'culturally': 0.009288500835965075, 'depictions': 0.009288500835965075, 'detection': 0.009288500835965075, 'emblems': 0.009288500835965075, 'entertainment': 0.009288500835965075, 'enticing': 0.009288500835965075, 'erotic': 0.009288500835965075, 'explicit': 0.016719301504737136, 'exposed': 0.009288500835965075, 'expressions': 0.009288500835965075, 'faith-based': 0.009288500835965075, 'feature': 0.009288500835965075, 'female': 0.009288500835965075, 'flirtatious': 0.009288500835965075, 'genital': 0.009288500835965075, 'genitalia': 0.016347761471298534, 'genitals': 0.009288500835965075, 'gestures': 0.009288500835965075, 'graphic': 0.009288500835965075, 'heritage': 0.009288500835965075, 'human': 0.009288500835965075, 'icons': 0.009288500835965075, 'identification': 0.009288500835965075, 'illustrations': 0.009288500835965075, 'imagery': 0.009288500835965075, 'insignias': 0.009288500835965075, 'intercourse': 0.009288500835965075, 'intimacy': 0.009288500835965075, 'intimate': 0.009288500835965075, 'language': 0.009288500835965075, 'logos': 0.009288500835965075, 'lovemaking': 0.009288500835965075, 'male': 0.009288500835965075, 'man': 0.009288500835965075, 'marks': 0.009288500835965075, 'mature': 0.009288500835965075, 'motions': 0.009288500835965075, 'movements': 0.009288500835965075, 'naked': 0.009288500835965075, 'nipple': 0.009288500835965075, 'nipples': 0.009288500835965075, 'nude': 0.015790451421140626, 'nudity': 0.009288500835965075, 'obscene': 0.009288500835965075, 'offensive': 0.009288500835965075, 'part': 0.009288500835965075, 'partial': 0.009288500835965075, 'penis': 0.009288500835965075, 'physical': 0.009288500835965075, 'porn': 0.009288500835965075, 'pornography': 0.017648151588333643, 'portrayals': 0.009288500835965075, 'poses': 0.009288500835965075, 'positions': 0.009288500835965075, 'postures': 0.009288500835965075, 'private': 0.009288500835965075, 'profane': 0.009288500835965075, 'provocative': 0.009288500835965075, 'recognition': 0.009288500835965075, 'region': 0.009288500835965075, 'remarks': 0.009288500835965075, 'representations': 0.009288500835965075, 'sacred': 0.009288500835965075, 'seductive': 0.009288500835965075, 'segmentation': 0.009288500835965075, 'sensual': 0.009288500835965075, 'sex': 0.009288500835965075, 'sexual': 0.009288500835965075, 'sexuality': 0.009288500835965075, 'signals': 0.009288500835965075, 'significant': 0.009288500835965075, 'speech': 0.009288500835965075, 'spiritual': 0.009288500835965075, 'stances': 0.009288500835965075, 'structure': 0.009288500835965075, 'suggestive': 0.009288500835965075, 'symbols': 0.009288500835965075, 'tempting': 0.009288500835965075, 'terminology': 0.009288500835965075, 'topless': 0.013932751253947613, 'tradition-sensitive': 0.009288500835965075, 'usage': 0.009288500835965075, 'visuals': 0.009288500835965075, 'vivid': 0.009288500835965075, 'vulgar': 0.009288500835965075, 'women': 0.009288500835965075}, 'threshold': 0.65, 'severity_scale': 1.2}, 'Hate Speech & Violence': {'tags': {'abduction': 0.012703252032520325, 'abuse': 0.012703252032520325, 'aggressive': 0.012703252032520325, 'armed': 0.012703252032520325, 'assassination': 0.012703252032520325, 'assault': 0.012703252032520325, 'attack': 0.012703252032520325, 'battle': 0.012703252032520325, 'bigotry': 0.012703252032520325, 'bodily': 0.012703252032520325, 'bully': 0.012703252032520325, 'capturing': 0.012703252032520325, 'coercive': 0.012703252032520325, 'combat': 0.012703252032520325, 'confinement': 0.012703252032520325, 'conflict': 0.012703252032520325, 'confrontation': 0.012703252032520325, 'corporal': 0.012703252032520325, 'cutting': 0.012703252032520325, 'damage': 0.012703252032520325, 'declarations': 0.012703252032520325, 'discrimination': 0.012703252032520325, 'execution': 0.012703252032520325, 'extremism': 0.012703252032520325, 'fight': 0.012703252032520325, 'forcible': 0.012703252032520325, 'guerrilla': 0.012703252032520325, 'harass': 0.012703252032520325, 'harassment': 0.012703252032520325, 'harm': 0.012703252032520325, 'hate': 0.012703252032520325, 'homicide': 0.012703252032520325, 'homophobia': 0.012703252032520325, 'hostage-taking': 0.012703252032520325, 'hostile': 0.012703252032520325, 'hostilities': 0.012703252032520325, 'hurt': 0.012703252032520325, 'injury': 0.012703252032520325, 'insurgency': 0.012703252032520325, 'intimidate': 0.012703252032520325, 'intimidating': 0.012703252032520325, 'intimidation': 0.012703252032520325, 'kidnapping': 0.012703252032520325, 'killing': 0.012703252032520325, 'manslaughter': 0.012703252032520325, 'menacing': 0.012703252032520325, 'messages': 0.012703252032520325, 'militancy': 0.012703252032520325, 'military': 0.012703252032520325, 'misogyny': 0.012703252032520325, 'murder': 0.02464430894308943, 'physical': 0.012703252032520325, 'political': 0.012703252032520325, 'racism': 0.012703252032520325, 'radicalism': 0.012703252032520325, 'remarks': 0.012703252032520325, 'seizure': 0.012703252032520325, 'self_harm': 0.012703252032520325, 'severing': 0.012703252032520325, 'skirmish': 0.012703252032520325, 'slaying': 0.012703252032520325, 'slicing': 0.012703252032520325, 'snatching': 0.012703252032520325, 'statements': 0.012703252032520325, 'suicide': 0.012703252032520325, 'terrorism': 0.025152439024390245, 'threatening': 0.012703252032520325, 'threats': 0.012703252032520325, 'transphobia': 0.012703252032520325, 'trauma': 0.012703252032520325, 'violence': 0.012703252032520325, 'violent': 0.012703252032520325, 'warfare': 0.012703252032520325, 'warnings': 0.012703252032520325, 'wounding': 0.012703252032520325, 'xenophobia': 0.022865853658536585}, 'threshold': 0.55, 'severity_scale': 1.4}} (Type: <class 'dict'>)\n",
            "‚úÖ Initialization model and processor (Type: <class 'transformers.models.llava.modeling_llava.LlavaForConditionalGeneration'>)\n",
            "‚úÖ Initialization Complete (Alpha: 3.0, Base Threshold: 0.65)\n",
            "üñºÔ∏è Processing Image: /content/HalfNaked_900.jpg (Type: <class 'str'>)\n",
            "‚úÖ Image Loaded Successfully (Mode: RGB, Size: (900, 550))\n",
            "üìù Generated Prompt:  <image> Describe this image in detail. Include:\n",
            "- Objects present\n",
            "- Actions being performed\n",
            "- Clothing, expressions, and gestures\n",
            "- Background and scene description\n",
            "- Any elements that might relate to compliance rules  (Type: <class 'str'>)\n",
            "üìú Generated Description:    Describe this image in detail. Include:\n",
            "- Objects present\n",
            "- Actions being performed\n",
            "- Clothing, expressions, and gestures\n",
            "- Background and scene description\n",
            "- Any elements that might relate to compliance rules \n",
            "- Any specific rules to follow\n",
            "\n",
            "Three bottles of Naked beverages are featured in the image. (Type: <class 'str'>)\n",
            "üìú Model Output Description:    Describe this image in detail. Include:\n",
            "- Objects present\n",
            "- Actions being performed\n",
            "- Clothing, expressions, and gestures\n",
            "- Background and scene description\n",
            "- Any elements that might relate to compliance rules \n",
            "- Any specific rules to follow\n",
            "\n",
            "Three bottles of Naked beverages are featured in the image. (Type: <class 'str'>)\n",
            "‚ö†Ô∏è No explicit score found in description. Estimating context score...\n",
            "üìù Text to Analyze:    Describe this image in detail. Include:\n",
            "- Objects present\n",
            "- Actions being performed\n",
            "- Clothing, expressions, and gestures\n",
            "- Background and scene description\n",
            "- Any elements that might relate to compliance rules \n",
            "- Any specific rules to follow\n",
            "\n",
            "Three bottles of Naked beverages are featured in the image. (Type: <class 'str'>)\n",
            "üìä Available Categories: ['Sensitive Content', 'Hate Speech & Violence'] (Type: <class 'dict'>)\n",
            "üîé Checking Category: Sensitive Content (Type: <class 'str'>)\n",
            "‚úÖ Matched Keyword: actions (Type: <class 'str'>) in Category: Sensitive Content\n",
            "üìà Updated Match Score for Sensitive Content: 0.009288500835965075 (Type: <class 'float'>)\n",
            "‚úÖ Matched Keyword: expressions (Type: <class 'str'>) in Category: Sensitive Content\n",
            "üìà Updated Match Score for Sensitive Content: 0.01857700167193015 (Type: <class 'float'>)\n",
            "‚úÖ Matched Keyword: gestures (Type: <class 'str'>) in Category: Sensitive Content\n",
            "üìà Updated Match Score for Sensitive Content: 0.027865502507895226 (Type: <class 'float'>)\n",
            "‚úÖ Matched Keyword: naked (Type: <class 'str'>) in Category: Sensitive Content\n",
            "üìà Updated Match Score for Sensitive Content: 0.0371540033438603 (Type: <class 'float'>)\n",
            "üîé Checking Category: Hate Speech & Violence (Type: <class 'str'>)\n",
            "üìå Final Match Scores: defaultdict(<class 'float'>, {'Sensitive Content': 0.0371540033438603}) (Type: <class 'collections.defaultdict'>)\n",
            "üìä Estimated Context Score: 0.5\n",
            "üìù Text to Analyze:    describe this image in detail. include:\n",
            "- objects present\n",
            "- actions being performed\n",
            "- clothing, expressions, and gestures\n",
            "- background and scene description\n",
            "- any elements that might relate to compliance rules \n",
            "- any specific rules to follow\n",
            "\n",
            "three bottles of naked beverages are featured in the image. (Type: <class 'str'>)\n",
            "üìä Available Categories: ['Sensitive Content', 'Hate Speech & Violence'] (Type: <class 'dict'>)\n",
            "üîé Checking Category: Sensitive Content (Type: <class 'str'>)\n",
            "‚úÖ Matched Keyword: actions (Type: <class 'str'>) in Category: Sensitive Content\n",
            "üìà Updated Match Score for Sensitive Content: 0.009288500835965075 (Type: <class 'float'>)\n",
            "‚úÖ Matched Keyword: expressions (Type: <class 'str'>) in Category: Sensitive Content\n",
            "üìà Updated Match Score for Sensitive Content: 0.01857700167193015 (Type: <class 'float'>)\n",
            "‚úÖ Matched Keyword: gestures (Type: <class 'str'>) in Category: Sensitive Content\n",
            "üìà Updated Match Score for Sensitive Content: 0.027865502507895226 (Type: <class 'float'>)\n",
            "‚úÖ Matched Keyword: naked (Type: <class 'str'>) in Category: Sensitive Content\n",
            "üìà Updated Match Score for Sensitive Content: 0.0371540033438603 (Type: <class 'float'>)\n",
            "üîé Checking Category: Hate Speech & Violence (Type: <class 'str'>)\n",
            "üìå Final Match Scores: defaultdict(<class 'float'>, {'Sensitive Content': 0.0371540033438603}) (Type: <class 'collections.defaultdict'>)\n",
            "üìä Context Score: 0.5 (Type: <class 'float'>)\n",
            "üìå Matched Keywords & Scores: {'Sensitive Content': 0.0371540033438603} (Type: <class 'dict'>)\n",
            "üîç Category: Sensitive Content, Score: 0.0371540033438603, Severity Scale: 1.2\n",
            "üìä Adjusted Score for adj Sensitive Content: 0.017359573444582336 (Type: <class 'numpy.float64'>) 17\n",
            "üìà Adjusted Score for vah Sensitive Content: 17 (Type: <class 'int'>)\n",
            "üìä Final Scores: {'Sensitive Content': 17} (Type: <class 'dict'>)\n",
            "üö® Final Compliance Scores: {'Sensitive Content': 17} (Type: <class 'dict'>)\n",
            "üèÜ Overall Risk Score: 17 (Type: <class 'int'>)\n",
            "results :  {'description': '   Describe this image in detail. Include:\\n- Objects present\\n- Actions being performed\\n- Clothing, expressions, and gestures\\n- Background and scene description\\n- Any elements that might relate to compliance rules \\n- Any specific rules to follow\\n\\nThree bottles of Naked beverages are featured in the image.', 'scores': {'Sensitive Content': 17}, 'context_score': 50, 'overall': 17}\n",
            "üñºÔ∏è Image Analysis:\n",
            "   Describe this image in detail. Include:\n",
            "- Objects present\n",
            "- Actions being performed\n",
            "- Clothing, expressions, and gestures\n",
            "- Background and scene description\n",
            "- Any elements that might relate to compliance rules \n",
            "- Any specific rules to follow\n",
            "\n",
            "Three bottles of Naked beverages are featured in the image.\n",
            "\n",
            "üîç Compliance Scores:\n",
            "- Sensitive Content: 17/100\n",
            "\n",
            "üö® Overall Risk: 17/100 (Context: 50/100)\n"
          ]
        }
      ],
      "source": [
        "# Cell 3: Usage Example\n",
        "# Initialize once per session\n",
        "moderator = ContentModerator(model, processor)\n",
        "\n",
        "# Analyze image\n",
        "results = moderator.analyze(\"/content/HalfNaked_900.jpg\")\n",
        "# results = moderator.analyze(\"/content/adult_image.jpg\")\n",
        "# results = moderator.analyze(\"/content/boobs_papa.jpg\")\n",
        "# results = moderator.analyze(\"/content/javhd-157.jpg\")\n",
        "\n",
        "print(\"results : \", results)\n",
        "print(\"üñºÔ∏è Image Analysis:\")\n",
        "print(results[\"description\"])\n",
        "print(\"\\nüîç Compliance Scores:\")\n",
        "for cat, score in results[\"scores\"].items():\n",
        "    print(f\"- {cat}: {score}/100\")\n",
        "print(f\"\\nüö® Overall Risk: {results['overall']}/100 (Context: {results['context_score']}/100)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "44na6mvhBcS4"
      },
      "outputs": [],
      "source": [
        "# pip install scikit-learn numpy pandas nltk spacy sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3elSZ0QjSFb0",
        "outputId": "e2d858a8-4953-433a-e60b-5475b1ab09bc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîç DEBUGGING TRACE STARTED\n",
            "\n",
            "\n",
            "üîπ Checking Compliance for: In the image, a man and a woman are lying on a bed and engaging in a lustful act. The woman is straddling the man while getting intimate with him. The scene is quite provocative, as both individuals appear to be enjoying their time together. The woman appears to be fully naked, and the man's intentions are clear as they both participate in their erotic pursuits.\n",
            "\n",
            "üìå Rule-Based Filtering: Found ['adult_content'] in text: In the image, a man and a woman are lying on a bed and engaging in a lustful act. The woman is straddling the man while getting intimate with him. The scene is quite provocative, as both individuals appear to be enjoying their time together. The woman appears to be fully naked, and the man's intentions are clear as they both participate in their erotic pursuits.\n",
            "üìå Transformer Model Classification: Safe for text: In the image, a man and a woman are lying on a bed and engaging in a lustful act. The woman is straddling the man while getting intimate with him. The scene is quite provocative, as both individuals appear to be enjoying their time together. The woman appears to be fully naked, and the man's intentions are clear as they both participate in their erotic pursuits.\n",
            "üìå Key Terms Extracted (Stop words removed): ['image', ',', 'man', 'woman', 'lying', 'bed', 'engaging', 'lustful', 'act', '.', 'woman', 'straddling', 'man', 'getting', 'intimate', '.', 'scene', 'quite', 'provocative', ',', 'individuals', 'appear', 'enjoying', 'time', 'together', '.', 'woman', 'appears', 'fully', 'naked', ',', 'man', \"'s\", 'intentions', 'clear', 'participate', 'erotic', 'pursuits', '.']\n",
            "üìå Named Entities Detected: []\n",
            "üìå Semantic Similarity Check: In the image, a man and a woman are lying on a bed and engaging in a lustful act. The woman is straddling the man while getting intimate with him. The scene is quite provocative, as both individuals appear to be enjoying their time together. The woman appears to be fully naked, and the man's intentions are clear as they both participate in their erotic pursuits. vs Assume -> Score: 0.13294166326522827\n",
            "üìå Semantic Similarity Check: In the image, a man and a woman are lying on a bed and engaging in a lustful act. The woman is straddling the man while getting intimate with him. The scene is quite provocative, as both individuals appear to be enjoying their time together. The woman appears to be fully naked, and the man's intentions are clear as they both participate in their erotic pursuits. vs Bass Guitar -> Score: 0.09547711163759232\n",
            "üìå Semantic Similarity Check: In the image, a man and a woman are lying on a bed and engaging in a lustful act. The woman is straddling the man while getting intimate with him. The scene is quite provocative, as both individuals appear to be enjoying their time together. The woman appears to be fully naked, and the man's intentions are clear as they both participate in their erotic pursuits. vs Bare Minerals -> Score: 0.1587095558643341\n",
            "üîπ Final Decision: Violation - Content classified as: ['adult_content'].\n",
            "\n",
            "\n",
            "üîπ Checking Compliance for: Three bottles of Naked beverages are featured in the image.\n",
            "\n",
            "üìå Rule-Based Filtering: Found [] in text: Three bottles of Naked beverages are featured in the image.\n",
            "üìå Key Terms Extracted (Stop words removed): ['three', 'bottles', 'naked', 'beverages', 'featured', 'image', '.']\n",
            "üìå Named Entities Detected: ['Three']\n",
            "üìå Semantic Similarity Check: Three bottles of Naked beverages are featured in the image. vs Assume -> Score: 0.042056165635585785\n",
            "üìå Semantic Similarity Check: Three bottles of Naked beverages are featured in the image. vs Bass Guitar -> Score: 0.039606597274541855\n",
            "üìå Semantic Similarity Check: Three bottles of Naked beverages are featured in the image. vs Bare Minerals -> Score: 0.18799862265586853\n",
            "üîπ Final Decision: Safe - \n",
            "\n",
            "\n",
            "üîπ Checking Compliance for: The image shows three bottles of a green beverage - Naked Half Naked - in three different colors.\n",
            "\n",
            "üìå Rule-Based Filtering: Found [] in text: The image shows three bottles of a green beverage - Naked Half Naked - in three different colors.\n",
            "üìå Key Terms Extracted (Stop words removed): ['image', 'shows', 'three', 'bottles', 'green', 'beverage', '-', 'naked', 'half', 'naked', '-', 'three', 'different', 'colors', '.']\n",
            "üìå Named Entities Detected: ['three', 'three']\n",
            "üìå Semantic Similarity Check: The image shows three bottles of a green beverage - Naked Half Naked - in three different colors. vs Assume -> Score: 0.101212278008461\n",
            "üìå Semantic Similarity Check: The image shows three bottles of a green beverage - Naked Half Naked - in three different colors. vs Bass Guitar -> Score: 0.07878946512937546\n",
            "üìå Semantic Similarity Check: The image shows three bottles of a green beverage - Naked Half Naked - in three different colors. vs Bare Minerals -> Score: 0.22803929448127747\n",
            "üîπ Final Decision: Safe - \n",
            "\n",
            "\n",
            "üîπ Checking Compliance for: I wanna fuck you up dog ass hole mother fuckr bitch I will fuck you up.\n",
            "\n",
            "üìå Rule-Based Filtering: Found ['harsh_language'] in text: I wanna fuck you up dog ass hole mother fuckr bitch I will fuck you up.\n",
            "üìå Transformer Model Classification: Hate Speech for text: I wanna fuck you up dog ass hole mother fuckr bitch I will fuck you up.\n",
            "üìå Key Terms Extracted (Stop words removed): ['wan', 'na', 'fuck', 'dog', 'ass', 'hole', 'mother', 'fuckr', 'bitch', 'fuck', '.']\n",
            "üìå Named Entities Detected: []\n",
            "üìå Semantic Similarity Check: I wanna fuck you up dog ass hole mother fuckr bitch I will fuck you up. vs Assume -> Score: 0.14594250917434692\n",
            "üìå Semantic Similarity Check: I wanna fuck you up dog ass hole mother fuckr bitch I will fuck you up. vs Bass Guitar -> Score: 0.08358348906040192\n",
            "üìå Semantic Similarity Check: I wanna fuck you up dog ass hole mother fuckr bitch I will fuck you up. vs Bare Minerals -> Score: 0.034444235265254974\n",
            "üîπ Final Decision: Violation - Content classified as: ['harsh_language'].\n",
            "\n",
            "Debugging Compliance Model\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<table id=\"itables_8e195973_3045_4ff0_8c82_09127f870753\" class=\"display nowrap\" data-quarto-disable-processing=\"true\" style=\"table-layout:auto;width:auto;margin:auto;caption-side:bottom\">\n",
              "<thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      \n",
              "      <th>text</th>\n",
              "      <th>rule_based_flags</th>\n",
              "      <th>ml_classification</th>\n",
              "      <th>context_safe</th>\n",
              "      <th>semantic_safe</th>\n",
              "      <th>final_decision</th>\n",
              "      <th>explanation</th>\n",
              "    </tr>\n",
              "  </thead><tbody><tr>\n",
              "<td style=\"vertical-align:middle; text-align:left\">\n",
              "<a href=https://mwouts.github.io/itables/><svg class=\"main-svg\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\n",
              "width=\"64\" viewBox=\"0 0 500 400\" style=\"font-family: 'Droid Sans', sans-serif;\">\n",
              "    <g style=\"fill:#d9d7fc\">\n",
              "        <path d=\"M100,400H500V357H100Z\" />\n",
              "        <path d=\"M100,300H400V257H100Z\" />\n",
              "        <path d=\"M0,200H400V157H0Z\" />\n",
              "        <path d=\"M100,100H500V57H100Z\" />\n",
              "        <path d=\"M100,350H500V307H100Z\" />\n",
              "        <path d=\"M100,250H400V207H100Z\" />\n",
              "        <path d=\"M0,150H400V107H0Z\" />\n",
              "        <path d=\"M100,50H500V7H100Z\" />\n",
              "    </g>\n",
              "    <g style=\"fill:#1a1366;stroke:#1a1366;\">\n",
              "   <rect x=\"100\" y=\"7\" width=\"400\" height=\"43\">\n",
              "    <animate\n",
              "      attributeName=\"width\"\n",
              "      values=\"0;400;0\"\n",
              "      dur=\"5s\"\n",
              "      repeatCount=\"indefinite\" />\n",
              "      <animate\n",
              "      attributeName=\"x\"\n",
              "      values=\"100;100;500\"\n",
              "      dur=\"5s\"\n",
              "      repeatCount=\"indefinite\" />\n",
              "  </rect>\n",
              "        <rect x=\"0\" y=\"107\" width=\"400\" height=\"43\">\n",
              "    <animate\n",
              "      attributeName=\"width\"\n",
              "      values=\"0;400;0\"\n",
              "      dur=\"3.5s\"\n",
              "      repeatCount=\"indefinite\" />\n",
              "    <animate\n",
              "      attributeName=\"x\"\n",
              "      values=\"0;0;400\"\n",
              "      dur=\"3.5s\"\n",
              "      repeatCount=\"indefinite\" />\n",
              "  </rect>\n",
              "        <rect x=\"100\" y=\"207\" width=\"300\" height=\"43\">\n",
              "    <animate\n",
              "      attributeName=\"width\"\n",
              "      values=\"0;300;0\"\n",
              "      dur=\"3s\"\n",
              "      repeatCount=\"indefinite\" />\n",
              "    <animate\n",
              "      attributeName=\"x\"\n",
              "      values=\"100;100;400\"\n",
              "      dur=\"3s\"\n",
              "      repeatCount=\"indefinite\" />\n",
              "  </rect>\n",
              "        <rect x=\"100\" y=\"307\" width=\"400\" height=\"43\">\n",
              "    <animate\n",
              "      attributeName=\"width\"\n",
              "      values=\"0;400;0\"\n",
              "      dur=\"4s\"\n",
              "      repeatCount=\"indefinite\" />\n",
              "      <animate\n",
              "      attributeName=\"x\"\n",
              "      values=\"100;100;500\"\n",
              "      dur=\"4s\"\n",
              "      repeatCount=\"indefinite\" />\n",
              "  </rect>\n",
              "        <g style=\"fill:transparent;stroke-width:8; stroke-linejoin:round\" rx=\"5\">\n",
              "            <g transform=\"translate(45 50) rotate(-45)\">\n",
              "                <circle r=\"33\" cx=\"0\" cy=\"0\" />\n",
              "                <rect x=\"-8\" y=\"32\" width=\"16\" height=\"30\" />\n",
              "            </g>\n",
              "\n",
              "            <g transform=\"translate(450 152)\">\n",
              "                <polyline points=\"-15,-20 -35,-20 -35,40 25,40 25,20\" />\n",
              "                <rect x=\"-15\" y=\"-40\" width=\"60\" height=\"60\" />\n",
              "            </g>\n",
              "\n",
              "            <g transform=\"translate(50 352)\">\n",
              "                <polygon points=\"-35,-5 0,-40 35,-5\" />\n",
              "                <polygon points=\"-35,10 0,45 35,10\" />\n",
              "            </g>\n",
              "\n",
              "            <g transform=\"translate(75 250)\">\n",
              "                <polyline points=\"-30,30 -60,0 -30,-30\" />\n",
              "                <polyline points=\"0,30 -30,0 0,-30\" />\n",
              "            </g>\n",
              "\n",
              "            <g transform=\"translate(425 250) rotate(180)\">\n",
              "                <polyline points=\"-30,30 -60,0 -30,-30\" />\n",
              "                <polyline points=\"0,30 -30,0 0,-30\" />\n",
              "            </g>\n",
              "        </g>\n",
              "    </g>\n",
              "</svg>\n",
              "</a>\n",
              "Loading ITables v2.2.5 from the internet...\n",
              "(need <a href=https://mwouts.github.io/itables/troubleshooting.html>help</a>?)</td>\n",
              "</tr></tbody>\n",
              "</table>\n",
              "<link href=\"https://www.unpkg.com/dt_for_itables@2.0.13/dt_bundle.css\" rel=\"stylesheet\">\n",
              "<script type=\"module\">\n",
              "    import {DataTable, jQuery as $} from 'https://www.unpkg.com/dt_for_itables@2.0.13/dt_bundle.js';\n",
              "\n",
              "    document.querySelectorAll(\"#itables_8e195973_3045_4ff0_8c82_09127f870753:not(.dataTable)\").forEach(table => {\n",
              "        if (!(table instanceof HTMLTableElement))\n",
              "            return;\n",
              "\n",
              "        // Define the table data\n",
              "        const data = [[\"In the image, a man and a woman are lying on a bed and engaging in a lustful act. The woman is straddling the man while getting intimate with him. The scene is quite provocative, as both individuals appear to be enjoying their time together. The woman appears to be fully naked, and the man's intentions are clear as they both participate in their erotic pursuits.\", \"[adult_content]\", \"Safe\", false, false, \"Violation\", \"Content classified as: ['adult_content'].\"], [\"Three bottles of Naked beverages are featured in the image.\", \"[]\", \"Safe\", false, false, \"Safe\", \"\"], [\"The image shows three bottles of a green beverage - Naked Half Naked - in three different colors.\", \"[]\", \"Safe\", false, false, \"Safe\", \"\"], [\"I wanna fuck you up dog ass hole mother fuckr bitch I will fuck you up.\", \"[harsh_language]\", \"Hate Speech\", false, false, \"Violation\", \"Content classified as: ['harsh_language'].\"]];\n",
              "\n",
              "        // Define the dt_args\n",
              "        let dt_args = {\"layout\": {\"topStart\": null, \"topEnd\": null, \"bottomStart\": null, \"bottomEnd\": null}, \"order\": [], \"warn_on_selected_rows_not_rendered\": true};\n",
              "        dt_args[\"data\"] = data;\n",
              "\n",
              "        \n",
              "        new DataTable(table, dt_args);\n",
              "    });\n",
              "</script>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ DEBUGGING TRACE COMPLETED\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import json\n",
        "import torch\n",
        "import spacy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from typing import Dict, List\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "# Load pre-trained models\n",
        "nlp = spacy.load(\"en_core_web_sm\")  # Named Entity Recognition (NER)\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # Semantic Similarity Model\n",
        "\n",
        "# Load transformer-based classifier for hate speech detection\n",
        "MODEL_NAME = \"facebook/roberta-hate-speech-dynabench-r4-target\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Define Compliance Categories\n",
        "CATEGORIES = {\n",
        "    \"adult_content\": [\"explicit\", \"nudity\", \"sexual\", \"pornographic\"],\n",
        "    \"harsh_language\": [\"profanity\", \"swearing\", \"abuse\"],\n",
        "    \"child_abuse\": [\"minor exploitation\", \"grooming\"],\n",
        "    \"hate_speech\": [\"racism\", \"sexism\", \"homophobia\"],\n",
        "    \"sensitive_info\": [\"leaked data\", \"PII exposure\"]\n",
        "}\n",
        "\n",
        "# Define Rule-Based Filtering\n",
        "RULES = {\n",
        "    \"adult_content\": r\"\\b(nude|porn|erotic|sex|lust|intimate|strip|orgasm|fetish)\\b\",\n",
        "    \"harsh_language\": r\"\\b(fuck|shit|bitch|bastard|cunt|asshole|dickhead)\\b\",\n",
        "    \"child_abuse\": r\"\\b(minor|underage|child porn|kid exploitation|pedo|grooming)\\b\",\n",
        "    \"hate_speech\": r\"\\b(nazi|white power|kkk|lynch|terrorist|homophobic|slur)\\b\",\n",
        "    \"sensitive_info\": r\"\\b(\\d{3}-\\d{2}-\\d{4}|\\d{16}|\\d{4}-\\d{4}-\\d{4}-\\d{4})\\b\"\n",
        "}\n",
        "\n",
        "# Dynamic Allow-List (initialized with known safe terms)\n",
        "dynamic_allow_list = set([\"Bare Minerals\", \"Assume\", \"Bass Guitar\" ])\n",
        "\n",
        "# Stop Words for Filtering\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "# Debugging Print Statement\n",
        "print(\"\\nüîç DEBUGGING TRACE STARTED\\n\")\n",
        "\n",
        "# Function to Apply Rule-Based Filtering\n",
        "def apply_rule_based_filter(text: str) -> List[str]:\n",
        "    \"\"\"Applies regex-based filtering for quick classification.\"\"\"\n",
        "    flagged_categories = []\n",
        "    for category, pattern in RULES.items():\n",
        "        if re.search(pattern, text, re.IGNORECASE):\n",
        "            flagged_categories.append(category)\n",
        "\n",
        "    print(f\"üìå Rule-Based Filtering: Found {flagged_categories} in text: {text}\")\n",
        "    return flagged_categories\n",
        "\n",
        "# Function to Perform Text Classification using a Transformer Model\n",
        "def classify_with_transformer(text: str) -> str:\n",
        "    \"\"\"Uses a transformer model to classify text into safe or non-compliant categories.\"\"\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    outputs = model(**inputs)\n",
        "    prediction = torch.argmax(outputs.logits, dim=-1).item()\n",
        "\n",
        "    classification = [\"Safe\", \"Hate Speech\"][prediction]\n",
        "    print(f\"üìå Transformer Model Classification: {classification} for text: {text}\")\n",
        "    return classification\n",
        "\n",
        "# Function to Extract Key Terms After Stop Word Removal\n",
        "def extract_key_terms(text: str) -> List[str]:\n",
        "    \"\"\"Removes stop words and extracts key terms for context analysis.\"\"\"\n",
        "    words = word_tokenize(text)\n",
        "    key_terms = [word.lower() for word in words if word.lower() not in stop_words]\n",
        "    print(f\"üìå Key Terms Extracted (Stop words removed): {key_terms}\")\n",
        "    return key_terms\n",
        "\n",
        "# Function to Check Context Using Named Entity Recognition (NER)\n",
        "def check_context(text: str) -> bool:\n",
        "    \"\"\"Uses NER to determine if flagged words appear in a benign context.\"\"\"\n",
        "    doc = nlp(text)\n",
        "    named_entities = [ent.text for ent in doc.ents]\n",
        "\n",
        "    print(f\"üìå Named Entities Detected: {named_entities}\")\n",
        "\n",
        "    # If a named entity is in the dynamic allow-list, it's likely safe\n",
        "    for entity in named_entities:\n",
        "        if entity in dynamic_allow_list:\n",
        "            print(f\"‚úÖ Context Safe: {entity} is in allow-list.\")\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "# Function to Calculate Semantic Similarity\n",
        "def is_contextually_safe(text: str, threshold: float = 0.75) -> bool:\n",
        "    \"\"\"Uses word embeddings to check if the text is similar to known benign terms.\"\"\"\n",
        "    embeddings_text = embedding_model.encode(text, convert_to_tensor=True)\n",
        "\n",
        "    for safe_term in dynamic_allow_list:\n",
        "        embeddings_safe_term = embedding_model.encode(safe_term, convert_to_tensor=True)\n",
        "        similarity_score = util.pytorch_cos_sim(embeddings_text, embeddings_safe_term).item()\n",
        "        print(f\"üìå Semantic Similarity Check: {text} vs {safe_term} -> Score: {similarity_score}\")\n",
        "        if similarity_score > threshold:\n",
        "            print(f\"‚úÖ Semantic Context Safe: Similar to {safe_term}\")\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "# Compliance Check Function\n",
        "def compliance_check(text: str) -> Dict[str, any]:\n",
        "    \"\"\"Runs a multi-layered compliance check on the given text.\"\"\"\n",
        "    print(f\"\\nüîπ Checking Compliance for: {text}\\n\")\n",
        "\n",
        "    result = {\n",
        "        \"text\": text,\n",
        "        \"rule_based_flags\": [],\n",
        "        \"ml_classification\": \"Safe\",\n",
        "        \"context_safe\": False,\n",
        "        \"semantic_safe\": False,\n",
        "        \"final_decision\": \"Safe\",\n",
        "        \"explanation\": \"\"\n",
        "    }\n",
        "\n",
        "    # Apply Rule-Based Filtering\n",
        "    result[\"rule_based_flags\"] = apply_rule_based_filter(text)\n",
        "\n",
        "    # Apply Transformer Model\n",
        "    if len(result[\"rule_based_flags\"]) > 0:\n",
        "        result[\"ml_classification\"] = classify_with_transformer(text)\n",
        "\n",
        "    # Extract Key Terms\n",
        "    key_terms = extract_key_terms(text)\n",
        "\n",
        "    # Check Context with NER\n",
        "    result[\"context_safe\"] = check_context(text)\n",
        "\n",
        "    # Check Semantic Similarity\n",
        "    result[\"semantic_safe\"] = is_contextually_safe(text)\n",
        "\n",
        "    # Final Decision\n",
        "    if result[\"ml_classification\"] != \"Safe\" or len(result[\"rule_based_flags\"]) > 0:\n",
        "        if result[\"context_safe\"] or result[\"semantic_safe\"]:\n",
        "            result[\"final_decision\"] = \"Safe\"\n",
        "            result[\"explanation\"] = \"Content contains flagged words but appears in a non-violating context.\"\n",
        "        else:\n",
        "            result[\"final_decision\"] = \"Violation\"\n",
        "            result[\"explanation\"] = f\"Content classified as: {result['rule_based_flags']}.\"\n",
        "\n",
        "    print(f\"üîπ Final Decision: {result['final_decision']} - {result['explanation']}\\n\")\n",
        "    return result\n",
        "\n",
        "# Test Cases\n",
        "test_cases = [\n",
        "    \"In the image, a man and a woman are lying on a bed and engaging in a lustful act. The woman is straddling the man while getting intimate with him. The scene is quite provocative, as both individuals appear to be enjoying their time together. The woman appears to be fully naked, and the man's intentions are clear as they both participate in their erotic pursuits.\",\n",
        "    \"Three bottles of Naked beverages are featured in the image.\",\n",
        "    \"The image shows three bottles of a green beverage - Naked Half Naked - in three different colors.\",\n",
        "    \"I wanna fuck you up dog ass hole mother fuckr bitch I will fuck you up.\"\n",
        "]\n",
        "\n",
        "# Run Compliance Checks\n",
        "results = [compliance_check(text) for text in test_cases]\n",
        "\n",
        "# Display Results\n",
        "df = pd.DataFrame(results)\n",
        "import ace_tools_open as tools\n",
        "tools.display_dataframe_to_user(name=\"Debugging Compliance Model\", dataframe=df)\n",
        "\n",
        "print(\"\\n‚úÖ DEBUGGING TRACE COMPLETED\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0161fe9b467641b79bc687b6d69c2712": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "02335270363a449b9ece2551b55d20ce": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05a55ea7263c4893970edff85857674e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "06093206fd8d4ef58c1e8f3741caf1ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "06cd2cef14574ba191902916806d1595": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0768b0d7dd864bfabe5f12418dfcfe76": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ffa0fb1b67e3465884ad3f60f18ac13f",
              "IPY_MODEL_ef3cc101cde64d5792edff060d849558",
              "IPY_MODEL_e35686d576ff451ab13c236df9e2ba97"
            ],
            "layout": "IPY_MODEL_384e1eaa7eac487f8c15a79137cebe54"
          }
        },
        "097ae431e07946b98dce00faffb3ec9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "098653a393934263b5b390675488f0bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0995c0c4a711465d8dcbaaa62642301a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0ab20ec7939342dea7ab3a9bd06a9065": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0af793b3f0d14fdc840f54d2c5369bed": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b8b021ad192444f9466713244ebe47f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0c5471ba94334c849cce149045512b85": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0cd8ade3c3f745768ebb06fcb4198d95": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1020ba121d494d98b946de1abf6d2e7d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "105e12c3acc4499a86f059ef0e48b979": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5a15e5f700e74803a6b9a6e033aa2107",
              "IPY_MODEL_76edb62358f943b8936a59d110d1dad4",
              "IPY_MODEL_4461995efbee492ca3b14379119427c4"
            ],
            "layout": "IPY_MODEL_98b9e4aa78964793abbd93b6f386d18e"
          }
        },
        "10977be18b3144e7a98f741c78275399": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "10e6b9f415bb48809b0c5a98dd96927b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "115036c2e2614fcdaebda2b78a988740": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11b265e2d9c54213b10848f90a74a320": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf9748fd88514783b593ddb661b5e746",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7b09b70ecb854904856742c0182d31be",
            "value": "‚Äá6/6‚Äá[00:10&lt;00:00,‚Äá‚Äá1.56s/it]"
          }
        },
        "12428d31f9484a5ba2e8887a07434cea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fbc821514a18484591c601ed18678552",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_36226dae30b545d8b9c7b6d38f8d85f2",
            "value": "‚Äá2/2‚Äá[00:03&lt;00:00,‚Äá‚Äá1.85s/it]"
          }
        },
        "124acec5653c40e89b4687bc4d6caa5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd479b74ae6e4bcd8cdd2e3cd83be7f9",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b02a1abae8cf44b6ac907d2bed3d468c",
            "value": "‚Äá6/6‚Äá[00:10&lt;00:00,‚Äá‚Äá1.64s/it]"
          }
        },
        "12bab7724d5a49868571c1572e0fa139": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "133b8af5f5944565a5db3c68c302d806": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3afd4668b204263a8ecbcef303206a3",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b46afe5aeab64e7b898766268f1bcaa9",
            "value": 6
          }
        },
        "13434db54bd04e208bd9ef033446d1e5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "137b193e1be646a0a26491dc0984e38b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "154e97b19e7c4cc2971157c239ebce20": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "164dde685a3848999d25281206d1f10b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "171ada5217eb4514b4f7b6342a06ebae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "171e14b083664289b68d58ebcadad314": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b6d5be66dc24be69195579a35c1ee61",
            "max": 77152,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f84166ee9d6b476a830081a41baf7e57",
            "value": 77152
          }
        },
        "171e1ab56acb441cbde5ec643faf65a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "174f64acaeb74877b85527ab9ac60261": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18460aec28144a05a70456ba583218c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "19078e6a65a2444a9b7d1f6bd3e65275": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "19d773f9d9064e839988f5d0cc914437": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1abef045e39b446eae3d8d680b8b992d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1b053694608b467882f797ce1332da34": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_48a12a36c77b47b3a962fb7e3f0eede5",
              "IPY_MODEL_db4afb94aa304bf197ceb9518fc175e5",
              "IPY_MODEL_124acec5653c40e89b4687bc4d6caa5f"
            ],
            "layout": "IPY_MODEL_af421f24a5a84bb082a3f2e2d02d91b6"
          }
        },
        "1bb4e74b6c1d4184b4a5d7722842e43d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f0ddffd4b85454484649c10122d0f87",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_29178358a54e4b8fbe0638066e464be9",
            "value": "model-00001-of-00006.safetensors:‚Äá100%"
          }
        },
        "1be5ea723a3f415eb6f01976a27203a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1c6a29bd1d0e4217b3f6dcaaca9fa300": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d360b6ed5394177bfd89c7805e1a9d3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d7b384d43784b67b42b2f9bb91b0180": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1de32592a96d439093c5a6bd8ff5b777": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e468072ae1a4a5c85fa4f0f5d31d792": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f50f4c52e5543b7bb4b194e01abee9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b9643096e24497190978176855df1e8",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_8a281f9cc30646839f7c5d9a9fd834e9",
            "value": "‚Äá500k/500k‚Äá[00:00&lt;00:00,‚Äá9.72MB/s]"
          }
        },
        "1f999bfdc60c489a9d38736ee0ddd2ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2046809f60e24821afcc8cee6d1db569": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2064462ac5334ae28dc3c438072aee37": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20cf22134a8f482a9e72fc5f4c02826b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "224695c1308b421dad6890cf4126daae": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "228829dfb52d43edbcaf2f5332b58f42": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "22f33450b0ba4f5c8a0e90594dfcb3bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1465600f8934ed9878971e613652c9e",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_05a55ea7263c4893970edff85857674e",
            "value": 2
          }
        },
        "252df2764bbc495a966653aa1adb77e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c44c96c00c774e3f83ddee2e8a7a8122",
              "IPY_MODEL_6547a69158cb499ca2eb6fdb55fef93d",
              "IPY_MODEL_3dc11154b8974cc5a820166855194b6c"
            ],
            "layout": "IPY_MODEL_4900901ca8754f7bb5e6c9ee759eaa0e"
          }
        },
        "263cded35c0f40889b72fe2132e464b0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "266e6b9d6ffd429ba4562d1d6b6fb36b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f8328d54e6b4556b8187220961ae740",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_91fd22599ef6497584d144f032eb77b9",
            "value": "‚Äá4.93G/4.93G‚Äá[00:31&lt;00:00,‚Äá64.3MB/s]"
          }
        },
        "27dc915d53764745ae8550691a231b9c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28359b6502c842b88a2c971195f4987e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28c382b065bd4438aa5c177f4fd63ab3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29178358a54e4b8fbe0638066e464be9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "298dbd52ae6441749cc576189dc57dcc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2bab671dbdb24a8e81c7f15a9e603f11": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e83a991a043478ca5879ec3a6ff1fe3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2f02d8fabe5845119148e48e3af3cdaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2f19f4f8835a4b3b98c546875d1b6d39": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac21d37ec70548e385d1b6466e532dee",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7237fb6d1ff049fca43045a0a6b3c496",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá‚Äá50%"
          }
        },
        "2f521dad8eb54b0aa264437462a1dfd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40e8147696954198b1f2ca20d9910cb1",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1c6a29bd1d0e4217b3f6dcaaca9fa300",
            "value": "Downloading‚Äáshards:‚Äá100%"
          }
        },
        "2f731602df604a4e8327fac2c395af99": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1bb4e74b6c1d4184b4a5d7722842e43d",
              "IPY_MODEL_9a88ce7f2ac44bffa6a938b8ba60c863",
              "IPY_MODEL_59458c7149844cada0be93f978269a88"
            ],
            "layout": "IPY_MODEL_5e2d96d9393a4ba08af4f7118b2fb600"
          }
        },
        "3020ee49c50f47109994c58631a3ee44": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30a441bee81f4073936fe007eac09590": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3228561877b04c45ab7338abd0d02f13": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "32a95e1db2464dbda32116ad5879e752": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "351a8622db5a40be9b01e1676f133405": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36226dae30b545d8b9c7b6d38f8d85f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "36d592a3757947b7bf739d13a2408d14": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36f6581c14914c5faf232ecbbf8c71a2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "372654e39cf54716844fb0d8c8fc9615": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "374236e0ef7d4e9888ed3e60972fded7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37b1810d57464a9c8cda4c19c1d7fe22": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf32d9862cda431dbb5c7396a47daf3b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_10977be18b3144e7a98f741c78275399",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "384e1eaa7eac487f8c15a79137cebe54": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3856a259d0a541b8bf131a81a455bc6a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38a3a87ec35747c4b724940d237b7e64": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "38fa4e98368948d793adb8f414c128de": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1c58f776af54a3d898130e9cd822747",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_618151761499454490b916a0548b2efd",
            "value": "tokenizer.model:‚Äá100%"
          }
        },
        "3960a7c4ad4d4af0b818ef1de7dc24ff": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39b88f4102ba4222a3afd572629746e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "39d8085120634454b8ac243c557df520": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a25e247fc9b4c5f9ddac407e265c4be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d02da867109d48b998b92fdb4a1f9c99",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_64ff73504e714597b5d115cea8ae43e8",
            "value": "model-00006-of-00006.safetensors:‚Äá100%"
          }
        },
        "3a9d3bbb2eba47949fcbf3b112eb2c2a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b8cc33626434244971000388c7ad476": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b9643096e24497190978176855df1e8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b9d5757879442829bdbf8c0c56f1f7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd2fa6d29b2940e2b370520b24821f6f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_18460aec28144a05a70456ba583218c1",
            "value": "‚Äá3.62M/3.62M‚Äá[00:00&lt;00:00,‚Äá29.9MB/s]"
          }
        },
        "3ce27b9d396b44108550c798c1fe443e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3f34d1e53ca24de0b3588220a1c494cb",
              "IPY_MODEL_7775eea944c4417684d63e307b1620ab",
              "IPY_MODEL_6b5ddce7563f41f0ae45c24b6c23d4e5"
            ],
            "layout": "IPY_MODEL_6a0ac49b9f2e41a6ab7cdcc479df675b"
          }
        },
        "3dc11154b8974cc5a820166855194b6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0af793b3f0d14fdc840f54d2c5369bed",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c10f81270c4c4aa094e788dbae75a8d5",
            "value": "‚Äá2/2‚Äá[00:03&lt;00:00,‚Äá‚Äá1.75s/it]"
          }
        },
        "3de4e73be3cd40449287d401b7ae86ac": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f34d1e53ca24de0b3588220a1c494cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6966f921e1a43a7b304368836e6a8b8",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_30a441bee81f4073936fe007eac09590",
            "value": "tokenizer_config.json:‚Äá100%"
          }
        },
        "40e8147696954198b1f2ca20d9910cb1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "413edeff0876446d9d0e6de4c5d4286f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "424ac53f6d194a4fb6eecfd4ae5e465e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d8369888ab44410889405511abc8dfc5",
              "IPY_MODEL_e8ee0c2c8ee2444a993b933f81f70b10",
              "IPY_MODEL_c11aee1f2e7b42ae98dedc47e84555bd"
            ],
            "layout": "IPY_MODEL_477ecf9b4f0e46ba9b7023c3457b2fa1"
          }
        },
        "437298397b7548af9217f1542a133bda": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b119b8e7f0db419a8e575352be2ce723",
              "IPY_MODEL_bfaf3fda70fb429f9d370def1373c1e0",
              "IPY_MODEL_ea0869a5b58a4bd4b9a7209ca008b369"
            ],
            "layout": "IPY_MODEL_d0541a4917994a8383a36eed3b1530c2"
          }
        },
        "4461995efbee492ca3b14379119427c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b91d6730f3924be6a4b9bfe34a099457",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_be5c6055ff8c4b06abe7c301701a2d49",
            "value": "‚Äá141/141‚Äá[00:00&lt;00:00,‚Äá18.8kB/s]"
          }
        },
        "44b56caf7a844daf97552f4bff8183ac": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4518b7a3d6c547dfa528a185699fa7f7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4538d6e6940f4ff8acbff03286bf503c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2bf1bb4b0054e00b384f04fbdbb86b3",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_bd9c45911bec4055849aef7eecc4d6cf",
            "value": "‚Äá6/6‚Äá[02:35&lt;00:00,‚Äá23.29s/it]"
          }
        },
        "477bd4ffe2384ca78900ac18a7e07c66": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "477ecf9b4f0e46ba9b7023c3457b2fa1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48a12a36c77b47b3a962fb7e3f0eede5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94a010bf063146b3b11e1f03daab5d70",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f02e47abf8514c6781562c6e1c335525",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "4900901ca8754f7bb5e6c9ee759eaa0e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49a7afdb02e848de925196923ee52b1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_92ba8644980940b89ad8894fdf2488ca",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f2d0797d50c64308bf2abd67b4bdf2a1",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "4a6e400c58244f6c8021742bb8d00543": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4db2b32991be4bdd81f9d321d94dd168": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44b56caf7a844daf97552f4bff8183ac",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f59d60f824c04855860b94fae3f63986",
            "value": "‚Äá6/6‚Äá[00:03&lt;00:00,‚Äá‚Äá2.29it/s]"
          }
        },
        "4e3406e1984744a19a14896042e1240a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e8b59feb2ac4be2884f409f4c57cff6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3856a259d0a541b8bf131a81a455bc6a",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d5a00820206348cba3a2dde939ed7823",
            "value": 6
          }
        },
        "4ebfc388ebce4fd4af62eb43d5b29853": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f0ddffd4b85454484649c10122d0f87": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ffb79decbb1495e8b3170997535371f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_477bd4ffe2384ca78900ac18a7e07c66",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_413edeff0876446d9d0e6de4c5d4286f",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "50d9a1a00a9c465c945c3c1c928f9771": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51b106e8bc18444fbdb84324cfad8c25": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5db57d036c8a483b8f221da813dc1a92",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7039731f6b8d4caa85367d24f0ae69c4",
            "value": 6
          }
        },
        "52c7323d1c6f44e7972805133e8ca134": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5436c8e4b1a2486dbdc58ff3a7ebeb13": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "548b13ed359745cfb6b9247189531c2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_68f6409ae0784c05850737623a616688",
              "IPY_MODEL_781e41864d484d999cd5e4ab5a54ac58",
              "IPY_MODEL_e2ce474b812f460f9992bc1956e9c150"
            ],
            "layout": "IPY_MODEL_36f6581c14914c5faf232ecbbf8c71a2"
          }
        },
        "5602428751f34d5ea08d167c05a7b360": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56c2a436293a4a8cb8b62789f4c7cf09": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7255b2fd13984d7b81bad09a05daa1ec",
              "IPY_MODEL_ab59bc7059af4075b94f2fac13ff33bb",
              "IPY_MODEL_b36670b48c51417794c21ed195dd1c3b"
            ],
            "layout": "IPY_MODEL_115036c2e2614fcdaebda2b78a988740"
          }
        },
        "57867682b2de4d3c93ed009fd6bedb20": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "58b610bec47743a5b495b87f30a7bd55": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59458c7149844cada0be93f978269a88": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58b610bec47743a5b495b87f30a7bd55",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_0cd8ade3c3f745768ebb06fcb4198d95",
            "value": "‚Äá4.96G/4.96G‚Äá[00:26&lt;00:00,‚Äá237MB/s]"
          }
        },
        "5a15e5f700e74803a6b9a6e033aa2107": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_298dbd52ae6441749cc576189dc57dcc",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_12bab7724d5a49868571c1572e0fa139",
            "value": "generation_config.json:‚Äá100%"
          }
        },
        "5a7f767393a9427d93486dd8356e8106": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ae22023fe3640579e37146171d9a36d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06cd2cef14574ba191902916806d1595",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_39b88f4102ba4222a3afd572629746e0",
            "value": "‚Äá701/701‚Äá[00:00&lt;00:00,‚Äá89.9kB/s]"
          }
        },
        "5b9e2a7d143a4d39980327fc9269b81d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28c382b065bd4438aa5c177f4fd63ab3",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f02bd61eb9614995a59dbf58f23221b1",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "5c5af41a75e34eb28cbf82f3f846c5f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36d592a3757947b7bf739d13a2408d14",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ebcae4853fd6479f99a1fef118689872",
            "value": 6
          }
        },
        "5d05f370e24d40a8b0c76275ce349ff7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5db57d036c8a483b8f221da813dc1a92": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e2d96d9393a4ba08af4f7118b2fb600": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ee1eda0eff54491a024a5fcea46c87b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f4f5122ace5499f81eae788ba274d5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4518b7a3d6c547dfa528a185699fa7f7",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1abef045e39b446eae3d8d680b8b992d",
            "value": "‚Äá173/173‚Äá[00:00&lt;00:00,‚Äá20.6kB/s]"
          }
        },
        "5f8328d54e6b4556b8187220961ae740": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "607e1cccc37844b98af4de125c2b14a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_38fa4e98368948d793adb8f414c128de",
              "IPY_MODEL_747f5b540cab4706b7e1ad22fc78fafd",
              "IPY_MODEL_1f50f4c52e5543b7bb4b194e01abee9a"
            ],
            "layout": "IPY_MODEL_02335270363a449b9ece2551b55d20ce"
          }
        },
        "60d6da517e0b42938ba953e630d7362c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27dc915d53764745ae8550691a231b9c",
            "max": 701,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8f4ee815d68e407f9c68e5b76fc6c5c5",
            "value": 701
          }
        },
        "618151761499454490b916a0548b2efd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "623a38668f574f83953fde06cf584508": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6321dbf9e1f241289574079c489975f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a67f21f5806d4b69925acb8f7e70a95a",
              "IPY_MODEL_60d6da517e0b42938ba953e630d7362c",
              "IPY_MODEL_5ae22023fe3640579e37146171d9a36d"
            ],
            "layout": "IPY_MODEL_5602428751f34d5ea08d167c05a7b360"
          }
        },
        "64ff73504e714597b5d115cea8ae43e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6547a69158cb499ca2eb6fdb55fef93d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70154e80a1b44bec8ffdc5f461ca2692",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_171e1ab56acb441cbde5ec643faf65a5",
            "value": 2
          }
        },
        "686b427a930042819fa618ad27bcfdba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0d5b29cdc4f411db2401e5860cc6a56",
            "max": 4933723208,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b90279bc775445e8b1f69e94eeb3a710",
            "value": 4933723208
          }
        },
        "689ffbb060da46369dac1f7445738f04": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "68b05e1dfd37431cb02af1681d16e4fd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68f6409ae0784c05850737623a616688": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a14f1a7126a427cbc33659f32ce0a59",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7cf6a50961cf477bbd17b6922278dc4d",
            "value": "config.json:‚Äá100%"
          }
        },
        "691c226205e34a0397f0ae51ed5f8358": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "698ca72845e247269a3e28f2d37fcdb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8e9b56e3dfeb4b78a78e9a2d47b4fb05",
              "IPY_MODEL_b9b752b3e1284cdea140a891ca9adbe7",
              "IPY_MODEL_880572259c5844b697a5018072d17fcc"
            ],
            "layout": "IPY_MODEL_9b5a9169572e4a92beb20e2042f1f143"
          }
        },
        "6a0ac49b9f2e41a6ab7cdcc479df675b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b5ddce7563f41f0ae45c24b6c23d4e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68b05e1dfd37431cb02af1681d16e4fd",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_0ab20ec7939342dea7ab3a9bd06a9065",
            "value": "‚Äá1.45k/1.45k‚Äá[00:00&lt;00:00,‚Äá187kB/s]"
          }
        },
        "6b6d5be66dc24be69195579a35c1ee61": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d64747b6cf94a6781dde91b87f593a4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6dc913be8ca24d0c917c480bf9b59d28": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_37b1810d57464a9c8cda4c19c1d7fe22",
              "IPY_MODEL_4e8b59feb2ac4be2884f409f4c57cff6",
              "IPY_MODEL_11b265e2d9c54213b10848f90a74a320"
            ],
            "layout": "IPY_MODEL_dd1355db04cd4c20b511a87e118afc6c"
          }
        },
        "70154e80a1b44bec8ffdc5f461ca2692": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7039731f6b8d4caa85367d24f0ae69c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "703a9e14e32644278e1ed3b85728facb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50d9a1a00a9c465c945c3c1c928f9771",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_3228561877b04c45ab7338abd0d02f13",
            "value": "‚Äá4.88G/4.88G‚Äá[00:32&lt;00:00,‚Äá84.8MB/s]"
          }
        },
        "70ba70053bb54a54912dfc38c29d2ee6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "71b11af316b34a5eb79e6f5efc15dd05": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5b9e2a7d143a4d39980327fc9269b81d",
              "IPY_MODEL_5c5af41a75e34eb28cbf82f3f846c5f2",
              "IPY_MODEL_4db2b32991be4bdd81f9d321d94dd168"
            ],
            "layout": "IPY_MODEL_d648b0e9bbd041e4bacd1c108e4f4885"
          }
        },
        "7237fb6d1ff049fca43045a0a6b3c496": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7255b2fd13984d7b81bad09a05daa1ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f12b26a1a464d9a8e3151627d0921d8",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_d732d195663347818588b05febebc3a3",
            "value": "model-00002-of-00006.safetensors:‚Äá100%"
          }
        },
        "7381515a531d4cf78f5c5ecc80a2ca94": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1020ba121d494d98b946de1abf6d2e7d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_91faf571a67447679f56043e6a353ee6",
            "value": "‚Äá4.93G/4.93G‚Äá[00:27&lt;00:00,‚Äá111MB/s]"
          }
        },
        "73e2870e3515492682471084f0696b58": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_137b193e1be646a0a26491dc0984e38b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_691c226205e34a0397f0ae51ed5f8358",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "747f5b540cab4706b7e1ad22fc78fafd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39d8085120634454b8ac243c557df520",
            "max": 499723,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_19078e6a65a2444a9b7d1f6bd3e65275",
            "value": 499723
          }
        },
        "74c89dbfa8254809be3ae0119ee4dad7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76edb62358f943b8936a59d110d1dad4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b8cc33626434244971000388c7ad476",
            "max": 141,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d3819e70fcf2496b8357fdb08da0ebf1",
            "value": 141
          }
        },
        "7775eea944c4417684d63e307b1620ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e11ab1eab314fe0bc122f78bf68ff6d",
            "max": 1451,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b9744ff5ef994155ad950bff292e6706",
            "value": 1451
          }
        },
        "781e41864d484d999cd5e4ab5a54ac58": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e3406e1984744a19a14896042e1240a",
            "max": 1103,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0b8b021ad192444f9466713244ebe47f",
            "value": 1103
          }
        },
        "792af95f97394065a2131f94058a7532": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7a14f1a7126a427cbc33659f32ce0a59": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a4fac2a7da14b73b7c6c01ab5c885e0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ace79cfc887468eae93f5892f8c1998": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b09b70ecb854904856742c0182d31be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7cf6a50961cf477bbd17b6922278dc4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7f6e1b4b30424f9dbdab6d1e3693fdb3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "817af4ee28114961b8deec547b9d224a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8cc8105b95544171ac67f752a42dcaaa",
              "IPY_MODEL_133b8af5f5944565a5db3c68c302d806",
              "IPY_MODEL_d2c2b1cfdd664e299e631cd29ade0104"
            ],
            "layout": "IPY_MODEL_c7c2d2b92ddc4d218989ae3d78877fd2"
          }
        },
        "81fe15871bce4f23aadbeed7a2d52c71": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8279bbe0adab413b95fe5541259afdc3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8284ef46f1f24a16bdc9c54af0d84900": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82b545728c944f519ad1e12134426771": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "82b7050c4fe349e6a6fc49a0608d4ec8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c3507c5985454c36aa4f83e2a661fb16",
              "IPY_MODEL_d54ea90f246f4ef5a54c7e28a968e274",
              "IPY_MODEL_703a9e14e32644278e1ed3b85728facb"
            ],
            "layout": "IPY_MODEL_3020ee49c50f47109994c58631a3ee44"
          }
        },
        "83fc886a6af7492e8cd5b71c2c58f4c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8587a2643f3e40049f691f23482239b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "874555ed1d3c478ca521608751492af8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8764d39c88494cd89f20d207f79701fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cfb58c0dd80048f3ae641962af17a56c",
              "IPY_MODEL_b66f5ce342ae4c0b8a6a7deb6c2ba912",
              "IPY_MODEL_266e6b9d6ffd429ba4562d1d6b6fb36b"
            ],
            "layout": "IPY_MODEL_87bd17d77a1d4caeb97b70234dbd7248"
          }
        },
        "87bd17d77a1d4caeb97b70234dbd7248": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "880572259c5844b697a5018072d17fcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f6e1b4b30424f9dbdab6d1e3693fdb3",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_098653a393934263b5b390675488f0bf",
            "value": "‚Äá505/505‚Äá[00:00&lt;00:00,‚Äá71.2kB/s]"
          }
        },
        "89e902f1912840389a1aa259c1c84252": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8a281f9cc30646839f7c5d9a9fd834e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8abbedc4dc0e4deab08b414ba9210266": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b7bc7aec1364625814ef5946683ffce": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b956e7a497644d294d246530db33a91": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_154e97b19e7c4cc2971157c239ebce20",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_d4db570bdf614c2aa85786674a97db37",
            "value": "‚Äá6/6‚Äá[00:03&lt;00:00,‚Äá‚Äá2.27it/s]"
          }
        },
        "8c00d5de93bc40d7811bce3ad85df4de": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c86469fb635495bb3d157dfb67616e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_374236e0ef7d4e9888ed3e60972fded7",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c30f654164c0437f993d7ea2bbaa77fa",
            "value": 3
          }
        },
        "8cc8105b95544171ac67f752a42dcaaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e312eb0a596749fbb9768a58c1d834b7",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1be5ea723a3f415eb6f01976a27203a7",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "8e9b56e3dfeb4b78a78e9a2d47b4fb05": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_943a02fa93e34fefab30f859dcef7e7d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_689ffbb060da46369dac1f7445738f04",
            "value": "preprocessor_config.json:‚Äá100%"
          }
        },
        "8f4ee815d68e407f9c68e5b76fc6c5c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "91faf571a67447679f56043e6a353ee6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "91fd22599ef6497584d144f032eb77b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "92ba8644980940b89ad8894fdf2488ca": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "943a02fa93e34fefab30f859dcef7e7d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94a010bf063146b3b11e1f03daab5d70": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9553e75ebc83484ebb8dd651864c19c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_164dde685a3848999d25281206d1f10b",
            "max": 2021860512,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2e83a991a043478ca5879ec3a6ff1fe3",
            "value": 2021860512
          }
        },
        "95fad93ab0a0484a9c223664f2deb1a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3de4e73be3cd40449287d401b7ae86ac",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_5d05f370e24d40a8b0c76275ce349ff7",
            "value": "‚Äá41.0/41.0‚Äá[00:00&lt;00:00,‚Äá5.07kB/s]"
          }
        },
        "97c2a58c1e0744e19b9b15d080f1d4e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f02cb2da4d07442babb25ac66930c200",
              "IPY_MODEL_cc86e318f3bb40a8a34164dbaf88aeef",
              "IPY_MODEL_95fad93ab0a0484a9c223664f2deb1a8"
            ],
            "layout": "IPY_MODEL_4ebfc388ebce4fd4af62eb43d5b29853"
          }
        },
        "98b9e4aa78964793abbd93b6f386d18e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a88ce7f2ac44bffa6a938b8ba60c863": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32a95e1db2464dbda32116ad5879e752",
            "max": 4962087016,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_874555ed1d3c478ca521608751492af8",
            "value": 4962087016
          }
        },
        "9b5a9169572e4a92beb20e2042f1f143": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e11ab1eab314fe0bc122f78bf68ff6d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f12b26a1a464d9a8e3151627d0921d8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0f822d36fcb42018165de4710bbf672": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2f521dad8eb54b0aa264437462a1dfd7",
              "IPY_MODEL_a2a391aafcd9468ba69bf9d49600dd4e",
              "IPY_MODEL_4538d6e6940f4ff8acbff03286bf503c"
            ],
            "layout": "IPY_MODEL_be0118525d1544b6a4a135d0ce101369"
          }
        },
        "a1465600f8934ed9878971e613652c9e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1c58f776af54a3d898130e9cd822747": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2a391aafcd9468ba69bf9d49600dd4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c5471ba94334c849cce149045512b85",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4a6e400c58244f6c8021742bb8d00543",
            "value": 6
          }
        },
        "a46baa939e8d42bbbfa3db1771f90bc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a52e2c917b584bd6a128173f97e6f298": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a98cdad10af043788497e0fb560366b8",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_372654e39cf54716844fb0d8c8fc9615",
            "value": "model.safetensors.index.json:‚Äá100%"
          }
        },
        "a53089b534e244c2aa2af895557eea12": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dd40dd4193d249698c0dd69276cbee4c",
              "IPY_MODEL_deb23f0814e146388128ecea0b5beda2",
              "IPY_MODEL_3b9d5757879442829bdbf8c0c56f1f7b"
            ],
            "layout": "IPY_MODEL_28359b6502c842b88a2c971195f4987e"
          }
        },
        "a67f21f5806d4b69925acb8f7e70a95a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6618ebb686b43ecaf8e62ec27ca928e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_0995c0c4a711465d8dcbaaa62642301a",
            "value": "chat_template.json:‚Äá100%"
          }
        },
        "a97d86e8dc3c47d2b53a046e26abdfc6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a98cdad10af043788497e0fb560366b8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab17979989a441959bb43a7ca0b048df": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab29260df001471d881a4e2752f5d0de": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ab59bc7059af4075b94f2fac13ff33bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a97d86e8dc3c47d2b53a046e26abdfc6",
            "max": 4970423200,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_228829dfb52d43edbcaf2f5332b58f42",
            "value": 4970423200
          }
        },
        "ac21d37ec70548e385d1b6466e532dee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad1bd020b7fe44a1a64c48dd8109eee4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c00d5de93bc40d7811bce3ad85df4de",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_792af95f97394065a2131f94058a7532",
            "value": "‚Äá6/6‚Äá[00:04&lt;00:00,‚Äá‚Äá1.78it/s]"
          }
        },
        "ad4d9413e1d746c69722b1ea42d709d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c237d95d55864f84b574d8a2cb59268d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_2046809f60e24821afcc8cee6d1db569",
            "value": "‚Äá2.02G/2.02G‚Äá[00:12&lt;00:00,‚Äá84.3MB/s]"
          }
        },
        "af421f24a5a84bb082a3f2e2d02d91b6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b02a1abae8cf44b6ac907d2bed3d468c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b0f8aef051fe490c86c1ffc920bbf4a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3a25e247fc9b4c5f9ddac407e265c4be",
              "IPY_MODEL_9553e75ebc83484ebb8dd651864c19c1",
              "IPY_MODEL_ad4d9413e1d746c69722b1ea42d709d8"
            ],
            "layout": "IPY_MODEL_ef44f44033324c1ea6699d90a9f4df9d"
          }
        },
        "b119b8e7f0db419a8e575352be2ce723": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5434235bd3f4f62934c7c5396aba4d6",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ab17979989a441959bb43a7ca0b048df",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "b36670b48c51417794c21ed195dd1c3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a7f767393a9427d93486dd8356e8106",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_a46baa939e8d42bbbfa3db1771f90bc3",
            "value": "‚Äá4.97G/4.97G‚Äá[00:25&lt;00:00,‚Äá225MB/s]"
          }
        },
        "b3afd4668b204263a8ecbcef303206a3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b46afe5aeab64e7b898766268f1bcaa9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b4de01d0890845f9992dc90ebd3b410d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b66f5ce342ae4c0b8a6a7deb6c2ba912": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_174f64acaeb74877b85527ab9ac60261",
            "max": 4933723208,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_82b545728c944f519ad1e12134426771",
            "value": 4933723208
          }
        },
        "b6f402ff54d640ccbb9672e66d9157c9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b90279bc775445e8b1f69e94eeb3a710": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b91d6730f3924be6a4b9bfe34a099457": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9744ff5ef994155ad950bff292e6706": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b9adcc96c2f04a1cbeccb079320e2abf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a52e2c917b584bd6a128173f97e6f298",
              "IPY_MODEL_171e14b083664289b68d58ebcadad314",
              "IPY_MODEL_ca7bca7de2ee4952bef8cc4b03dfa9c7"
            ],
            "layout": "IPY_MODEL_10e6b9f415bb48809b0c5a98dd96927b"
          }
        },
        "b9b752b3e1284cdea140a891ca9adbe7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_20cf22134a8f482a9e72fc5f4c02826b",
            "max": 505,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ed997990922a4e298203825ed094242b",
            "value": 505
          }
        },
        "bc3b8a41a31448a5b8cada089bbbf428": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4ffb79decbb1495e8b3170997535371f",
              "IPY_MODEL_ebe48e28147e4068813f5e88c64eeb98",
              "IPY_MODEL_8b956e7a497644d294d246530db33a91"
            ],
            "layout": "IPY_MODEL_f359454a80be4630a31f22156046037e"
          }
        },
        "bd0db7d34f334506833116528043d6a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c665d8ec1c4d490488696f2348e6faf8",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_cf91abe9cb2b482e86a2c494d08eb82d",
            "value": "‚Äá3/6‚Äá[01:02&lt;01:03,‚Äá21.24s/it]"
          }
        },
        "bd9c45911bec4055849aef7eecc4d6cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "be0118525d1544b6a4a135d0ce101369": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be5c6055ff8c4b06abe7c301701a2d49": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf32d9862cda431dbb5c7396a47daf3b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf9748fd88514783b593ddb661b5e746": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bfaf3fda70fb429f9d370def1373c1e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a4fac2a7da14b73b7c6c01ab5c885e0",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cda8508eaf994d94aefbe2fe5f211756",
            "value": 6
          }
        },
        "bfe5351acf1647b5b0e93656e318dae1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c10f81270c4c4aa094e788dbae75a8d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c11aee1f2e7b42ae98dedc47e84555bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2064462ac5334ae28dc3c438072aee37",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_19d773f9d9064e839988f5d0cc914437",
            "value": "‚Äá552/552‚Äá[00:00&lt;00:00,‚Äá72.7kB/s]"
          }
        },
        "c19bd2431f9a451fb60e9d37657928c6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c237d95d55864f84b574d8a2cb59268d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c30f654164c0437f993d7ea2bbaa77fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c3507c5985454c36aa4f83e2a661fb16": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_224695c1308b421dad6890cf4126daae",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_06093206fd8d4ef58c1e8f3741caf1ee",
            "value": "model-00003-of-00006.safetensors:‚Äá100%"
          }
        },
        "c44c96c00c774e3f83ddee2e8a7a8122": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3960a7c4ad4d4af0b818ef1de7dc24ff",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_83fc886a6af7492e8cd5b71c2c58f4c1",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "c5434235bd3f4f62934c7c5396aba4d6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c665d8ec1c4d490488696f2348e6faf8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7c2d2b92ddc4d218989ae3d78877fd2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca5024de9d8f4543a98437f4b46dde4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_49a7afdb02e848de925196923ee52b1d",
              "IPY_MODEL_22f33450b0ba4f5c8a0e90594dfcb3bc",
              "IPY_MODEL_12428d31f9484a5ba2e8887a07434cea"
            ],
            "layout": "IPY_MODEL_1e468072ae1a4a5c85fa4f0f5d31d792"
          }
        },
        "ca7bca7de2ee4952bef8cc4b03dfa9c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1de32592a96d439093c5a6bd8ff5b777",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7ace79cfc887468eae93f5892f8c1998",
            "value": "‚Äá77.2k/77.2k‚Äá[00:00&lt;00:00,‚Äá9.24MB/s]"
          }
        },
        "cbb0310da632478a9dad8cdeafe909fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2f19f4f8835a4b3b98c546875d1b6d39",
              "IPY_MODEL_8c86469fb635495bb3d157dfb67616e6",
              "IPY_MODEL_bd0db7d34f334506833116528043d6a5"
            ],
            "layout": "IPY_MODEL_1d360b6ed5394177bfd89c7805e1a9d3"
          }
        },
        "cbe15c57ea0446d38bbc33fae10264ca": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc1cc4bca9fe4646a420465acc20a6cd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc268da4acad43f6b4a02535be513add": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c19bd2431f9a451fb60e9d37657928c6",
            "max": 173,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_38a3a87ec35747c4b724940d237b7e64",
            "value": 173
          }
        },
        "cc86e318f3bb40a8a34164dbaf88aeef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff5c7ac00b734cdaa0a44cbe1acff29e",
            "max": 41,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ab29260df001471d881a4e2752f5d0de",
            "value": 41
          }
        },
        "ccd1ea6fc6d64a7fb4496369138aa6a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fbce96681f0d4e2f9076ccf85a9d97c0",
              "IPY_MODEL_686b427a930042819fa618ad27bcfdba",
              "IPY_MODEL_7381515a531d4cf78f5c5ecc80a2ca94"
            ],
            "layout": "IPY_MODEL_cc1cc4bca9fe4646a420465acc20a6cd"
          }
        },
        "cda8508eaf994d94aefbe2fe5f211756": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cf91abe9cb2b482e86a2c494d08eb82d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cfb58c0dd80048f3ae641962af17a56c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13434db54bd04e208bd9ef033446d1e5",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_bfe5351acf1647b5b0e93656e318dae1",
            "value": "model-00004-of-00006.safetensors:‚Äá100%"
          }
        },
        "d02da867109d48b998b92fdb4a1f9c99": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0541a4917994a8383a36eed3b1530c2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0d5b29cdc4f411db2401e5860cc6a56": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2c2b1cfdd664e299e631cd29ade0104": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81fe15871bce4f23aadbeed7a2d52c71",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_2f02d8fabe5845119148e48e3af3cdaa",
            "value": "‚Äá6/6‚Äá[00:10&lt;00:00,‚Äá‚Äá1.64s/it]"
          }
        },
        "d2ebf8f6dcc245b7b8a5e264ce17903c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d3819e70fcf2496b8357fdb08da0ebf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d4db570bdf614c2aa85786674a97db37": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d51451fbe0914d419149dfd01298721c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ff3ce6ad11104d3b9461b4cfac7f86d9",
              "IPY_MODEL_cc268da4acad43f6b4a02535be513add",
              "IPY_MODEL_5f4f5122ace5499f81eae788ba274d5d"
            ],
            "layout": "IPY_MODEL_b6f402ff54d640ccbb9672e66d9157c9"
          }
        },
        "d54ea90f246f4ef5a54c7e28a968e274": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8abbedc4dc0e4deab08b414ba9210266",
            "max": 4881273536,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b4de01d0890845f9992dc90ebd3b410d",
            "value": 4881273536
          }
        },
        "d5a00820206348cba3a2dde939ed7823": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d648b0e9bbd041e4bacd1c108e4f4885": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6618ebb686b43ecaf8e62ec27ca928e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6966f921e1a43a7b304368836e6a8b8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d732d195663347818588b05febebc3a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d8369888ab44410889405511abc8dfc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec7aae7775b6421699fb66139433e904",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f3dac6ff66324419a228cfea92cf9727",
            "value": "special_tokens_map.json:‚Äá100%"
          }
        },
        "da20685aa9b24d58ad41223f45485fe0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "da9ee3c583ac49fa8b3908ed4dc67271": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_73e2870e3515492682471084f0696b58",
              "IPY_MODEL_51b106e8bc18444fbdb84324cfad8c25",
              "IPY_MODEL_ad1bd020b7fe44a1a64c48dd8109eee4"
            ],
            "layout": "IPY_MODEL_2bab671dbdb24a8e81c7f15a9e603f11"
          }
        },
        "db4afb94aa304bf197ceb9518fc175e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74c89dbfa8254809be3ae0119ee4dad7",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_097ae431e07946b98dce00faffb3ec9a",
            "value": 6
          }
        },
        "dd1355db04cd4c20b511a87e118afc6c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd40dd4193d249698c0dd69276cbee4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8279bbe0adab413b95fe5541259afdc3",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_0161fe9b467641b79bc687b6d69c2712",
            "value": "tokenizer.json:‚Äá100%"
          }
        },
        "deb23f0814e146388128ecea0b5beda2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_263cded35c0f40889b72fe2132e464b0",
            "max": 3619380,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_70ba70053bb54a54912dfc38c29d2ee6",
            "value": 3619380
          }
        },
        "e2ce474b812f460f9992bc1956e9c150": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cbe15c57ea0446d38bbc33fae10264ca",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_89e902f1912840389a1aa259c1c84252",
            "value": "‚Äá1.10k/1.10k‚Äá[00:00&lt;00:00,‚Äá143kB/s]"
          }
        },
        "e312eb0a596749fbb9768a58c1d834b7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e35686d576ff451ab13c236df9e2ba97": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ee1eda0eff54491a024a5fcea46c87b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_623a38668f574f83953fde06cf584508",
            "value": "‚Äá6/6‚Äá[02:25&lt;00:00,‚Äá21.50s/it]"
          }
        },
        "e6ba135bd754484d963f46368f9e7b6b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e76afb5ab8924765827fa8b6e7d1e389": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e8ee0c2c8ee2444a993b933f81f70b10": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b7bc7aec1364625814ef5946683ffce",
            "max": 552,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e76afb5ab8924765827fa8b6e7d1e389",
            "value": 552
          }
        },
        "ea0869a5b58a4bd4b9a7209ca008b369": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8284ef46f1f24a16bdc9c54af0d84900",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_8587a2643f3e40049f691f23482239b6",
            "value": "‚Äá6/6‚Äá[00:04&lt;00:00,‚Äá‚Äá1.88it/s]"
          }
        },
        "ebcae4853fd6479f99a1fef118689872": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ebe48e28147e4068813f5e88c64eeb98": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d64747b6cf94a6781dde91b87f593a4",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1f999bfdc60c489a9d38736ee0ddd2ba",
            "value": 6
          }
        },
        "ec7aae7775b6421699fb66139433e904": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed997990922a4e298203825ed094242b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ef3cc101cde64d5792edff060d849558": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_351a8622db5a40be9b01e1676f133405",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_171ada5217eb4514b4f7b6342a06ebae",
            "value": 6
          }
        },
        "ef44f44033324c1ea6699d90a9f4df9d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f02bd61eb9614995a59dbf58f23221b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f02cb2da4d07442babb25ac66930c200": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52c7323d1c6f44e7972805133e8ca134",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_da20685aa9b24d58ad41223f45485fe0",
            "value": "added_tokens.json:‚Äá100%"
          }
        },
        "f02e47abf8514c6781562c6e1c335525": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f2bf1bb4b0054e00b384f04fbdbb86b3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2d0797d50c64308bf2abd67b4bdf2a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f359454a80be4630a31f22156046037e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3dac6ff66324419a228cfea92cf9727": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f59d60f824c04855860b94fae3f63986": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f84166ee9d6b476a830081a41baf7e57": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fbc821514a18484591c601ed18678552": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbce96681f0d4e2f9076ccf85a9d97c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a9d3bbb2eba47949fcbf3b112eb2c2a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_5436c8e4b1a2486dbdc58ff3a7ebeb13",
            "value": "model-00005-of-00006.safetensors:‚Äá100%"
          }
        },
        "fd2fa6d29b2940e2b370520b24821f6f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd479b74ae6e4bcd8cdd2e3cd83be7f9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff3ce6ad11104d3b9461b4cfac7f86d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d7b384d43784b67b42b2f9bb91b0180",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_57867682b2de4d3c93ed009fd6bedb20",
            "value": "processor_config.json:‚Äá100%"
          }
        },
        "ff5c7ac00b734cdaa0a44cbe1acff29e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ffa0fb1b67e3465884ad3f60f18ac13f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6ba135bd754484d963f46368f9e7b6b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_d2ebf8f6dcc245b7b8a5e264ce17903c",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
