{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ex7YQw2aPxyF",
        "outputId": "9b78cc25-1703-4bc4-a281-2d7782fc430d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.5.1+cu124\n",
            "CUDA available: True\n",
            "CUDA device count: 1\n",
            "Current CUDA device: 0\n",
            "GPU name: NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"CUDA device count:\", torch.cuda.device_count())\n",
        "print(\"Current CUDA device:\", torch.cuda.current_device() if torch.cuda.is_available() else \"No CUDA device\")\n",
        "print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "AiHAXztWFvjL"
      },
      "outputs": [],
      "source": [
        "# !pip install -U bitsandbytes\n",
        "# !pip install -U accelerate transformers\n",
        "# !pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjkLMdYeRW6Y"
      },
      "outputs": [],
      "source": [
        "# import zipfile\n",
        "# import os\n",
        "\n",
        "# zip_path = '/content/Compliance_model.zip'\n",
        "# extract_path = \"\"\n",
        "\n",
        "# with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "#     zip_ref.extractall(extract_path)\n",
        "\n",
        "# print(\"Extraction complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPCqXHfLuNMU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9OGrdwvuOg4"
      },
      "source": [
        "###LLava 13 b model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plUYMLqfuNPV"
      },
      "outputs": [],
      "source": [
        "hf_token ='huggin_face_token'\n",
        "#before preparing the data as per model cofiguratios we will read document and take note of how the data preparations should be done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 767,
          "referenced_widgets": [
            "d51451fbe0914d419149dfd01298721c",
            "ff3ce6ad11104d3b9461b4cfac7f86d9",
            "cc268da4acad43f6b4a02535be513add",
            "5f4f5122ace5499f81eae788ba274d5d",
            "b6f402ff54d640ccbb9672e66d9157c9",
            "1d7b384d43784b67b42b2f9bb91b0180",
            "57867682b2de4d3c93ed009fd6bedb20",
            "c19bd2431f9a451fb60e9d37657928c6",
            "38a3a87ec35747c4b724940d237b7e64",
            "4518b7a3d6c547dfa528a185699fa7f7",
            "1abef045e39b446eae3d8d680b8b992d",
            "6321dbf9e1f241289574079c489975f6",
            "a67f21f5806d4b69925acb8f7e70a95a",
            "60d6da517e0b42938ba953e630d7362c",
            "5ae22023fe3640579e37146171d9a36d",
            "5602428751f34d5ea08d167c05a7b360",
            "d6618ebb686b43ecaf8e62ec27ca928e",
            "0995c0c4a711465d8dcbaaa62642301a",
            "27dc915d53764745ae8550691a231b9c",
            "8f4ee815d68e407f9c68e5b76fc6c5c5",
            "06cd2cef14574ba191902916806d1595",
            "39b88f4102ba4222a3afd572629746e0",
            "698ca72845e247269a3e28f2d37fcdb6",
            "8e9b56e3dfeb4b78a78e9a2d47b4fb05",
            "b9b752b3e1284cdea140a891ca9adbe7",
            "880572259c5844b697a5018072d17fcc",
            "9b5a9169572e4a92beb20e2042f1f143",
            "943a02fa93e34fefab30f859dcef7e7d",
            "689ffbb060da46369dac1f7445738f04",
            "20cf22134a8f482a9e72fc5f4c02826b",
            "ed997990922a4e298203825ed094242b",
            "7f6e1b4b30424f9dbdab6d1e3693fdb3",
            "098653a393934263b5b390675488f0bf",
            "3ce27b9d396b44108550c798c1fe443e",
            "3f34d1e53ca24de0b3588220a1c494cb",
            "7775eea944c4417684d63e307b1620ab",
            "6b5ddce7563f41f0ae45c24b6c23d4e5",
            "6a0ac49b9f2e41a6ab7cdcc479df675b",
            "d6966f921e1a43a7b304368836e6a8b8",
            "30a441bee81f4073936fe007eac09590",
            "9e11ab1eab314fe0bc122f78bf68ff6d",
            "b9744ff5ef994155ad950bff292e6706",
            "68b05e1dfd37431cb02af1681d16e4fd",
            "0ab20ec7939342dea7ab3a9bd06a9065",
            "607e1cccc37844b98af4de125c2b14a9",
            "38fa4e98368948d793adb8f414c128de",
            "747f5b540cab4706b7e1ad22fc78fafd",
            "1f50f4c52e5543b7bb4b194e01abee9a",
            "02335270363a449b9ece2551b55d20ce",
            "a1c58f776af54a3d898130e9cd822747",
            "618151761499454490b916a0548b2efd",
            "39d8085120634454b8ac243c557df520",
            "19078e6a65a2444a9b7d1f6bd3e65275",
            "3b9643096e24497190978176855df1e8",
            "8a281f9cc30646839f7c5d9a9fd834e9",
            "a53089b534e244c2aa2af895557eea12",
            "dd40dd4193d249698c0dd69276cbee4c",
            "deb23f0814e146388128ecea0b5beda2",
            "3b9d5757879442829bdbf8c0c56f1f7b",
            "28359b6502c842b88a2c971195f4987e",
            "8279bbe0adab413b95fe5541259afdc3",
            "0161fe9b467641b79bc687b6d69c2712",
            "263cded35c0f40889b72fe2132e464b0",
            "70ba70053bb54a54912dfc38c29d2ee6",
            "fd2fa6d29b2940e2b370520b24821f6f",
            "18460aec28144a05a70456ba583218c1",
            "97c2a58c1e0744e19b9b15d080f1d4e0",
            "f02cb2da4d07442babb25ac66930c200",
            "cc86e318f3bb40a8a34164dbaf88aeef",
            "95fad93ab0a0484a9c223664f2deb1a8",
            "4ebfc388ebce4fd4af62eb43d5b29853",
            "52c7323d1c6f44e7972805133e8ca134",
            "da20685aa9b24d58ad41223f45485fe0",
            "ff5c7ac00b734cdaa0a44cbe1acff29e",
            "ab29260df001471d881a4e2752f5d0de",
            "3de4e73be3cd40449287d401b7ae86ac",
            "5d05f370e24d40a8b0c76275ce349ff7",
            "424ac53f6d194a4fb6eecfd4ae5e465e",
            "d8369888ab44410889405511abc8dfc5",
            "e8ee0c2c8ee2444a993b933f81f70b10",
            "c11aee1f2e7b42ae98dedc47e84555bd",
            "477ecf9b4f0e46ba9b7023c3457b2fa1",
            "ec7aae7775b6421699fb66139433e904",
            "f3dac6ff66324419a228cfea92cf9727",
            "8b7bc7aec1364625814ef5946683ffce",
            "e76afb5ab8924765827fa8b6e7d1e389",
            "2064462ac5334ae28dc3c438072aee37",
            "19d773f9d9064e839988f5d0cc914437",
            "548b13ed359745cfb6b9247189531c2b",
            "68f6409ae0784c05850737623a616688",
            "781e41864d484d999cd5e4ab5a54ac58",
            "e2ce474b812f460f9992bc1956e9c150",
            "36f6581c14914c5faf232ecbbf8c71a2",
            "7a14f1a7126a427cbc33659f32ce0a59",
            "7cf6a50961cf477bbd17b6922278dc4d",
            "4e3406e1984744a19a14896042e1240a",
            "0b8b021ad192444f9466713244ebe47f",
            "cbe15c57ea0446d38bbc33fae10264ca",
            "89e902f1912840389a1aa259c1c84252",
            "b9adcc96c2f04a1cbeccb079320e2abf",
            "a52e2c917b584bd6a128173f97e6f298",
            "171e14b083664289b68d58ebcadad314",
            "ca7bca7de2ee4952bef8cc4b03dfa9c7",
            "10e6b9f415bb48809b0c5a98dd96927b",
            "a98cdad10af043788497e0fb560366b8",
            "372654e39cf54716844fb0d8c8fc9615",
            "6b6d5be66dc24be69195579a35c1ee61",
            "f84166ee9d6b476a830081a41baf7e57",
            "1de32592a96d439093c5a6bd8ff5b777",
            "7ace79cfc887468eae93f5892f8c1998",
            "a0f822d36fcb42018165de4710bbf672",
            "2f521dad8eb54b0aa264437462a1dfd7",
            "a2a391aafcd9468ba69bf9d49600dd4e",
            "4538d6e6940f4ff8acbff03286bf503c",
            "be0118525d1544b6a4a135d0ce101369",
            "40e8147696954198b1f2ca20d9910cb1",
            "1c6a29bd1d0e4217b3f6dcaaca9fa300",
            "0c5471ba94334c849cce149045512b85",
            "4a6e400c58244f6c8021742bb8d00543",
            "f2bf1bb4b0054e00b384f04fbdbb86b3",
            "bd9c45911bec4055849aef7eecc4d6cf",
            "2f731602df604a4e8327fac2c395af99",
            "1bb4e74b6c1d4184b4a5d7722842e43d",
            "9a88ce7f2ac44bffa6a938b8ba60c863",
            "59458c7149844cada0be93f978269a88",
            "5e2d96d9393a4ba08af4f7118b2fb600",
            "4f0ddffd4b85454484649c10122d0f87",
            "29178358a54e4b8fbe0638066e464be9",
            "32a95e1db2464dbda32116ad5879e752",
            "874555ed1d3c478ca521608751492af8",
            "58b610bec47743a5b495b87f30a7bd55",
            "0cd8ade3c3f745768ebb06fcb4198d95",
            "56c2a436293a4a8cb8b62789f4c7cf09",
            "7255b2fd13984d7b81bad09a05daa1ec",
            "ab59bc7059af4075b94f2fac13ff33bb",
            "b36670b48c51417794c21ed195dd1c3b",
            "115036c2e2614fcdaebda2b78a988740",
            "9f12b26a1a464d9a8e3151627d0921d8",
            "d732d195663347818588b05febebc3a3",
            "a97d86e8dc3c47d2b53a046e26abdfc6",
            "228829dfb52d43edbcaf2f5332b58f42",
            "5a7f767393a9427d93486dd8356e8106",
            "a46baa939e8d42bbbfa3db1771f90bc3",
            "82b7050c4fe349e6a6fc49a0608d4ec8",
            "c3507c5985454c36aa4f83e2a661fb16",
            "d54ea90f246f4ef5a54c7e28a968e274",
            "703a9e14e32644278e1ed3b85728facb",
            "3020ee49c50f47109994c58631a3ee44",
            "224695c1308b421dad6890cf4126daae",
            "06093206fd8d4ef58c1e8f3741caf1ee",
            "8abbedc4dc0e4deab08b414ba9210266",
            "b4de01d0890845f9992dc90ebd3b410d",
            "50d9a1a00a9c465c945c3c1c928f9771",
            "3228561877b04c45ab7338abd0d02f13",
            "8764d39c88494cd89f20d207f79701fe",
            "cfb58c0dd80048f3ae641962af17a56c",
            "b66f5ce342ae4c0b8a6a7deb6c2ba912",
            "266e6b9d6ffd429ba4562d1d6b6fb36b",
            "87bd17d77a1d4caeb97b70234dbd7248",
            "13434db54bd04e208bd9ef033446d1e5",
            "bfe5351acf1647b5b0e93656e318dae1",
            "174f64acaeb74877b85527ab9ac60261",
            "82b545728c944f519ad1e12134426771",
            "5f8328d54e6b4556b8187220961ae740",
            "91fd22599ef6497584d144f032eb77b9",
            "ccd1ea6fc6d64a7fb4496369138aa6a9",
            "fbce96681f0d4e2f9076ccf85a9d97c0",
            "686b427a930042819fa618ad27bcfdba",
            "7381515a531d4cf78f5c5ecc80a2ca94",
            "cc1cc4bca9fe4646a420465acc20a6cd",
            "3a9d3bbb2eba47949fcbf3b112eb2c2a",
            "5436c8e4b1a2486dbdc58ff3a7ebeb13",
            "d0d5b29cdc4f411db2401e5860cc6a56",
            "b90279bc775445e8b1f69e94eeb3a710",
            "1020ba121d494d98b946de1abf6d2e7d",
            "91faf571a67447679f56043e6a353ee6",
            "b0f8aef051fe490c86c1ffc920bbf4a7",
            "3a25e247fc9b4c5f9ddac407e265c4be",
            "9553e75ebc83484ebb8dd651864c19c1",
            "ad4d9413e1d746c69722b1ea42d709d8",
            "ef44f44033324c1ea6699d90a9f4df9d",
            "d02da867109d48b998b92fdb4a1f9c99",
            "64ff73504e714597b5d115cea8ae43e8",
            "164dde685a3848999d25281206d1f10b",
            "2e83a991a043478ca5879ec3a6ff1fe3",
            "c237d95d55864f84b574d8a2cb59268d",
            "2046809f60e24821afcc8cee6d1db569",
            "6dc913be8ca24d0c917c480bf9b59d28",
            "37b1810d57464a9c8cda4c19c1d7fe22",
            "4e8b59feb2ac4be2884f409f4c57cff6",
            "11b265e2d9c54213b10848f90a74a320",
            "dd1355db04cd4c20b511a87e118afc6c",
            "bf32d9862cda431dbb5c7396a47daf3b",
            "10977be18b3144e7a98f741c78275399",
            "3856a259d0a541b8bf131a81a455bc6a",
            "d5a00820206348cba3a2dde939ed7823",
            "bf9748fd88514783b593ddb661b5e746",
            "7b09b70ecb854904856742c0182d31be",
            "105e12c3acc4499a86f059ef0e48b979",
            "5a15e5f700e74803a6b9a6e033aa2107",
            "76edb62358f943b8936a59d110d1dad4",
            "4461995efbee492ca3b14379119427c4",
            "98b9e4aa78964793abbd93b6f386d18e",
            "298dbd52ae6441749cc576189dc57dcc",
            "12bab7724d5a49868571c1572e0fa139",
            "3b8cc33626434244971000388c7ad476",
            "d3819e70fcf2496b8357fdb08da0ebf1",
            "b91d6730f3924be6a4b9bfe34a099457",
            "be5c6055ff8c4b06abe7c301701a2d49"
          ]
        },
        "collapsed": true,
        "id": "AjzqLSSxuNSN",
        "outputId": "190be93d-ea38-4004-bcd3-c39a1487f274"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA available: True\n",
            "Device name: NVIDIA A100-SXM4-40GB\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d51451fbe0914d419149dfd01298721c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "processor_config.json:   0%|          | 0.00/173 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6321dbf9e1f241289574079c489975f6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "chat_template.json:   0%|          | 0.00/701 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "698ca72845e247269a3e28f2d37fcdb6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/505 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3ce27b9d396b44108550c798c1fe443e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.45k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "607e1cccc37844b98af4de125c2b14a9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a53089b534e244c2aa2af895557eea12",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/3.62M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "97c2a58c1e0744e19b9b15d080f1d4e0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/41.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "424ac53f6d194a4fb6eecfd4ae5e465e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/552 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "548b13ed359745cfb6b9247189531c2b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/1.10k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b9adcc96c2f04a1cbeccb079320e2abf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/77.2k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a0f822d36fcb42018165de4710bbf672",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading shards:   0%|          | 0/6 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2f731602df604a4e8327fac2c395af99",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00006.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "56c2a436293a4a8cb8b62789f4c7cf09",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00006.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "82b7050c4fe349e6a6fc49a0608d4ec8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00003-of-00006.safetensors:   0%|          | 0.00/4.88G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8764d39c88494cd89f20d207f79701fe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00004-of-00006.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ccd1ea6fc6d64a7fb4496369138aa6a9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00005-of-00006.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b0f8aef051fe490c86c1ffc920bbf4a7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00006-of-00006.safetensors:   0%|          | 0.00/2.02G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6dc913be8ca24d0c917c480bf9b59d28",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "105e12c3acc4499a86f059ef0e48b979",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/141 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✅ Model and processor loaded successfully!\n",
            "Model loaded on: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoProcessor, LlavaForConditionalGeneration, BitsAndBytesConfig\n",
        "\n",
        "# Check CUDA availability\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "# Model name\n",
        "model_name = \"llava-hf/llava-1.5-13b-hf\"\n",
        "\n",
        "# Load processor\n",
        "processor = AutoProcessor.from_pretrained(model_name, token='hf_token')\n",
        "\n",
        "# Load model and move to GPU\n",
        "try:\n",
        "\n",
        "    model = LlavaForConditionalGeneration.from_pretrained(\n",
        "        model_name,\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        llm_int8_enable_fp32_cpu_offload=True,\n",
        "        token=\"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\",\n",
        "        device_map=\"auto\",  # Automatically place on GPU\n",
        "        torch_dtype=torch.float16,\n",
        "        #torch_compile=False,\n",
        "    )\n",
        "\n",
        "    # Enable gradient checkpointing for memory efficiency\n",
        "    model.gradient_checkpointing_enable()\n",
        "\n",
        "    print(\"\\n✅ Model and processor loaded successfully!\")\n",
        "    print(f\"Model loaded on: {device}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n❌ Error loading model: {e}\")\n",
        "    model = None\n",
        "    processor = None\n",
        "\n",
        "# Ensure processor is also set to use GPU\n",
        "if processor is not None:\n",
        "    processor.device = device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-oKXUyP1xByT"
      },
      "outputs": [],
      "source": [
        "# print(\"\\n🔍 **Model Expected Input Signature** 🔍\")\n",
        "# print(model.forward.__doc__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkVXMiB606CI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "class LLaVADataset(Dataset):\n",
        "    def __init__(self, image_folder, processor, max_length=2048, image_size=(336, 336), patch_size=14):\n",
        "        self.image_folder = image_folder\n",
        "        self.processor = processor\n",
        "        self.max_length = max_length\n",
        "        self.image_size = image_size\n",
        "        self.patch_size = patch_size\n",
        "        self.image_token_id = processor.tokenizer.convert_tokens_to_ids(\"<image>\")\n",
        "\n",
        "        # Gather .jpg files\n",
        "        self.image_files = [f for f in os.listdir(image_folder) if f.endswith(\".jpg\")]\n",
        "\n",
        "        if not self.image_files:\n",
        "            raise ValueError(f\"No images found in {image_folder}. Check dataset path!\")\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize(self.image_size),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "        self.patch_count_h = self.image_size[0] // self.patch_size\n",
        "        self.patch_count_w = self.image_size[1] // self.patch_size\n",
        "        self.num_patches = self.patch_count_h * self.patch_count_w\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_file = self.image_files[idx]\n",
        "        image_path = os.path.join(self.image_folder, image_file)\n",
        "\n",
        "        if not os.path.exists(image_path):\n",
        "            print(f\"⚠️ Missing image: {image_path}\")\n",
        "            return None  # Return None to be filtered later\n",
        "\n",
        "        try:\n",
        "            caption_text = os.path.splitext(image_file)[0].replace(\"_\", \" \")\n",
        "            text_prompt = f\"Here is an image: {caption_text}\\n\"\n",
        "            image = Image.open(image_path).convert(\"RGB\")\n",
        "            image_tensor = self.transform(image)\n",
        "\n",
        "            text_inputs = self.processor.tokenizer(\n",
        "                text_prompt,\n",
        "                padding=\"max_length\",\n",
        "                truncation=True,\n",
        "                max_length=max(0, self.max_length - self.num_patches),\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "            input_ids = text_inputs[\"input_ids\"].squeeze(0).to(torch.int64)\n",
        "            attention_mask = text_inputs[\"attention_mask\"].squeeze(0).to(torch.int64)\n",
        "\n",
        "            # Append 576 <image> tokens\n",
        "            image_tokens = torch.tensor([self.image_token_id] * self.num_patches, dtype=torch.int64)\n",
        "            input_ids = torch.cat([input_ids, image_tokens])\n",
        "            image_attn = torch.ones_like(image_tokens, dtype=torch.int64)\n",
        "            attention_mask = torch.cat([attention_mask, image_attn])\n",
        "\n",
        "            image_grid_thw = torch.tensor([1, self.patch_count_h, self.patch_count_w], dtype=torch.int64)\n",
        "\n",
        "            labels = input_ids.clone()\n",
        "            pad_id = self.processor.tokenizer.pad_token_id\n",
        "            if pad_id is not None:\n",
        "                labels[labels == pad_id] = -100\n",
        "\n",
        "            return {\n",
        "                \"input_ids\": input_ids,\n",
        "                \"attention_mask\": attention_mask,\n",
        "                \"pixel_values\": image_tensor.to(torch.float32),\n",
        "                \"image_grid_thw\": image_grid_thw,\n",
        "                \"labels\": labels,\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Error processing {image_file}: {e}\")\n",
        "            return None  # Return None to be filtered\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KE23keoQ10-_"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch\n",
        "\n",
        "def collate_fn(batch):\n",
        "    batch = [b for b in batch if b is not None]\n",
        "\n",
        "    if not batch:\n",
        "        raise ValueError(\"Empty batch after filtering - check dataset or tokenization errors\")\n",
        "\n",
        "    input_ids = [item[\"input_ids\"].squeeze(0) for item in batch]\n",
        "    attention_mask = [item[\"attention_mask\"].squeeze(0) for item in batch]\n",
        "    pixel_values = [item[\"pixel_values\"] for item in batch]\n",
        "    image_grid_thw = [item[\"image_grid_thw\"] for item in batch]\n",
        "    labels = [item[\"labels\"].squeeze(0) for item in batch]\n",
        "\n",
        "    padded_input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
        "    padded_attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
        "    padded_labels = pad_sequence(labels, batch_first=True, padding_value=-100)\n",
        "\n",
        "    try:\n",
        "        pixel_values = torch.stack(pixel_values)\n",
        "        image_grid_thw = torch.stack(image_grid_thw)\n",
        "    except RuntimeError as e:\n",
        "        raise ValueError(f\"Image tensor shape mismatch: {[p.shape for p in pixel_values]}\") from e\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": padded_input_ids,\n",
        "        \"attention_mask\": padded_attention_mask,\n",
        "        \"pixel_values\": pixel_values,\n",
        "        \"image_grid_thw\": image_grid_thw,\n",
        "        \"labels\": padded_labels\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1Odcs_6ALnP"
      },
      "outputs": [],
      "source": [
        "# \"image_grid_thw\": image_grid_thw,\n",
        "# image_grid_thw = torch.stack(image_grid_thw)\n",
        "# image_grid_thw = [item[\"image_grid_thw\"] for item in batch]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ROzSgEDd13Wl",
        "outputId": "9dec7f62-bbb7-43e3-97dd-da3e1ed1ba9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 746/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 747/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 748/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 749/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 750/1460\n",
            "   input_ids: torch.Size([4, 607])\n",
            "   attention_mask: torch.Size([4, 607])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 607])\n",
            "\n",
            "🔹 Training Step 751/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "🔹 Training Step 752/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 753/1460\n",
            "   input_ids: torch.Size([4, 611])\n",
            "   attention_mask: torch.Size([4, 611])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 611])\n",
            "\n",
            "🔹 Training Step 754/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 755/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "🔹 Training Step 756/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 757/1460\n",
            "   input_ids: torch.Size([4, 612])\n",
            "   attention_mask: torch.Size([4, 612])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 612])\n",
            "\n",
            "🔹 Training Step 758/1460\n",
            "   input_ids: torch.Size([4, 601])\n",
            "   attention_mask: torch.Size([4, 601])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 601])\n",
            "\n",
            "🔹 Training Step 759/1460\n",
            "   input_ids: torch.Size([4, 604])\n",
            "   attention_mask: torch.Size([4, 604])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 604])\n",
            "\n",
            "🔹 Training Step 760/1460\n",
            "   input_ids: torch.Size([4, 611])\n",
            "   attention_mask: torch.Size([4, 611])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 611])\n",
            "\n",
            "🔹 Training Step 761/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 762/1460\n",
            "   input_ids: torch.Size([4, 611])\n",
            "   attention_mask: torch.Size([4, 611])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 611])\n",
            "\n",
            "🔹 Training Step 763/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 764/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 765/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 766/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 767/1460\n",
            "   input_ids: torch.Size([4, 610])\n",
            "   attention_mask: torch.Size([4, 610])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 610])\n",
            "\n",
            "🔹 Training Step 768/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 769/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "🔹 Training Step 770/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 771/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 772/1460\n",
            "   input_ids: torch.Size([4, 601])\n",
            "   attention_mask: torch.Size([4, 601])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 601])\n",
            "\n",
            "🔹 Training Step 773/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "🔹 Training Step 774/1460\n",
            "   input_ids: torch.Size([4, 605])\n",
            "   attention_mask: torch.Size([4, 605])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 605])\n",
            "\n",
            "🔹 Training Step 775/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 776/1460\n",
            "   input_ids: torch.Size([4, 595])\n",
            "   attention_mask: torch.Size([4, 595])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 595])\n",
            "\n",
            "🔹 Training Step 777/1460\n",
            "   input_ids: torch.Size([4, 601])\n",
            "   attention_mask: torch.Size([4, 601])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 601])\n",
            "\n",
            "🔹 Training Step 778/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "🔹 Training Step 779/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 780/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 781/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 782/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "🔹 Training Step 783/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 784/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 785/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 786/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 787/1460\n",
            "   input_ids: torch.Size([4, 607])\n",
            "   attention_mask: torch.Size([4, 607])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 607])\n",
            "\n",
            "🔹 Training Step 788/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 789/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 790/1460\n",
            "   input_ids: torch.Size([4, 606])\n",
            "   attention_mask: torch.Size([4, 606])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 606])\n",
            "\n",
            "🔹 Training Step 791/1460\n",
            "   input_ids: torch.Size([4, 607])\n",
            "   attention_mask: torch.Size([4, 607])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 607])\n",
            "\n",
            "🔹 Training Step 792/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 793/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 794/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 795/1460\n",
            "   input_ids: torch.Size([4, 616])\n",
            "   attention_mask: torch.Size([4, 616])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 616])\n",
            "\n",
            "🔹 Training Step 796/1460\n",
            "   input_ids: torch.Size([4, 602])\n",
            "   attention_mask: torch.Size([4, 602])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 602])\n",
            "\n",
            "🔹 Training Step 797/1460\n",
            "   input_ids: torch.Size([4, 595])\n",
            "   attention_mask: torch.Size([4, 595])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 595])\n",
            "\n",
            "🔹 Training Step 798/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 799/1460\n",
            "   input_ids: torch.Size([4, 601])\n",
            "   attention_mask: torch.Size([4, 601])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 601])\n",
            "\n",
            "🔹 Training Step 800/1460\n",
            "   input_ids: torch.Size([4, 614])\n",
            "   attention_mask: torch.Size([4, 614])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 614])\n",
            "\n",
            "🔹 Training Step 801/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 802/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 803/1460\n",
            "   input_ids: torch.Size([4, 613])\n",
            "   attention_mask: torch.Size([4, 613])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 613])\n",
            "\n",
            "🔹 Training Step 804/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 805/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "🔹 Training Step 806/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 807/1460\n",
            "   input_ids: torch.Size([4, 602])\n",
            "   attention_mask: torch.Size([4, 602])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 602])\n",
            "\n",
            "🔹 Training Step 808/1460\n",
            "   input_ids: torch.Size([4, 607])\n",
            "   attention_mask: torch.Size([4, 607])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 607])\n",
            "\n",
            "🔹 Training Step 809/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "🔹 Training Step 810/1460\n",
            "   input_ids: torch.Size([4, 606])\n",
            "   attention_mask: torch.Size([4, 606])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 606])\n",
            "\n",
            "🔹 Training Step 811/1460\n",
            "   input_ids: torch.Size([4, 612])\n",
            "   attention_mask: torch.Size([4, 612])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 612])\n",
            "\n",
            "🔹 Training Step 812/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 813/1460\n",
            "   input_ids: torch.Size([4, 604])\n",
            "   attention_mask: torch.Size([4, 604])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 604])\n",
            "\n",
            "🔹 Training Step 814/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 815/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 816/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 817/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 818/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 819/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 820/1460\n",
            "   input_ids: torch.Size([4, 605])\n",
            "   attention_mask: torch.Size([4, 605])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 605])\n",
            "\n",
            "🔹 Training Step 821/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 822/1460\n",
            "   input_ids: torch.Size([4, 605])\n",
            "   attention_mask: torch.Size([4, 605])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 605])\n",
            "\n",
            "🔹 Training Step 823/1460\n",
            "   input_ids: torch.Size([4, 608])\n",
            "   attention_mask: torch.Size([4, 608])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 608])\n",
            "\n",
            "🔹 Training Step 824/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 825/1460\n",
            "   input_ids: torch.Size([4, 603])\n",
            "   attention_mask: torch.Size([4, 603])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 603])\n",
            "\n",
            "🔹 Training Step 826/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 827/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 828/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 829/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 830/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 831/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 832/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 833/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 834/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 835/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 836/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 837/1460\n",
            "   input_ids: torch.Size([4, 608])\n",
            "   attention_mask: torch.Size([4, 608])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 608])\n",
            "\n",
            "🔹 Training Step 838/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 839/1460\n",
            "   input_ids: torch.Size([4, 609])\n",
            "   attention_mask: torch.Size([4, 609])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 609])\n",
            "\n",
            "🔹 Training Step 840/1460\n",
            "   input_ids: torch.Size([4, 603])\n",
            "   attention_mask: torch.Size([4, 603])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 603])\n",
            "\n",
            "🔹 Training Step 841/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 842/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 843/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 844/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 845/1460\n",
            "   input_ids: torch.Size([4, 608])\n",
            "   attention_mask: torch.Size([4, 608])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 608])\n",
            "\n",
            "🔹 Training Step 846/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 847/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 848/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 849/1460\n",
            "   input_ids: torch.Size([4, 607])\n",
            "   attention_mask: torch.Size([4, 607])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 607])\n",
            "\n",
            "🔹 Training Step 850/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "🔹 Training Step 851/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 852/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 853/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 854/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 855/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 856/1460\n",
            "   input_ids: torch.Size([4, 607])\n",
            "   attention_mask: torch.Size([4, 607])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 607])\n",
            "\n",
            "🔹 Training Step 857/1460\n",
            "   input_ids: torch.Size([4, 606])\n",
            "   attention_mask: torch.Size([4, 606])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 606])\n",
            "\n",
            "🔹 Training Step 858/1460\n",
            "   input_ids: torch.Size([4, 614])\n",
            "   attention_mask: torch.Size([4, 614])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 614])\n",
            "\n",
            "🔹 Training Step 859/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 860/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "🔹 Training Step 861/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 862/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "🔹 Training Step 863/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "🔹 Training Step 864/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "🔹 Training Step 865/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 866/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 867/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 868/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 869/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 870/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 871/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 872/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 873/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 874/1460\n",
            "   input_ids: torch.Size([4, 609])\n",
            "   attention_mask: torch.Size([4, 609])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 609])\n",
            "\n",
            "🔹 Training Step 875/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 876/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 877/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "🔹 Training Step 878/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 879/1460\n",
            "   input_ids: torch.Size([4, 611])\n",
            "   attention_mask: torch.Size([4, 611])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 611])\n",
            "\n",
            "🔹 Training Step 880/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 881/1460\n",
            "   input_ids: torch.Size([4, 606])\n",
            "   attention_mask: torch.Size([4, 606])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 606])\n",
            "\n",
            "🔹 Training Step 882/1460\n",
            "   input_ids: torch.Size([4, 606])\n",
            "   attention_mask: torch.Size([4, 606])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 606])\n",
            "\n",
            "🔹 Training Step 883/1460\n",
            "   input_ids: torch.Size([4, 595])\n",
            "   attention_mask: torch.Size([4, 595])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 595])\n",
            "\n",
            "🔹 Training Step 884/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 885/1460\n",
            "   input_ids: torch.Size([4, 608])\n",
            "   attention_mask: torch.Size([4, 608])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 608])\n",
            "\n",
            "🔹 Training Step 886/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 887/1460\n",
            "   input_ids: torch.Size([4, 611])\n",
            "   attention_mask: torch.Size([4, 611])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 611])\n",
            "\n",
            "🔹 Training Step 888/1460\n",
            "   input_ids: torch.Size([4, 608])\n",
            "   attention_mask: torch.Size([4, 608])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 608])\n",
            "\n",
            "🔹 Training Step 889/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 890/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 891/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "🔹 Training Step 892/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 893/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 894/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 895/1460\n",
            "   input_ids: torch.Size([4, 611])\n",
            "   attention_mask: torch.Size([4, 611])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 611])\n",
            "\n",
            "🔹 Training Step 896/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 897/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 898/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 899/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 900/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 901/1460\n",
            "   input_ids: torch.Size([4, 609])\n",
            "   attention_mask: torch.Size([4, 609])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 609])\n",
            "\n",
            "🔹 Training Step 902/1460\n",
            "   input_ids: torch.Size([4, 595])\n",
            "   attention_mask: torch.Size([4, 595])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 595])\n",
            "\n",
            "🔹 Training Step 903/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 904/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 905/1460\n",
            "   input_ids: torch.Size([4, 603])\n",
            "   attention_mask: torch.Size([4, 603])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 603])\n",
            "\n",
            "🔹 Training Step 906/1460\n",
            "   input_ids: torch.Size([4, 606])\n",
            "   attention_mask: torch.Size([4, 606])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 606])\n",
            "\n",
            "🔹 Training Step 907/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 908/1460\n",
            "   input_ids: torch.Size([4, 606])\n",
            "   attention_mask: torch.Size([4, 606])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 606])\n",
            "\n",
            "🔹 Training Step 909/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 910/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 911/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 912/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 913/1460\n",
            "   input_ids: torch.Size([4, 595])\n",
            "   attention_mask: torch.Size([4, 595])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 595])\n",
            "\n",
            "🔹 Training Step 914/1460\n",
            "   input_ids: torch.Size([4, 607])\n",
            "   attention_mask: torch.Size([4, 607])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 607])\n",
            "\n",
            "🔹 Training Step 915/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 916/1460\n",
            "   input_ids: torch.Size([4, 605])\n",
            "   attention_mask: torch.Size([4, 605])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 605])\n",
            "\n",
            "🔹 Training Step 917/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "🔹 Training Step 918/1460\n",
            "   input_ids: torch.Size([4, 603])\n",
            "   attention_mask: torch.Size([4, 603])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 603])\n",
            "\n",
            "🔹 Training Step 919/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 920/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "🔹 Training Step 921/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 922/1460\n",
            "   input_ids: torch.Size([4, 611])\n",
            "   attention_mask: torch.Size([4, 611])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 611])\n",
            "\n",
            "🔹 Training Step 923/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 924/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 925/1460\n",
            "   input_ids: torch.Size([4, 607])\n",
            "   attention_mask: torch.Size([4, 607])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 607])\n",
            "\n",
            "🔹 Training Step 926/1460\n",
            "   input_ids: torch.Size([4, 604])\n",
            "   attention_mask: torch.Size([4, 604])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 604])\n",
            "\n",
            "🔹 Training Step 927/1460\n",
            "   input_ids: torch.Size([4, 613])\n",
            "   attention_mask: torch.Size([4, 613])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 613])\n",
            "\n",
            "🔹 Training Step 928/1460\n",
            "   input_ids: torch.Size([4, 607])\n",
            "   attention_mask: torch.Size([4, 607])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 607])\n",
            "\n",
            "🔹 Training Step 929/1460\n",
            "   input_ids: torch.Size([4, 610])\n",
            "   attention_mask: torch.Size([4, 610])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 610])\n",
            "\n",
            "🔹 Training Step 930/1460\n",
            "   input_ids: torch.Size([4, 608])\n",
            "   attention_mask: torch.Size([4, 608])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 608])\n",
            "\n",
            "🔹 Training Step 931/1460\n",
            "   input_ids: torch.Size([4, 609])\n",
            "   attention_mask: torch.Size([4, 609])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 609])\n",
            "\n",
            "🔹 Training Step 932/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "🔹 Training Step 933/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 934/1460\n",
            "   input_ids: torch.Size([4, 602])\n",
            "   attention_mask: torch.Size([4, 602])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 602])\n",
            "\n",
            "🔹 Training Step 935/1460\n",
            "   input_ids: torch.Size([4, 608])\n",
            "   attention_mask: torch.Size([4, 608])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 608])\n",
            "\n",
            "🔹 Training Step 936/1460\n",
            "   input_ids: torch.Size([4, 605])\n",
            "   attention_mask: torch.Size([4, 605])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 605])\n",
            "\n",
            "🔹 Training Step 937/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 938/1460\n",
            "   input_ids: torch.Size([4, 610])\n",
            "   attention_mask: torch.Size([4, 610])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 610])\n",
            "\n",
            "🔹 Training Step 939/1460\n",
            "   input_ids: torch.Size([4, 612])\n",
            "   attention_mask: torch.Size([4, 612])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 612])\n",
            "\n",
            "🔹 Training Step 940/1460\n",
            "   input_ids: torch.Size([4, 606])\n",
            "   attention_mask: torch.Size([4, 606])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 606])\n",
            "\n",
            "🔹 Training Step 941/1460\n",
            "   input_ids: torch.Size([4, 606])\n",
            "   attention_mask: torch.Size([4, 606])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 606])\n",
            "\n",
            "🔹 Training Step 942/1460\n",
            "   input_ids: torch.Size([4, 611])\n",
            "   attention_mask: torch.Size([4, 611])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 611])\n",
            "\n",
            "🔹 Training Step 943/1460\n",
            "   input_ids: torch.Size([4, 613])\n",
            "   attention_mask: torch.Size([4, 613])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 613])\n",
            "\n",
            "🔹 Training Step 944/1460\n",
            "   input_ids: torch.Size([4, 601])\n",
            "   attention_mask: torch.Size([4, 601])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 601])\n",
            "\n",
            "🔹 Training Step 945/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 946/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 947/1460\n",
            "   input_ids: torch.Size([4, 606])\n",
            "   attention_mask: torch.Size([4, 606])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 606])\n",
            "\n",
            "🔹 Training Step 948/1460\n",
            "   input_ids: torch.Size([4, 616])\n",
            "   attention_mask: torch.Size([4, 616])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 616])\n",
            "\n",
            "🔹 Training Step 949/1460\n",
            "   input_ids: torch.Size([4, 601])\n",
            "   attention_mask: torch.Size([4, 601])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 601])\n",
            "\n",
            "🔹 Training Step 950/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 951/1460\n",
            "   input_ids: torch.Size([4, 610])\n",
            "   attention_mask: torch.Size([4, 610])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 610])\n",
            "\n",
            "🔹 Training Step 952/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 953/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "🔹 Training Step 954/1460\n",
            "   input_ids: torch.Size([4, 602])\n",
            "   attention_mask: torch.Size([4, 602])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 602])\n",
            "\n",
            "🔹 Training Step 955/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 956/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 957/1460\n",
            "   input_ids: torch.Size([4, 595])\n",
            "   attention_mask: torch.Size([4, 595])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 595])\n",
            "\n",
            "🔹 Training Step 958/1460\n",
            "   input_ids: torch.Size([4, 595])\n",
            "   attention_mask: torch.Size([4, 595])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 595])\n",
            "\n",
            "🔹 Training Step 959/1460\n",
            "   input_ids: torch.Size([4, 606])\n",
            "   attention_mask: torch.Size([4, 606])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 606])\n",
            "\n",
            "🔹 Training Step 960/1460\n",
            "   input_ids: torch.Size([4, 614])\n",
            "   attention_mask: torch.Size([4, 614])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 614])\n",
            "\n",
            "🔹 Training Step 961/1460\n",
            "   input_ids: torch.Size([4, 604])\n",
            "   attention_mask: torch.Size([4, 604])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 604])\n",
            "\n",
            "🔹 Training Step 962/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 963/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 964/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 965/1460\n",
            "   input_ids: torch.Size([4, 612])\n",
            "   attention_mask: torch.Size([4, 612])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 612])\n",
            "\n",
            "🔹 Training Step 966/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 967/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 968/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 969/1460\n",
            "   input_ids: torch.Size([4, 610])\n",
            "   attention_mask: torch.Size([4, 610])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 610])\n",
            "\n",
            "🔹 Training Step 970/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 971/1460\n",
            "   input_ids: torch.Size([4, 595])\n",
            "   attention_mask: torch.Size([4, 595])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 595])\n",
            "\n",
            "🔹 Training Step 972/1460\n",
            "   input_ids: torch.Size([4, 606])\n",
            "   attention_mask: torch.Size([4, 606])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 606])\n",
            "\n",
            "🔹 Training Step 973/1460\n",
            "   input_ids: torch.Size([4, 606])\n",
            "   attention_mask: torch.Size([4, 606])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 606])\n",
            "\n",
            "🔹 Training Step 974/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "🔹 Training Step 975/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 976/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 977/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 978/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 979/1460\n",
            "   input_ids: torch.Size([4, 607])\n",
            "   attention_mask: torch.Size([4, 607])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 607])\n",
            "\n",
            "🔹 Training Step 980/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 981/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 982/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 983/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 984/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 985/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 986/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 987/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 988/1460\n",
            "   input_ids: torch.Size([4, 604])\n",
            "   attention_mask: torch.Size([4, 604])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 604])\n",
            "\n",
            "🔹 Training Step 989/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 990/1460\n",
            "   input_ids: torch.Size([4, 604])\n",
            "   attention_mask: torch.Size([4, 604])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 604])\n",
            "\n",
            "🔹 Training Step 991/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 992/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 993/1460\n",
            "   input_ids: torch.Size([4, 609])\n",
            "   attention_mask: torch.Size([4, 609])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 609])\n",
            "\n",
            "🔹 Training Step 994/1460\n",
            "   input_ids: torch.Size([4, 616])\n",
            "   attention_mask: torch.Size([4, 616])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 616])\n",
            "\n",
            "🔹 Training Step 995/1460\n",
            "   input_ids: torch.Size([4, 610])\n",
            "   attention_mask: torch.Size([4, 610])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 610])\n",
            "\n",
            "🔹 Training Step 996/1460\n",
            "   input_ids: torch.Size([4, 609])\n",
            "   attention_mask: torch.Size([4, 609])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 609])\n",
            "\n",
            "🔹 Training Step 997/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "🔹 Training Step 998/1460\n",
            "   input_ids: torch.Size([4, 605])\n",
            "   attention_mask: torch.Size([4, 605])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 605])\n",
            "\n",
            "🔹 Training Step 999/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1000/1460\n",
            "   input_ids: torch.Size([4, 607])\n",
            "   attention_mask: torch.Size([4, 607])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 607])\n",
            "\n",
            "🔹 Training Step 1001/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 1002/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 1003/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1004/1460\n",
            "   input_ids: torch.Size([4, 595])\n",
            "   attention_mask: torch.Size([4, 595])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 595])\n",
            "\n",
            "🔹 Training Step 1005/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1006/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 1007/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 1008/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 1009/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "🔹 Training Step 1010/1460\n",
            "   input_ids: torch.Size([4, 594])\n",
            "   attention_mask: torch.Size([4, 594])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 594])\n",
            "\n",
            "🔹 Training Step 1011/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1012/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1013/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1014/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1015/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1016/1460\n",
            "   input_ids: torch.Size([4, 616])\n",
            "   attention_mask: torch.Size([4, 616])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 616])\n",
            "\n",
            "🔹 Training Step 1017/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1018/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1019/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "🔹 Training Step 1020/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1021/1460\n",
            "   input_ids: torch.Size([4, 616])\n",
            "   attention_mask: torch.Size([4, 616])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 616])\n",
            "\n",
            "🔹 Training Step 1022/1460\n",
            "   input_ids: torch.Size([4, 615])\n",
            "   attention_mask: torch.Size([4, 615])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 615])\n",
            "\n",
            "🔹 Training Step 1023/1460\n",
            "   input_ids: torch.Size([4, 601])\n",
            "   attention_mask: torch.Size([4, 601])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 601])\n",
            "\n",
            "🔹 Training Step 1024/1460\n",
            "   input_ids: torch.Size([4, 610])\n",
            "   attention_mask: torch.Size([4, 610])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 610])\n",
            "\n",
            "🔹 Training Step 1025/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 1026/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 1027/1460\n",
            "   input_ids: torch.Size([4, 607])\n",
            "   attention_mask: torch.Size([4, 607])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 607])\n",
            "\n",
            "🔹 Training Step 1028/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1029/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1030/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1031/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1032/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1033/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 1034/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1035/1460\n",
            "   input_ids: torch.Size([4, 609])\n",
            "   attention_mask: torch.Size([4, 609])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 609])\n",
            "\n",
            "🔹 Training Step 1036/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 1037/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1038/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1039/1460\n",
            "   input_ids: torch.Size([4, 595])\n",
            "   attention_mask: torch.Size([4, 595])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 595])\n",
            "\n",
            "🔹 Training Step 1040/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "🔹 Training Step 1041/1460\n",
            "   input_ids: torch.Size([4, 603])\n",
            "   attention_mask: torch.Size([4, 603])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 603])\n",
            "\n",
            "🔹 Training Step 1042/1460\n",
            "   input_ids: torch.Size([4, 617])\n",
            "   attention_mask: torch.Size([4, 617])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 617])\n",
            "\n",
            "🔹 Training Step 1043/1460\n",
            "   input_ids: torch.Size([4, 605])\n",
            "   attention_mask: torch.Size([4, 605])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 605])\n",
            "\n",
            "🔹 Training Step 1044/1460\n",
            "   input_ids: torch.Size([4, 611])\n",
            "   attention_mask: torch.Size([4, 611])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 611])\n",
            "\n",
            "🔹 Training Step 1045/1460\n",
            "   input_ids: torch.Size([4, 608])\n",
            "   attention_mask: torch.Size([4, 608])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 608])\n",
            "\n",
            "🔹 Training Step 1046/1460\n",
            "   input_ids: torch.Size([4, 607])\n",
            "   attention_mask: torch.Size([4, 607])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 607])\n",
            "\n",
            "🔹 Training Step 1047/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1048/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1049/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1050/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 1051/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1052/1460\n",
            "   input_ids: torch.Size([4, 611])\n",
            "   attention_mask: torch.Size([4, 611])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 611])\n",
            "\n",
            "🔹 Training Step 1053/1460\n",
            "   input_ids: torch.Size([4, 605])\n",
            "   attention_mask: torch.Size([4, 605])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 605])\n",
            "\n",
            "🔹 Training Step 1054/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "🔹 Training Step 1055/1460\n",
            "   input_ids: torch.Size([4, 602])\n",
            "   attention_mask: torch.Size([4, 602])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 602])\n",
            "\n",
            "🔹 Training Step 1056/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 1057/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1058/1460\n",
            "   input_ids: torch.Size([4, 614])\n",
            "   attention_mask: torch.Size([4, 614])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 614])\n",
            "\n",
            "🔹 Training Step 1059/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 1060/1460\n",
            "   input_ids: torch.Size([4, 595])\n",
            "   attention_mask: torch.Size([4, 595])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 595])\n",
            "\n",
            "🔹 Training Step 1061/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1062/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 1063/1460\n",
            "   input_ids: torch.Size([4, 612])\n",
            "   attention_mask: torch.Size([4, 612])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 612])\n",
            "\n",
            "🔹 Training Step 1064/1460\n",
            "   input_ids: torch.Size([4, 595])\n",
            "   attention_mask: torch.Size([4, 595])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 595])\n",
            "\n",
            "🔹 Training Step 1065/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1066/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1067/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1068/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 1069/1460\n",
            "   input_ids: torch.Size([4, 595])\n",
            "   attention_mask: torch.Size([4, 595])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 595])\n",
            "\n",
            "🔹 Training Step 1070/1460\n",
            "   input_ids: torch.Size([4, 609])\n",
            "   attention_mask: torch.Size([4, 609])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 609])\n",
            "\n",
            "🔹 Training Step 1071/1460\n",
            "   input_ids: torch.Size([4, 603])\n",
            "   attention_mask: torch.Size([4, 603])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 603])\n",
            "\n",
            "🔹 Training Step 1072/1460\n",
            "   input_ids: torch.Size([4, 603])\n",
            "   attention_mask: torch.Size([4, 603])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 603])\n",
            "\n",
            "🔹 Training Step 1073/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1074/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 1075/1460\n",
            "   input_ids: torch.Size([4, 601])\n",
            "   attention_mask: torch.Size([4, 601])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 601])\n",
            "\n",
            "🔹 Training Step 1076/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 1077/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1078/1460\n",
            "   input_ids: torch.Size([4, 607])\n",
            "   attention_mask: torch.Size([4, 607])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 607])\n",
            "\n",
            "🔹 Training Step 1079/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "🔹 Training Step 1080/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 1081/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1082/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 1083/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 1084/1460\n",
            "   input_ids: torch.Size([4, 604])\n",
            "   attention_mask: torch.Size([4, 604])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 604])\n",
            "\n",
            "🔹 Training Step 1085/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1086/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 1087/1460\n",
            "   input_ids: torch.Size([4, 612])\n",
            "   attention_mask: torch.Size([4, 612])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 612])\n",
            "\n",
            "🔹 Training Step 1088/1460\n",
            "   input_ids: torch.Size([4, 609])\n",
            "   attention_mask: torch.Size([4, 609])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 609])\n",
            "\n",
            "🔹 Training Step 1089/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1090/1460\n",
            "   input_ids: torch.Size([4, 610])\n",
            "   attention_mask: torch.Size([4, 610])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 610])\n",
            "\n",
            "🔹 Training Step 1091/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1092/1460\n",
            "   input_ids: torch.Size([4, 611])\n",
            "   attention_mask: torch.Size([4, 611])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 611])\n",
            "\n",
            "🔹 Training Step 1093/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 1094/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1095/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1096/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1097/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 1098/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "🔹 Training Step 1099/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1100/1460\n",
            "   input_ids: torch.Size([4, 606])\n",
            "   attention_mask: torch.Size([4, 606])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 606])\n",
            "\n",
            "🔹 Training Step 1101/1460\n",
            "   input_ids: torch.Size([4, 607])\n",
            "   attention_mask: torch.Size([4, 607])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 607])\n",
            "\n",
            "🔹 Training Step 1102/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "🔹 Training Step 1103/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 1104/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1105/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 1106/1460\n",
            "   input_ids: torch.Size([4, 595])\n",
            "   attention_mask: torch.Size([4, 595])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 595])\n",
            "\n",
            "🔹 Training Step 1107/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1108/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1109/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "🔹 Training Step 1110/1460\n",
            "   input_ids: torch.Size([4, 609])\n",
            "   attention_mask: torch.Size([4, 609])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 609])\n",
            "\n",
            "🔹 Training Step 1111/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1112/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1113/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1114/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 1115/1460\n",
            "   input_ids: torch.Size([4, 608])\n",
            "   attention_mask: torch.Size([4, 608])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 608])\n",
            "\n",
            "🔹 Training Step 1116/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 1117/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1118/1460\n",
            "   input_ids: torch.Size([4, 612])\n",
            "   attention_mask: torch.Size([4, 612])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 612])\n",
            "\n",
            "🔹 Training Step 1119/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1120/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 1121/1460\n",
            "   input_ids: torch.Size([4, 610])\n",
            "   attention_mask: torch.Size([4, 610])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 610])\n",
            "\n",
            "🔹 Training Step 1122/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 1123/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1124/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1125/1460\n",
            "   input_ids: torch.Size([4, 614])\n",
            "   attention_mask: torch.Size([4, 614])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 614])\n",
            "\n",
            "🔹 Training Step 1126/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 1127/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "🔹 Training Step 1128/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1129/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 1130/1460\n",
            "   input_ids: torch.Size([4, 601])\n",
            "   attention_mask: torch.Size([4, 601])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 601])\n",
            "\n",
            "🔹 Training Step 1131/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 1132/1460\n",
            "   input_ids: torch.Size([4, 601])\n",
            "   attention_mask: torch.Size([4, 601])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 601])\n",
            "\n",
            "🔹 Training Step 1133/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "🔹 Training Step 1134/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 1135/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 1136/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1137/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1138/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1139/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1140/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1141/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1142/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "🔹 Training Step 1143/1460\n",
            "   input_ids: torch.Size([4, 611])\n",
            "   attention_mask: torch.Size([4, 611])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 611])\n",
            "\n",
            "🔹 Training Step 1144/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "🔹 Training Step 1145/1460\n",
            "   input_ids: torch.Size([4, 595])\n",
            "   attention_mask: torch.Size([4, 595])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 595])\n",
            "\n",
            "🔹 Training Step 1146/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "🔹 Training Step 1147/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 1148/1460\n",
            "   input_ids: torch.Size([4, 613])\n",
            "   attention_mask: torch.Size([4, 613])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 613])\n",
            "\n",
            "🔹 Training Step 1149/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "🔹 Training Step 1150/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1151/1460\n",
            "   input_ids: torch.Size([4, 609])\n",
            "   attention_mask: torch.Size([4, 609])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 609])\n",
            "\n",
            "🔹 Training Step 1152/1460\n",
            "   input_ids: torch.Size([4, 606])\n",
            "   attention_mask: torch.Size([4, 606])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 606])\n",
            "\n",
            "🔹 Training Step 1153/1460\n",
            "   input_ids: torch.Size([4, 595])\n",
            "   attention_mask: torch.Size([4, 595])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 595])\n",
            "\n",
            "🔹 Training Step 1154/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 1155/1460\n",
            "   input_ids: torch.Size([4, 601])\n",
            "   attention_mask: torch.Size([4, 601])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 601])\n",
            "\n",
            "🔹 Training Step 1156/1460\n",
            "   input_ids: torch.Size([4, 601])\n",
            "   attention_mask: torch.Size([4, 601])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 601])\n",
            "\n",
            "🔹 Training Step 1157/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1158/1460\n",
            "   input_ids: torch.Size([4, 601])\n",
            "   attention_mask: torch.Size([4, 601])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 601])\n",
            "\n",
            "🔹 Training Step 1159/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1160/1460\n",
            "   input_ids: torch.Size([4, 601])\n",
            "   attention_mask: torch.Size([4, 601])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 601])\n",
            "\n",
            "🔹 Training Step 1161/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 1162/1460\n",
            "   input_ids: torch.Size([4, 608])\n",
            "   attention_mask: torch.Size([4, 608])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 608])\n",
            "\n",
            "🔹 Training Step 1163/1460\n",
            "   input_ids: torch.Size([4, 604])\n",
            "   attention_mask: torch.Size([4, 604])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 604])\n",
            "\n",
            "🔹 Training Step 1164/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1165/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "🔹 Training Step 1166/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 1167/1460\n",
            "   input_ids: torch.Size([4, 610])\n",
            "   attention_mask: torch.Size([4, 610])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 610])\n",
            "\n",
            "🔹 Training Step 1168/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 1169/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1170/1460\n",
            "   input_ids: torch.Size([4, 595])\n",
            "   attention_mask: torch.Size([4, 595])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 595])\n",
            "\n",
            "🔹 Training Step 1171/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1172/1460\n",
            "   input_ids: torch.Size([4, 601])\n",
            "   attention_mask: torch.Size([4, 601])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 601])\n",
            "\n",
            "🔹 Training Step 1173/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 1174/1460\n",
            "   input_ids: torch.Size([4, 607])\n",
            "   attention_mask: torch.Size([4, 607])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 607])\n",
            "\n",
            "🔹 Training Step 1175/1460\n",
            "   input_ids: torch.Size([4, 610])\n",
            "   attention_mask: torch.Size([4, 610])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 610])\n",
            "\n",
            "🔹 Training Step 1176/1460\n",
            "   input_ids: torch.Size([4, 605])\n",
            "   attention_mask: torch.Size([4, 605])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 605])\n",
            "\n",
            "🔹 Training Step 1177/1460\n",
            "   input_ids: torch.Size([4, 605])\n",
            "   attention_mask: torch.Size([4, 605])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 605])\n",
            "\n",
            "🔹 Training Step 1178/1460\n",
            "   input_ids: torch.Size([4, 602])\n",
            "   attention_mask: torch.Size([4, 602])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 602])\n",
            "\n",
            "🔹 Training Step 1179/1460\n",
            "   input_ids: torch.Size([4, 601])\n",
            "   attention_mask: torch.Size([4, 601])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 601])\n",
            "\n",
            "🔹 Training Step 1180/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 1181/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 1182/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1183/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1184/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1185/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 1186/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1187/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 1188/1460\n",
            "   input_ids: torch.Size([4, 614])\n",
            "   attention_mask: torch.Size([4, 614])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 614])\n",
            "\n",
            "🔹 Training Step 1189/1460\n",
            "   input_ids: torch.Size([4, 607])\n",
            "   attention_mask: torch.Size([4, 607])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 607])\n",
            "\n",
            "🔹 Training Step 1190/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1191/1460\n",
            "   input_ids: torch.Size([4, 607])\n",
            "   attention_mask: torch.Size([4, 607])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 607])\n",
            "\n",
            "🔹 Training Step 1192/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1193/1460\n",
            "   input_ids: torch.Size([4, 595])\n",
            "   attention_mask: torch.Size([4, 595])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 595])\n",
            "\n",
            "🔹 Training Step 1194/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1195/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1196/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1197/1460\n",
            "   input_ids: torch.Size([4, 609])\n",
            "   attention_mask: torch.Size([4, 609])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 609])\n",
            "\n",
            "🔹 Training Step 1198/1460\n",
            "   input_ids: torch.Size([4, 611])\n",
            "   attention_mask: torch.Size([4, 611])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 611])\n",
            "\n",
            "🔹 Training Step 1199/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 1200/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 1201/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "🔹 Training Step 1202/1460\n",
            "   input_ids: torch.Size([4, 601])\n",
            "   attention_mask: torch.Size([4, 601])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 601])\n",
            "\n",
            "🔹 Training Step 1203/1460\n",
            "   input_ids: torch.Size([4, 601])\n",
            "   attention_mask: torch.Size([4, 601])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 601])\n",
            "\n",
            "🔹 Training Step 1204/1460\n",
            "   input_ids: torch.Size([4, 602])\n",
            "   attention_mask: torch.Size([4, 602])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 602])\n",
            "\n",
            "🔹 Training Step 1205/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 1206/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1207/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 1208/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 1209/1460\n",
            "   input_ids: torch.Size([4, 595])\n",
            "   attention_mask: torch.Size([4, 595])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 595])\n",
            "\n",
            "🔹 Training Step 1210/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "🔹 Training Step 1211/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 1212/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 1213/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1214/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 1215/1460\n",
            "   input_ids: torch.Size([4, 607])\n",
            "   attention_mask: torch.Size([4, 607])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 607])\n",
            "\n",
            "🔹 Training Step 1216/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1217/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "🔹 Training Step 1218/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1219/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "🔹 Training Step 1220/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1221/1460\n",
            "   input_ids: torch.Size([4, 608])\n",
            "   attention_mask: torch.Size([4, 608])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 608])\n",
            "\n",
            "🔹 Training Step 1222/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 1223/1460\n",
            "   input_ids: torch.Size([4, 604])\n",
            "   attention_mask: torch.Size([4, 604])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 604])\n",
            "\n",
            "🔹 Training Step 1224/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1225/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 1226/1460\n",
            "   input_ids: torch.Size([4, 595])\n",
            "   attention_mask: torch.Size([4, 595])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 595])\n",
            "\n",
            "🔹 Training Step 1227/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 1228/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1229/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 1230/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 1231/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1232/1460\n",
            "   input_ids: torch.Size([4, 609])\n",
            "   attention_mask: torch.Size([4, 609])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 609])\n",
            "\n",
            "🔹 Training Step 1233/1460\n",
            "   input_ids: torch.Size([4, 610])\n",
            "   attention_mask: torch.Size([4, 610])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 610])\n",
            "\n",
            "🔹 Training Step 1234/1460\n",
            "   input_ids: torch.Size([4, 595])\n",
            "   attention_mask: torch.Size([4, 595])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 595])\n",
            "\n",
            "🔹 Training Step 1235/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1236/1460\n",
            "   input_ids: torch.Size([4, 616])\n",
            "   attention_mask: torch.Size([4, 616])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 616])\n",
            "\n",
            "🔹 Training Step 1237/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1238/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1239/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 1240/1460\n",
            "   input_ids: torch.Size([4, 607])\n",
            "   attention_mask: torch.Size([4, 607])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 607])\n",
            "\n",
            "🔹 Training Step 1241/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 1242/1460\n",
            "   input_ids: torch.Size([4, 605])\n",
            "   attention_mask: torch.Size([4, 605])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 605])\n",
            "\n",
            "🔹 Training Step 1243/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 1244/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 1245/1460\n",
            "   input_ids: torch.Size([4, 612])\n",
            "   attention_mask: torch.Size([4, 612])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 612])\n",
            "\n",
            "🔹 Training Step 1246/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1247/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "🔹 Training Step 1248/1460\n",
            "   input_ids: torch.Size([4, 617])\n",
            "   attention_mask: torch.Size([4, 617])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 617])\n",
            "\n",
            "🔹 Training Step 1249/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1250/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1251/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 1252/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1253/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1254/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 1255/1460\n",
            "   input_ids: torch.Size([4, 611])\n",
            "   attention_mask: torch.Size([4, 611])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 611])\n",
            "\n",
            "🔹 Training Step 1256/1460\n",
            "   input_ids: torch.Size([4, 608])\n",
            "   attention_mask: torch.Size([4, 608])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 608])\n",
            "\n",
            "🔹 Training Step 1257/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 1258/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1259/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 1260/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 1261/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1262/1460\n",
            "   input_ids: torch.Size([4, 613])\n",
            "   attention_mask: torch.Size([4, 613])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 613])\n",
            "\n",
            "🔹 Training Step 1263/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "🔹 Training Step 1264/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1265/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1266/1460\n",
            "   input_ids: torch.Size([4, 610])\n",
            "   attention_mask: torch.Size([4, 610])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 610])\n",
            "\n",
            "🔹 Training Step 1267/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "🔹 Training Step 1268/1460\n",
            "   input_ids: torch.Size([4, 614])\n",
            "   attention_mask: torch.Size([4, 614])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 614])\n",
            "\n",
            "🔹 Training Step 1269/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "🔹 Training Step 1270/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1271/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "🔹 Training Step 1272/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 1273/1460\n",
            "   input_ids: torch.Size([4, 606])\n",
            "   attention_mask: torch.Size([4, 606])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 606])\n",
            "\n",
            "🔹 Training Step 1274/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 1275/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 1276/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1277/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "🔹 Training Step 1278/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 1279/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1280/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1281/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1282/1460\n",
            "   input_ids: torch.Size([4, 616])\n",
            "   attention_mask: torch.Size([4, 616])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 616])\n",
            "\n",
            "🔹 Training Step 1283/1460\n",
            "   input_ids: torch.Size([4, 604])\n",
            "   attention_mask: torch.Size([4, 604])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 604])\n",
            "\n",
            "🔹 Training Step 1284/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1285/1460\n",
            "   input_ids: torch.Size([4, 604])\n",
            "   attention_mask: torch.Size([4, 604])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 604])\n",
            "\n",
            "🔹 Training Step 1286/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1287/1460\n",
            "   input_ids: torch.Size([4, 609])\n",
            "   attention_mask: torch.Size([4, 609])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 609])\n",
            "\n",
            "🔹 Training Step 1288/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 1289/1460\n",
            "   input_ids: torch.Size([4, 611])\n",
            "   attention_mask: torch.Size([4, 611])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 611])\n",
            "\n",
            "🔹 Training Step 1290/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1291/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 1292/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1293/1460\n",
            "   input_ids: torch.Size([4, 595])\n",
            "   attention_mask: torch.Size([4, 595])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 595])\n",
            "\n",
            "🔹 Training Step 1294/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 1295/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1296/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "🔹 Training Step 1297/1460\n",
            "   input_ids: torch.Size([4, 608])\n",
            "   attention_mask: torch.Size([4, 608])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 608])\n",
            "\n",
            "🔹 Training Step 1298/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1299/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 1300/1460\n",
            "   input_ids: torch.Size([4, 610])\n",
            "   attention_mask: torch.Size([4, 610])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 610])\n",
            "\n",
            "🔹 Training Step 1301/1460\n",
            "   input_ids: torch.Size([4, 601])\n",
            "   attention_mask: torch.Size([4, 601])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 601])\n",
            "\n",
            "🔹 Training Step 1302/1460\n",
            "   input_ids: torch.Size([4, 609])\n",
            "   attention_mask: torch.Size([4, 609])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 609])\n",
            "\n",
            "🔹 Training Step 1303/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 1304/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "🔹 Training Step 1305/1460\n",
            "   input_ids: torch.Size([4, 604])\n",
            "   attention_mask: torch.Size([4, 604])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 604])\n",
            "\n",
            "🔹 Training Step 1306/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 1307/1460\n",
            "   input_ids: torch.Size([4, 606])\n",
            "   attention_mask: torch.Size([4, 606])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 606])\n",
            "\n",
            "🔹 Training Step 1308/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1309/1460\n",
            "   input_ids: torch.Size([4, 608])\n",
            "   attention_mask: torch.Size([4, 608])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 608])\n",
            "\n",
            "🔹 Training Step 1310/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1311/1460\n",
            "   input_ids: torch.Size([4, 611])\n",
            "   attention_mask: torch.Size([4, 611])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 611])\n",
            "\n",
            "🔹 Training Step 1312/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 1313/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1314/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 1315/1460\n",
            "   input_ids: torch.Size([4, 595])\n",
            "   attention_mask: torch.Size([4, 595])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 595])\n",
            "\n",
            "🔹 Training Step 1316/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1317/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "🔹 Training Step 1318/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "🔹 Training Step 1319/1460\n",
            "   input_ids: torch.Size([4, 595])\n",
            "   attention_mask: torch.Size([4, 595])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 595])\n",
            "\n",
            "🔹 Training Step 1320/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "🔹 Training Step 1321/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1322/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 1323/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 1324/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 1325/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1326/1460\n",
            "   input_ids: torch.Size([4, 604])\n",
            "   attention_mask: torch.Size([4, 604])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 604])\n",
            "\n",
            "🔹 Training Step 1327/1460\n",
            "   input_ids: torch.Size([4, 610])\n",
            "   attention_mask: torch.Size([4, 610])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 610])\n",
            "\n",
            "🔹 Training Step 1328/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 1329/1460\n",
            "   input_ids: torch.Size([4, 602])\n",
            "   attention_mask: torch.Size([4, 602])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 602])\n",
            "\n",
            "🔹 Training Step 1330/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 1331/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 1332/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1333/1460\n",
            "   input_ids: torch.Size([4, 610])\n",
            "   attention_mask: torch.Size([4, 610])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 610])\n",
            "\n",
            "🔹 Training Step 1334/1460\n",
            "   input_ids: torch.Size([4, 608])\n",
            "   attention_mask: torch.Size([4, 608])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 608])\n",
            "\n",
            "🔹 Training Step 1335/1460\n",
            "   input_ids: torch.Size([4, 612])\n",
            "   attention_mask: torch.Size([4, 612])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 612])\n",
            "\n",
            "🔹 Training Step 1336/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1337/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1338/1460\n",
            "   input_ids: torch.Size([4, 614])\n",
            "   attention_mask: torch.Size([4, 614])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 614])\n",
            "\n",
            "🔹 Training Step 1339/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1340/1460\n",
            "   input_ids: torch.Size([4, 608])\n",
            "   attention_mask: torch.Size([4, 608])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 608])\n",
            "\n",
            "🔹 Training Step 1341/1460\n",
            "   input_ids: torch.Size([4, 611])\n",
            "   attention_mask: torch.Size([4, 611])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 611])\n",
            "\n",
            "🔹 Training Step 1342/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1343/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1344/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1345/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1346/1460\n",
            "   input_ids: torch.Size([4, 608])\n",
            "   attention_mask: torch.Size([4, 608])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 608])\n",
            "\n",
            "🔹 Training Step 1347/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1348/1460\n",
            "   input_ids: torch.Size([4, 613])\n",
            "   attention_mask: torch.Size([4, 613])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 613])\n",
            "\n",
            "🔹 Training Step 1349/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1350/1460\n",
            "   input_ids: torch.Size([4, 612])\n",
            "   attention_mask: torch.Size([4, 612])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 612])\n",
            "\n",
            "🔹 Training Step 1351/1460\n",
            "   input_ids: torch.Size([4, 608])\n",
            "   attention_mask: torch.Size([4, 608])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 608])\n",
            "\n",
            "🔹 Training Step 1352/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1353/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1354/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 1355/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 1356/1460\n",
            "   input_ids: torch.Size([4, 609])\n",
            "   attention_mask: torch.Size([4, 609])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 609])\n",
            "\n",
            "🔹 Training Step 1357/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 1358/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1359/1460\n",
            "   input_ids: torch.Size([4, 601])\n",
            "   attention_mask: torch.Size([4, 601])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 601])\n",
            "\n",
            "🔹 Training Step 1360/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1361/1460\n",
            "   input_ids: torch.Size([4, 618])\n",
            "   attention_mask: torch.Size([4, 618])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 618])\n",
            "\n",
            "🔹 Training Step 1362/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1363/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 1364/1460\n",
            "   input_ids: torch.Size([4, 605])\n",
            "   attention_mask: torch.Size([4, 605])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 605])\n",
            "\n",
            "🔹 Training Step 1365/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1366/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1367/1460\n",
            "   input_ids: torch.Size([4, 595])\n",
            "   attention_mask: torch.Size([4, 595])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 595])\n",
            "\n",
            "🔹 Training Step 1368/1460\n",
            "   input_ids: torch.Size([4, 607])\n",
            "   attention_mask: torch.Size([4, 607])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 607])\n",
            "\n",
            "🔹 Training Step 1369/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1370/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1371/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1372/1460\n",
            "   input_ids: torch.Size([4, 608])\n",
            "   attention_mask: torch.Size([4, 608])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 608])\n",
            "\n",
            "🔹 Training Step 1373/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1374/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 1375/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1376/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1377/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1378/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "🔹 Training Step 1379/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1380/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 1381/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 1382/1460\n",
            "   input_ids: torch.Size([4, 609])\n",
            "   attention_mask: torch.Size([4, 609])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 609])\n",
            "\n",
            "🔹 Training Step 1383/1460\n",
            "   input_ids: torch.Size([4, 603])\n",
            "   attention_mask: torch.Size([4, 603])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 603])\n",
            "\n",
            "🔹 Training Step 1384/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1385/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1386/1460\n",
            "   input_ids: torch.Size([4, 601])\n",
            "   attention_mask: torch.Size([4, 601])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 601])\n",
            "\n",
            "🔹 Training Step 1387/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "🔹 Training Step 1388/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1389/1460\n",
            "   input_ids: torch.Size([4, 610])\n",
            "   attention_mask: torch.Size([4, 610])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 610])\n",
            "\n",
            "🔹 Training Step 1390/1460\n",
            "   input_ids: torch.Size([4, 610])\n",
            "   attention_mask: torch.Size([4, 610])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 610])\n",
            "\n",
            "🔹 Training Step 1391/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 1392/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1393/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 1394/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1395/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 1396/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 1397/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "🔹 Training Step 1398/1460\n",
            "   input_ids: torch.Size([4, 602])\n",
            "   attention_mask: torch.Size([4, 602])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 602])\n",
            "\n",
            "🔹 Training Step 1399/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1400/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "🔹 Training Step 1401/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1402/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 1403/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1404/1460\n",
            "   input_ids: torch.Size([4, 595])\n",
            "   attention_mask: torch.Size([4, 595])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 595])\n",
            "\n",
            "🔹 Training Step 1405/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 1406/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 1407/1460\n",
            "   input_ids: torch.Size([4, 601])\n",
            "   attention_mask: torch.Size([4, 601])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 601])\n",
            "\n",
            "🔹 Training Step 1408/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1409/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 1410/1460\n",
            "   input_ids: torch.Size([4, 609])\n",
            "   attention_mask: torch.Size([4, 609])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 609])\n",
            "\n",
            "🔹 Training Step 1411/1460\n",
            "   input_ids: torch.Size([4, 601])\n",
            "   attention_mask: torch.Size([4, 601])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 601])\n",
            "\n",
            "🔹 Training Step 1412/1460\n",
            "   input_ids: torch.Size([4, 594])\n",
            "   attention_mask: torch.Size([4, 594])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 594])\n",
            "\n",
            "🔹 Training Step 1413/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "🔹 Training Step 1414/1460\n",
            "   input_ids: torch.Size([4, 610])\n",
            "   attention_mask: torch.Size([4, 610])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 610])\n",
            "\n",
            "🔹 Training Step 1415/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1416/1460\n",
            "   input_ids: torch.Size([4, 603])\n",
            "   attention_mask: torch.Size([4, 603])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 603])\n",
            "\n",
            "🔹 Training Step 1417/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1418/1460\n",
            "   input_ids: torch.Size([4, 601])\n",
            "   attention_mask: torch.Size([4, 601])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 601])\n",
            "\n",
            "🔹 Training Step 1419/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1420/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1421/1460\n",
            "   input_ids: torch.Size([4, 594])\n",
            "   attention_mask: torch.Size([4, 594])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 594])\n",
            "\n",
            "🔹 Training Step 1422/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1423/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1424/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1425/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 1426/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 1427/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1428/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 1429/1460\n",
            "   input_ids: torch.Size([4, 605])\n",
            "   attention_mask: torch.Size([4, 605])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 605])\n",
            "\n",
            "🔹 Training Step 1430/1460\n",
            "   input_ids: torch.Size([4, 603])\n",
            "   attention_mask: torch.Size([4, 603])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 603])\n",
            "\n",
            "🔹 Training Step 1431/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "🔹 Training Step 1432/1460\n",
            "   input_ids: torch.Size([4, 611])\n",
            "   attention_mask: torch.Size([4, 611])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 611])\n",
            "\n",
            "🔹 Training Step 1433/1460\n",
            "   input_ids: torch.Size([4, 594])\n",
            "   attention_mask: torch.Size([4, 594])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 594])\n",
            "\n",
            "🔹 Training Step 1434/1460\n",
            "   input_ids: torch.Size([4, 607])\n",
            "   attention_mask: torch.Size([4, 607])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 607])\n",
            "\n",
            "🔹 Training Step 1435/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1436/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1437/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 1438/1460\n",
            "   input_ids: torch.Size([4, 607])\n",
            "   attention_mask: torch.Size([4, 607])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 607])\n",
            "\n",
            "🔹 Training Step 1439/1460\n",
            "   input_ids: torch.Size([4, 611])\n",
            "   attention_mask: torch.Size([4, 611])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 611])\n",
            "\n",
            "🔹 Training Step 1440/1460\n",
            "   input_ids: torch.Size([4, 600])\n",
            "   attention_mask: torch.Size([4, 600])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 600])\n",
            "\n",
            "🔹 Training Step 1441/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 1442/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1443/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1444/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1445/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 1446/1460\n",
            "   input_ids: torch.Size([4, 609])\n",
            "   attention_mask: torch.Size([4, 609])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 609])\n",
            "\n",
            "🔹 Training Step 1447/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 1448/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1449/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 1450/1460\n",
            "   input_ids: torch.Size([4, 595])\n",
            "   attention_mask: torch.Size([4, 595])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 595])\n",
            "\n",
            "🔹 Training Step 1451/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1452/1460\n",
            "   input_ids: torch.Size([4, 598])\n",
            "   attention_mask: torch.Size([4, 598])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 598])\n",
            "\n",
            "🔹 Training Step 1453/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 1454/1460\n",
            "   input_ids: torch.Size([4, 611])\n",
            "   attention_mask: torch.Size([4, 611])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 611])\n",
            "\n",
            "🔹 Training Step 1455/1460\n",
            "   input_ids: torch.Size([4, 597])\n",
            "   attention_mask: torch.Size([4, 597])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 597])\n",
            "\n",
            "🔹 Training Step 1456/1460\n",
            "   input_ids: torch.Size([4, 596])\n",
            "   attention_mask: torch.Size([4, 596])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 596])\n",
            "\n",
            "🔹 Training Step 1457/1460\n",
            "   input_ids: torch.Size([4, 599])\n",
            "   attention_mask: torch.Size([4, 599])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 599])\n",
            "\n",
            "🔹 Training Step 1458/1460\n",
            "   input_ids: torch.Size([4, 595])\n",
            "   attention_mask: torch.Size([4, 595])\n",
            "   pixel_values: torch.Size([4, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([4, 3])\n",
            "   labels: torch.Size([4, 595])\n",
            "\n",
            "🔹 Training Step 1459/1460\n",
            "   input_ids: torch.Size([2, 595])\n",
            "   attention_mask: torch.Size([2, 595])\n",
            "   pixel_values: torch.Size([2, 3, 336, 336])\n",
            "   image_grid_thw: torch.Size([2, 3])\n",
            "   labels: torch.Size([2, 595])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from transformers import AutoProcessor\n",
        "\n",
        "# Define dataset path\n",
        "image_folder = \"/content/Compliance_model_data\"\n",
        "batch_size = 4\n",
        "max_length = 512\n",
        "image_size = (336, 336)\n",
        "\n",
        "# Initialize full dataset\n",
        "full_dataset = LLaVADataset(image_folder, processor, max_length=max_length, image_size=image_size)\n",
        "\n",
        "# **Split into Train and Validation Sets (80% Train, 20% Validation)**\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_dataset, eval_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# Initialize DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, pin_memory=False)\n",
        "eval_loader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, pin_memory=False)\n",
        "\n",
        "# Verify batch structure (Debugging)\n",
        "for step, batch in enumerate(train_loader):\n",
        "    if batch is None:\n",
        "        print(f\"⚠️ Skipping empty batch at step {step}\")\n",
        "        continue\n",
        "\n",
        "    print(f\"\\n🔹 Training Step {step}/{len(train_loader)}\")\n",
        "    for key, value in batch.items():\n",
        "        print(f\"   {key}: {value.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "J52VNSx4-Kaq",
        "outputId": "9d706759-23c8-47ba-f5b1-fdb024d317d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔍 **Dataset Sample Keys**: dict_keys(['input_ids', 'attention_mask', 'pixel_values', 'image_grid_thw', 'labels'])\n",
            "🔹 input_ids: <class 'torch.Tensor'>, Shape: torch.Size([595])\n",
            "🔹 attention_mask: <class 'torch.Tensor'>, Shape: torch.Size([595])\n",
            "🔹 pixel_values: <class 'torch.Tensor'>, Shape: torch.Size([3, 336, 336])\n",
            "🔹 image_grid_thw: <class 'torch.Tensor'>, Shape: torch.Size([3])\n",
            "🔹 labels: <class 'torch.Tensor'>, Shape: torch.Size([595])\n"
          ]
        }
      ],
      "source": [
        "# Get a sample from the dataset\n",
        "sample = train_dataset[99]\n",
        "\n",
        "# Print available keys in the dataset sample\n",
        "print(\"🔍 **Dataset Sample Keys**:\", sample.keys())\n",
        "\n",
        "# Print the shape and type of each key\n",
        "for key, value in sample.items():\n",
        "    print(f\"🔹 {key}: {type(value)}, Shape: {value.shape if isinstance(value, torch.Tensor) else 'N/A'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgU2Z64m6mrY",
        "outputId": "00f3353a-9ba5-453b-b842-c078e4e5cd87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✅ Model and processor loaded successfully on GPU with LoRA!\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoProcessor, LlavaForConditionalGeneration, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "# Enable memory optimization\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=4, #previously its 8\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.enable_input_require_grads()\n",
        "model.config.use_cache = False  # Important for gradient checkpointing\n",
        "\n",
        "print(f\"\\n✅ Model and processor loaded successfully on GPU with LoRA!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQZT3sfj_MVG"
      },
      "outputs": [],
      "source": [
        "# for name, param in model.named_parameters():\n",
        "#     if param.requires_grad:\n",
        "#         print(name)\n",
        "\n",
        "# for name, param in model.named_parameters():\n",
        "#     print(f\"{name}: requires_grad = {param.requires_grad}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruzaqzzWK0Z8"
      },
      "source": [
        "#free cuda before training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egQvLgLzK3KI",
        "outputId": "f793b4ca-f48b-4980-fb95-2400e42e3fa4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "80"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gc\n",
        "import torch\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnCbktK4BLx2"
      },
      "source": [
        "#Original training code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "KM3ZvqlD7F5n",
        "outputId": "7e8d3771-932b-422e-86fe-d71ca5cb0d70"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-15-7083241d0146>:100: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10/10 03:34, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>76.865400</td>\n",
              "      <td>9.517768</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model and necessary components saved at /content/Fine_tuned_llava_model\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    EarlyStoppingCallback,\n",
        ")\n",
        "\n",
        "# 1) Define TrainingArguments with Early Stopping\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/Fine_tuned_llava_model\",\n",
        "    per_device_train_batch_size=1,  # Adjust based on available GPU memory\n",
        "    gradient_accumulation_steps=8,\n",
        "    max_grad_norm=1.0,\n",
        "    gradient_checkpointing=True,\n",
        "    logging_strategy=\"steps\",\n",
        "    max_steps=10,\n",
        "    logging_steps=10,  # Logs every 10 steps\n",
        "    evaluation_strategy=\"epoch\",  # Evaluate at the end of each epoch\n",
        "    save_strategy=\"epoch\",\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    fp16=True,  # Enables mixed precision for speed\n",
        "    bf16=False,\n",
        "    push_to_hub=False,\n",
        "    logging_dir=\"/content/logs\",\n",
        "    remove_unused_columns=False,\n",
        "    label_names=[\"labels\"],\n",
        "    dataloader_num_workers=0,\n",
        "    save_total_limit=1,  # Keeps only the last 2 checkpoints\n",
        "    load_best_model_at_end=True,  # Loads best model after training\n",
        "    metric_for_best_model=\"loss\"\n",
        ")\n",
        "\n",
        "# 2) Create a Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=processor.tokenizer,\n",
        "    data_collator=collate_fn,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
        ")\n",
        "\n",
        "# 3) Train model\n",
        "trainer.train()\n",
        "\n",
        "# 4) Save all components for reloading\n",
        "save_path = \"/content/Fine_tuned_llava_model\"\n",
        "trainer.save_model(save_path)\n",
        "processor.tokenizer.save_pretrained(save_path)\n",
        "processor.save_pretrained(save_path)\n",
        "torch.save(training_args, f\"{save_path}/training_args.bin\")\n",
        "torch.save(trainer.state.optimizer.state_dict(), f\"{save_path}/optimizer_state.pt\")\n",
        "\n",
        "print(f\"Model and necessary components saved at {save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "cbb0310da632478a9dad8cdeafe909fe",
            "2f19f4f8835a4b3b98c546875d1b6d39",
            "8c86469fb635495bb3d157dfb67616e6",
            "bd0db7d34f334506833116528043d6a5",
            "1d360b6ed5394177bfd89c7805e1a9d3",
            "ac21d37ec70548e385d1b6466e532dee",
            "7237fb6d1ff049fca43045a0a6b3c496",
            "374236e0ef7d4e9888ed3e60972fded7",
            "c30f654164c0437f993d7ea2bbaa77fa",
            "c665d8ec1c4d490488696f2348e6faf8",
            "cf91abe9cb2b482e86a2c494d08eb82d"
          ]
        },
        "id": "pUFSz7_dPK7K",
        "outputId": "e592a0de-4986-43ac-de49-31b32f9bb8e5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cbb0310da632478a9dad8cdeafe909fe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import LlavaForConditionalGeneration, AutoProcessor\n",
        "\n",
        "# Define the path where the fine-tuned model is saved\n",
        "save_path = \"/content/Fine_tuned_llava_model\"\n",
        "\n",
        "# Load the fine-tuned LLaVA-13B model\n",
        "model_reloaded = LlavaForConditionalGeneration.from_pretrained(save_path)\n",
        "processor_reloaded = AutoProcessor.from_pretrained(save_path)\n",
        "tokenizer_reloaded = processor_reloaded.tokenizer  # Tokenizer is part of the processor\n",
        "\n",
        "print(\"Fine-tuned LLaVA-13B model reloaded successfully!\")\n",
        "\n",
        "# ---- Load the Image ----\n",
        "image_path = \"/content/violant_image.jpg\"  # Change this to your actual image file path\n",
        "image = Image.open(image_path).convert(\"RGB\")  # Convert to RGB format\n",
        "print(\"Image:\",image)\n",
        "\n",
        "\n",
        "# ---- Process the Image ----\n",
        "inputs = processor_reloaded(image, return_tensors=\"pt\")\n",
        "\n",
        "# Move to GPU if available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_reloaded.to(device)\n",
        "inputs = {key: val.to(device) for key, val in inputs.items()}\n",
        "\n",
        "# ---- Generate Description ----\n",
        "with torch.no_grad():\n",
        "    generated_ids = model_reloaded.generate(**inputs, max_length=50)\n",
        "\n",
        "# Decode the generated output\n",
        "generated_text = tokenizer_reloaded.decode(generated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Generated Description:\", generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVQjH1N0i3OD",
        "outputId": "1c391c0a-0681-4f07-8d22-69f16de1476f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It is Returning\n",
            "\n",
            "🔍 Extracted Required Image Tokens: 577\n",
            "577\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "def extract_required_image_tokens(model, dataset, device=\"cuda\"):\n",
        "    \"\"\"Extract image features from the model to determine required `<image>` tokens.\"\"\"\n",
        "\n",
        "    # Get a single sample from the dataset\n",
        "    sample = dataset[0]\n",
        "    pixel_values = sample[\"pixel_values\"].unsqueeze(0).to(device)  # Add batch dimension\n",
        "\n",
        "    # Ensure no text input is given\n",
        "    with torch.no_grad():\n",
        "        image_features = model.vision_tower(pixel_values)\n",
        "\n",
        "    # Handle the output structure of vision_tower\n",
        "    if isinstance(image_features, torch.Tensor):\n",
        "        num_image_features = image_features.shape[1] * image_features.shape[2]\n",
        "    elif hasattr(image_features, \"last_hidden_state\"):\n",
        "        num_image_features = image_features.last_hidden_state.shape[1]\n",
        "    else:\n",
        "        raise ValueError(\"❌ Could not extract valid image features. Debug model output.\")\n",
        "\n",
        "    print(f\"\\n🔍 Extracted Required Image Tokens: {num_image_features}\")\n",
        "    return num_image_features\n",
        "\n",
        "# Run feature extraction\n",
        "num_required_image_tokens = extract_required_image_tokens(model, dataset)\n",
        "print(num_required_image_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "1b053694608b467882f797ce1332da34",
            "48a12a36c77b47b3a962fb7e3f0eede5",
            "db4afb94aa304bf197ceb9518fc175e5",
            "124acec5653c40e89b4687bc4d6caa5f",
            "af421f24a5a84bb082a3f2e2d02d91b6",
            "94a010bf063146b3b11e1f03daab5d70",
            "f02e47abf8514c6781562c6e1c335525",
            "74c89dbfa8254809be3ae0119ee4dad7",
            "097ae431e07946b98dce00faffb3ec9a",
            "fd479b74ae6e4bcd8cdd2e3cd83be7f9",
            "b02a1abae8cf44b6ac907d2bed3d468c"
          ]
        },
        "id": "hgeFJRQf4xGA",
        "outputId": "aa79a746-e8f2-4e3a-d507-928b03fd863d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1b053694608b467882f797ce1332da34",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   0%|          | 0/1825 [00:00<?, ?it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   0%|          | 1/1825 [00:00<06:57,  4.37it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   0%|          | 3/1825 [00:00<03:18,  9.17it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   0%|          | 5/1825 [00:00<02:39, 11.43it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   0%|          | 7/1825 [00:00<02:21, 12.88it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   0%|          | 9/1825 [00:00<02:12, 13.71it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   1%|          | 11/1825 [00:00<02:13, 13.62it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   1%|          | 13/1825 [00:01<02:12, 13.72it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   1%|          | 15/1825 [00:01<02:08, 14.09it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   1%|          | 17/1825 [00:01<02:09, 13.93it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   1%|          | 19/1825 [00:01<02:05, 14.42it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   1%|          | 21/1825 [00:01<02:02, 14.75it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   1%|▏         | 23/1825 [00:01<02:00, 14.89it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   1%|▏         | 25/1825 [00:01<01:59, 15.05it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   1%|▏         | 27/1825 [00:01<01:58, 15.14it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   2%|▏         | 29/1825 [00:02<01:58, 15.22it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   2%|▏         | 31/1825 [00:02<01:58, 15.16it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   2%|▏         | 33/1825 [00:02<02:01, 14.76it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   2%|▏         | 35/1825 [00:02<02:00, 14.90it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   2%|▏         | 37/1825 [00:02<01:59, 14.99it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   2%|▏         | 39/1825 [00:02<01:59, 15.00it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   2%|▏         | 41/1825 [00:02<02:00, 14.76it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   2%|▏         | 43/1825 [00:03<02:00, 14.80it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   2%|▏         | 45/1825 [00:03<01:59, 14.94it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   3%|▎         | 47/1825 [00:03<02:00, 14.79it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   3%|▎         | 49/1825 [00:03<02:00, 14.80it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   3%|▎         | 51/1825 [00:03<01:58, 14.94it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   3%|▎         | 53/1825 [00:03<01:58, 14.97it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   3%|▎         | 55/1825 [00:03<01:58, 14.88it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   3%|▎         | 57/1825 [00:04<02:02, 14.46it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   3%|▎         | 59/1825 [00:04<02:02, 14.43it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   3%|▎         | 61/1825 [00:04<01:59, 14.79it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   3%|▎         | 63/1825 [00:04<02:01, 14.47it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   4%|▎         | 65/1825 [00:04<02:07, 13.78it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   4%|▎         | 67/1825 [00:04<02:11, 13.38it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   4%|▍         | 69/1825 [00:04<02:11, 13.35it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   4%|▍         | 71/1825 [00:05<02:12, 13.26it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   4%|▍         | 73/1825 [00:05<02:08, 13.65it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   4%|▍         | 75/1825 [00:05<02:05, 13.96it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   4%|▍         | 77/1825 [00:05<02:03, 14.13it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   4%|▍         | 79/1825 [00:05<02:06, 13.82it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   4%|▍         | 81/1825 [00:05<02:03, 14.07it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   5%|▍         | 83/1825 [00:05<02:05, 13.85it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   5%|▍         | 85/1825 [00:06<02:04, 14.02it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   5%|▍         | 87/1825 [00:06<02:04, 13.95it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   5%|▍         | 89/1825 [00:06<02:04, 13.90it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   5%|▍         | 91/1825 [00:06<02:04, 13.90it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   5%|▌         | 93/1825 [00:06<02:02, 14.15it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   5%|▌         | 95/1825 [00:06<02:04, 13.85it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   5%|▌         | 97/1825 [00:06<02:06, 13.66it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   5%|▌         | 99/1825 [00:07<02:06, 13.67it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   6%|▌         | 101/1825 [00:07<02:05, 13.71it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   6%|▌         | 103/1825 [00:07<02:03, 13.96it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   6%|▌         | 105/1825 [00:07<02:02, 14.03it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   6%|▌         | 107/1825 [00:07<02:01, 14.13it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   6%|▌         | 109/1825 [00:07<02:01, 14.18it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   6%|▌         | 111/1825 [00:07<02:01, 14.12it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   6%|▌         | 113/1825 [00:08<02:03, 13.89it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   6%|▋         | 115/1825 [00:08<01:59, 14.34it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   6%|▋         | 117/1825 [00:08<01:58, 14.43it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   7%|▋         | 119/1825 [00:08<01:57, 14.49it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   7%|▋         | 121/1825 [00:08<02:01, 14.07it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   7%|▋         | 123/1825 [00:08<02:00, 14.09it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   7%|▋         | 125/1825 [00:08<02:03, 13.78it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   7%|▋         | 127/1825 [00:09<02:05, 13.56it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   7%|▋         | 129/1825 [00:09<02:03, 13.77it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   7%|▋         | 131/1825 [00:09<02:02, 13.86it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   7%|▋         | 133/1825 [00:09<01:58, 14.28it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   7%|▋         | 135/1825 [00:09<01:55, 14.68it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   8%|▊         | 137/1825 [00:09<01:52, 15.00it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   8%|▊         | 139/1825 [00:09<01:51, 15.12it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   8%|▊         | 141/1825 [00:09<01:50, 15.26it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   8%|▊         | 143/1825 [00:10<01:53, 14.84it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   8%|▊         | 145/1825 [00:10<01:52, 14.91it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   8%|▊         | 147/1825 [00:10<01:52, 14.87it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   8%|▊         | 149/1825 [00:10<01:52, 14.95it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   8%|▊         | 151/1825 [00:10<01:51, 15.06it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   8%|▊         | 153/1825 [00:10<01:50, 15.17it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   8%|▊         | 155/1825 [00:10<01:49, 15.26it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   9%|▊         | 157/1825 [00:11<01:51, 15.00it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   9%|▊         | 159/1825 [00:11<01:53, 14.71it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   9%|▉         | 161/1825 [00:11<01:52, 14.81it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   9%|▉         | 163/1825 [00:11<01:54, 14.57it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   9%|▉         | 165/1825 [00:11<01:57, 14.08it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   9%|▉         | 167/1825 [00:11<01:57, 14.11it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   9%|▉         | 169/1825 [00:11<01:55, 14.40it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   9%|▉         | 171/1825 [00:12<01:53, 14.57it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:   9%|▉         | 173/1825 [00:12<01:51, 14.78it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  10%|▉         | 175/1825 [00:12<01:53, 14.49it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  10%|▉         | 177/1825 [00:12<01:52, 14.71it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  10%|▉         | 179/1825 [00:12<01:51, 14.76it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  10%|▉         | 181/1825 [00:12<01:50, 14.87it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  10%|█         | 183/1825 [00:12<01:49, 14.99it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  10%|█         | 185/1825 [00:12<01:48, 15.13it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  10%|█         | 187/1825 [00:13<01:47, 15.20it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  10%|█         | 189/1825 [00:13<01:50, 14.75it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  10%|█         | 191/1825 [00:13<01:49, 14.90it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  11%|█         | 193/1825 [00:13<01:48, 15.09it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  11%|█         | 195/1825 [00:13<01:46, 15.24it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  11%|█         | 197/1825 [00:13<01:46, 15.22it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  11%|█         | 199/1825 [00:13<01:45, 15.34it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  11%|█         | 201/1825 [00:13<01:45, 15.44it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  11%|█         | 203/1825 [00:14<01:44, 15.50it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  11%|█         | 205/1825 [00:14<01:44, 15.54it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  11%|█▏        | 207/1825 [00:14<01:44, 15.49it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  11%|█▏        | 209/1825 [00:14<01:43, 15.57it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  12%|█▏        | 211/1825 [00:14<01:44, 15.45it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  12%|█▏        | 213/1825 [00:14<01:44, 15.42it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  12%|█▏        | 215/1825 [00:14<01:47, 15.02it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  12%|█▏        | 217/1825 [00:15<01:50, 14.50it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  12%|█▏        | 219/1825 [00:15<01:50, 14.58it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  12%|█▏        | 221/1825 [00:15<01:53, 14.11it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  12%|█▏        | 223/1825 [00:15<01:51, 14.34it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  12%|█▏        | 225/1825 [00:15<01:50, 14.51it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  12%|█▏        | 227/1825 [00:15<01:48, 14.66it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  13%|█▎        | 229/1825 [00:15<01:47, 14.81it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  13%|█▎        | 231/1825 [00:16<01:47, 14.84it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  13%|█▎        | 233/1825 [00:16<01:46, 14.91it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  13%|█▎        | 235/1825 [00:16<01:46, 14.99it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  13%|█▎        | 237/1825 [00:16<01:45, 15.07it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  13%|█▎        | 239/1825 [00:16<01:47, 14.72it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  13%|█▎        | 241/1825 [00:16<01:46, 14.90it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  13%|█▎        | 243/1825 [00:16<01:45, 14.98it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  13%|█▎        | 245/1825 [00:16<01:45, 15.00it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  14%|█▎        | 247/1825 [00:17<01:45, 14.96it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  14%|█▎        | 249/1825 [00:17<01:45, 14.89it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  14%|█▍        | 251/1825 [00:17<01:46, 14.77it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  14%|█▍        | 253/1825 [00:17<01:45, 14.92it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  14%|█▍        | 255/1825 [00:17<01:44, 15.09it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  14%|█▍        | 257/1825 [00:17<01:43, 15.12it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  14%|█▍        | 259/1825 [00:17<01:44, 15.05it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  14%|█▍        | 261/1825 [00:18<01:46, 14.67it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  14%|█▍        | 263/1825 [00:18<01:46, 14.73it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  15%|█▍        | 265/1825 [00:18<01:45, 14.79it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  15%|█▍        | 267/1825 [00:18<01:46, 14.57it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  15%|█▍        | 269/1825 [00:18<01:47, 14.47it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  15%|█▍        | 271/1825 [00:18<01:47, 14.49it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  15%|█▍        | 273/1825 [00:18<01:45, 14.73it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  15%|█▌        | 275/1825 [00:18<01:46, 14.62it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  15%|█▌        | 277/1825 [00:19<01:44, 14.84it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  15%|█▌        | 279/1825 [00:19<01:44, 14.79it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  15%|█▌        | 281/1825 [00:19<01:48, 14.23it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  16%|█▌        | 283/1825 [00:19<01:46, 14.41it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  16%|█▌        | 285/1825 [00:19<01:46, 14.47it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  16%|█▌        | 287/1825 [00:19<01:45, 14.61it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  16%|█▌        | 289/1825 [00:19<01:44, 14.77it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  16%|█▌        | 291/1825 [00:20<01:42, 14.94it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  16%|█▌        | 293/1825 [00:20<01:42, 14.95it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  16%|█▌        | 295/1825 [00:20<01:41, 15.03it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  16%|█▋        | 297/1825 [00:20<01:41, 15.08it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  16%|█▋        | 299/1825 [00:20<01:40, 15.14it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  16%|█▋        | 301/1825 [00:20<01:40, 15.19it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  17%|█▋        | 303/1825 [00:20<01:40, 15.12it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  17%|█▋        | 305/1825 [00:21<01:41, 14.97it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  17%|█▋        | 307/1825 [00:21<01:41, 15.02it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  17%|█▋        | 309/1825 [00:21<01:40, 15.04it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  17%|█▋        | 311/1825 [00:21<01:43, 14.65it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  17%|█▋        | 313/1825 [00:21<01:45, 14.38it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  17%|█▋        | 315/1825 [00:21<01:46, 14.21it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  17%|█▋        | 317/1825 [00:21<01:45, 14.30it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  17%|█▋        | 319/1825 [00:21<01:42, 14.65it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  18%|█▊        | 321/1825 [00:22<01:40, 14.91it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  18%|█▊        | 323/1825 [00:22<01:39, 15.11it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  18%|█▊        | 325/1825 [00:22<01:38, 15.23it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  18%|█▊        | 327/1825 [00:22<01:39, 15.07it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  18%|█▊        | 329/1825 [00:22<01:39, 15.09it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  18%|█▊        | 331/1825 [00:22<01:38, 15.17it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  18%|█▊        | 333/1825 [00:22<01:42, 14.54it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  18%|█▊        | 335/1825 [00:23<01:41, 14.66it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  18%|█▊        | 337/1825 [00:23<01:39, 14.91it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  19%|█▊        | 339/1825 [00:23<01:40, 14.73it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  19%|█▊        | 341/1825 [00:23<01:40, 14.70it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  19%|█▉        | 343/1825 [00:23<01:39, 14.85it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  19%|█▉        | 345/1825 [00:23<01:39, 14.90it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  19%|█▉        | 347/1825 [00:23<01:41, 14.61it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  19%|█▉        | 349/1825 [00:23<01:40, 14.67it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  19%|█▉        | 351/1825 [00:24<01:39, 14.81it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  19%|█▉        | 353/1825 [00:24<01:38, 14.97it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  19%|█▉        | 355/1825 [00:24<01:38, 14.96it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  20%|█▉        | 357/1825 [00:24<01:37, 15.04it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  20%|█▉        | 359/1825 [00:24<01:37, 15.11it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  20%|█▉        | 361/1825 [00:24<01:38, 14.82it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  20%|█▉        | 363/1825 [00:24<01:40, 14.48it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  20%|██        | 365/1825 [00:25<01:39, 14.64it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  20%|██        | 367/1825 [00:25<01:38, 14.79it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  20%|██        | 369/1825 [00:25<01:37, 14.93it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  20%|██        | 371/1825 [00:25<01:37, 14.87it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  20%|██        | 373/1825 [00:25<01:39, 14.65it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  21%|██        | 375/1825 [00:25<01:37, 14.83it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  21%|██        | 377/1825 [00:25<01:37, 14.91it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  21%|██        | 379/1825 [00:26<01:36, 14.97it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  21%|██        | 381/1825 [00:26<01:35, 15.09it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  21%|██        | 383/1825 [00:26<01:35, 15.16it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  21%|██        | 385/1825 [00:26<01:34, 15.19it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  21%|██        | 387/1825 [00:26<01:34, 15.16it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  21%|██▏       | 389/1825 [00:26<01:35, 15.02it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  21%|██▏       | 391/1825 [00:26<01:34, 15.15it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  22%|██▏       | 393/1825 [00:26<01:34, 15.08it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  22%|██▏       | 395/1825 [00:27<01:36, 14.85it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  22%|██▏       | 397/1825 [00:27<01:37, 14.60it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  22%|██▏       | 399/1825 [00:27<01:42, 13.94it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  22%|██▏       | 401/1825 [00:27<01:42, 13.95it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  22%|██▏       | 403/1825 [00:27<01:39, 14.34it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  22%|██▏       | 405/1825 [00:27<01:42, 13.80it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  22%|██▏       | 407/1825 [00:27<01:45, 13.46it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  22%|██▏       | 409/1825 [00:28<01:43, 13.70it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  23%|██▎       | 411/1825 [00:28<01:39, 14.21it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  23%|██▎       | 413/1825 [00:28<01:38, 14.33it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  23%|██▎       | 415/1825 [00:28<01:36, 14.59it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  23%|██▎       | 417/1825 [00:28<01:39, 14.21it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  23%|██▎       | 419/1825 [00:28<01:40, 13.98it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  23%|██▎       | 421/1825 [00:28<01:40, 14.02it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  23%|██▎       | 423/1825 [00:29<01:37, 14.37it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  23%|██▎       | 425/1825 [00:29<01:36, 14.47it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  23%|██▎       | 427/1825 [00:29<01:37, 14.28it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  24%|██▎       | 429/1825 [00:29<01:40, 13.95it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  24%|██▎       | 431/1825 [00:29<01:39, 13.95it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  24%|██▎       | 433/1825 [00:29<01:37, 14.34it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  24%|██▍       | 435/1825 [00:29<01:36, 14.37it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  24%|██▍       | 437/1825 [00:30<01:37, 14.24it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  24%|██▍       | 439/1825 [00:30<01:38, 14.07it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  24%|██▍       | 441/1825 [00:30<01:35, 14.44it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  24%|██▍       | 443/1825 [00:30<01:35, 14.53it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  24%|██▍       | 445/1825 [00:30<01:34, 14.67it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  24%|██▍       | 447/1825 [00:30<01:33, 14.79it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  25%|██▍       | 449/1825 [00:30<01:32, 14.85it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  25%|██▍       | 451/1825 [00:30<01:33, 14.73it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  25%|██▍       | 453/1825 [00:31<01:33, 14.69it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  25%|██▍       | 455/1825 [00:31<01:34, 14.57it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  25%|██▌       | 457/1825 [00:31<01:32, 14.76it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  25%|██▌       | 459/1825 [00:31<01:34, 14.40it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  25%|██▌       | 461/1825 [00:31<01:35, 14.29it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  25%|██▌       | 463/1825 [00:31<01:34, 14.45it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  25%|██▌       | 465/1825 [00:31<01:32, 14.65it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  26%|██▌       | 467/1825 [00:32<01:32, 14.74it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  26%|██▌       | 469/1825 [00:32<01:30, 14.97it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  26%|██▌       | 471/1825 [00:32<01:34, 14.37it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  26%|██▌       | 473/1825 [00:32<01:33, 14.41it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  26%|██▌       | 475/1825 [00:32<01:32, 14.67it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  26%|██▌       | 477/1825 [00:32<01:31, 14.77it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  26%|██▌       | 479/1825 [00:32<01:30, 14.93it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  26%|██▋       | 481/1825 [00:33<01:30, 14.77it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  26%|██▋       | 483/1825 [00:33<01:32, 14.48it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  27%|██▋       | 485/1825 [00:33<01:33, 14.34it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  27%|██▋       | 487/1825 [00:33<01:30, 14.73it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  27%|██▋       | 489/1825 [00:33<01:29, 14.89it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  27%|██▋       | 491/1825 [00:33<01:29, 14.98it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  27%|██▋       | 493/1825 [00:33<01:28, 15.01it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  27%|██▋       | 495/1825 [00:33<01:29, 14.81it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  27%|██▋       | 497/1825 [00:34<01:29, 14.78it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  27%|██▋       | 499/1825 [00:34<01:28, 14.98it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  27%|██▋       | 501/1825 [00:34<01:27, 15.08it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  28%|██▊       | 503/1825 [00:34<01:28, 15.01it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  28%|██▊       | 505/1825 [00:34<01:27, 15.04it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  28%|██▊       | 507/1825 [00:34<01:27, 15.02it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  28%|██▊       | 509/1825 [00:34<01:29, 14.75it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  28%|██▊       | 511/1825 [00:35<01:28, 14.80it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  28%|██▊       | 513/1825 [00:35<01:28, 14.82it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  28%|██▊       | 515/1825 [00:35<01:29, 14.66it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  28%|██▊       | 517/1825 [00:35<01:29, 14.68it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  28%|██▊       | 519/1825 [00:35<01:27, 14.91it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  29%|██▊       | 521/1825 [00:35<01:26, 15.02it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  29%|██▊       | 523/1825 [00:35<01:26, 15.14it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  29%|██▉       | 525/1825 [00:35<01:25, 15.19it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  29%|██▉       | 527/1825 [00:36<01:25, 15.20it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  29%|██▉       | 529/1825 [00:36<01:25, 15.16it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  29%|██▉       | 531/1825 [00:36<01:27, 14.72it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  29%|██▉       | 533/1825 [00:36<01:26, 14.90it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  29%|██▉       | 535/1825 [00:36<01:25, 15.01it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  29%|██▉       | 537/1825 [00:36<01:25, 15.06it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  30%|██▉       | 539/1825 [00:36<01:24, 15.19it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  30%|██▉       | 541/1825 [00:37<01:24, 15.16it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  30%|██▉       | 543/1825 [00:37<01:24, 15.25it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  30%|██▉       | 545/1825 [00:37<01:24, 15.14it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  30%|██▉       | 547/1825 [00:37<01:25, 15.01it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  30%|███       | 549/1825 [00:37<01:23, 15.23it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  30%|███       | 551/1825 [00:37<01:25, 14.91it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  30%|███       | 553/1825 [00:37<01:26, 14.78it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  30%|███       | 555/1825 [00:38<01:26, 14.70it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  31%|███       | 557/1825 [00:38<01:24, 14.93it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  31%|███       | 559/1825 [00:38<01:24, 14.93it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  31%|███       | 561/1825 [00:38<01:24, 14.92it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  31%|███       | 563/1825 [00:38<01:23, 15.04it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  31%|███       | 565/1825 [00:38<01:23, 15.10it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  31%|███       | 567/1825 [00:38<01:23, 15.12it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  31%|███       | 569/1825 [00:38<01:23, 15.13it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  31%|███▏      | 571/1825 [00:39<01:25, 14.73it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  31%|███▏      | 573/1825 [00:39<01:23, 14.98it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  32%|███▏      | 575/1825 [00:39<01:25, 14.62it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  32%|███▏      | 577/1825 [00:39<01:24, 14.75it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  32%|███▏      | 579/1825 [00:39<01:27, 14.26it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  32%|███▏      | 581/1825 [00:39<01:26, 14.43it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  32%|███▏      | 583/1825 [00:39<01:25, 14.49it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  32%|███▏      | 585/1825 [00:40<01:24, 14.65it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  32%|███▏      | 587/1825 [00:40<01:25, 14.44it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  32%|███▏      | 589/1825 [00:40<01:25, 14.48it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  32%|███▏      | 591/1825 [00:40<01:25, 14.35it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  32%|███▏      | 593/1825 [00:40<01:26, 14.20it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  33%|███▎      | 595/1825 [00:40<01:25, 14.34it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  33%|███▎      | 597/1825 [00:40<01:25, 14.42it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  33%|███▎      | 599/1825 [00:41<01:25, 14.33it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  33%|███▎      | 601/1825 [00:41<01:24, 14.53it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  33%|███▎      | 603/1825 [00:41<01:25, 14.37it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  33%|███▎      | 605/1825 [00:41<01:25, 14.20it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  33%|███▎      | 607/1825 [00:41<01:26, 14.14it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  33%|███▎      | 609/1825 [00:41<01:26, 14.12it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  33%|███▎      | 611/1825 [00:41<01:30, 13.34it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  34%|███▎      | 613/1825 [00:42<01:32, 13.04it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  34%|███▎      | 615/1825 [00:42<01:32, 13.08it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  34%|███▍      | 617/1825 [00:42<01:31, 13.22it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  34%|███▍      | 619/1825 [00:42<01:28, 13.58it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  34%|███▍      | 621/1825 [00:42<01:25, 14.03it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  34%|███▍      | 623/1825 [00:42<01:26, 13.93it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  34%|███▍      | 625/1825 [00:42<01:28, 13.61it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  34%|███▍      | 627/1825 [00:43<01:33, 12.83it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  34%|███▍      | 629/1825 [00:43<01:34, 12.71it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  35%|███▍      | 631/1825 [00:43<01:30, 13.18it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  35%|███▍      | 633/1825 [00:43<01:28, 13.43it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  35%|███▍      | 635/1825 [00:43<01:25, 13.88it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  35%|███▍      | 637/1825 [00:43<01:25, 13.91it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  35%|███▌      | 639/1825 [00:43<01:25, 13.79it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  35%|███▌      | 641/1825 [00:44<01:29, 13.24it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  35%|███▌      | 643/1825 [00:44<01:29, 13.24it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  35%|███▌      | 645/1825 [00:44<01:27, 13.44it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  35%|███▌      | 647/1825 [00:44<01:28, 13.30it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  36%|███▌      | 649/1825 [00:44<01:26, 13.66it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  36%|███▌      | 651/1825 [00:44<01:23, 14.08it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  36%|███▌      | 653/1825 [00:44<01:21, 14.40it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  36%|███▌      | 655/1825 [00:45<01:21, 14.41it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  36%|███▌      | 657/1825 [00:45<01:19, 14.64it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  36%|███▌      | 659/1825 [00:45<01:18, 14.80it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  36%|███▌      | 661/1825 [00:45<01:18, 14.74it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  36%|███▋      | 663/1825 [00:45<01:18, 14.81it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  36%|███▋      | 665/1825 [00:45<01:17, 14.93it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  37%|███▋      | 667/1825 [00:45<01:16, 15.13it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  37%|███▋      | 669/1825 [00:46<01:15, 15.30it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  37%|███▋      | 671/1825 [00:46<01:19, 14.57it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  37%|███▋      | 673/1825 [00:46<01:17, 14.86it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  37%|███▋      | 675/1825 [00:46<01:16, 15.05it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  37%|███▋      | 677/1825 [00:46<01:18, 14.66it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  37%|███▋      | 679/1825 [00:46<01:16, 14.90it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  37%|███▋      | 681/1825 [00:46<01:15, 15.07it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  37%|███▋      | 683/1825 [00:46<01:15, 15.12it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  38%|███▊      | 685/1825 [00:47<01:15, 15.18it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  38%|███▊      | 687/1825 [00:47<01:14, 15.20it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  38%|███▊      | 689/1825 [00:47<01:16, 14.83it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  38%|███▊      | 691/1825 [00:47<01:17, 14.59it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  38%|███▊      | 693/1825 [00:47<01:18, 14.51it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  38%|███▊      | 695/1825 [00:47<01:17, 14.53it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  38%|███▊      | 697/1825 [00:47<01:16, 14.74it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  38%|███▊      | 699/1825 [00:48<01:15, 14.94it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  38%|███▊      | 701/1825 [00:48<01:14, 15.03it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  39%|███▊      | 703/1825 [00:48<01:14, 14.99it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  39%|███▊      | 705/1825 [00:48<01:15, 14.77it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  39%|███▊      | 707/1825 [00:48<01:16, 14.63it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  39%|███▉      | 709/1825 [00:48<01:16, 14.64it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  39%|███▉      | 711/1825 [00:48<01:16, 14.64it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  39%|███▉      | 713/1825 [00:49<01:16, 14.54it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  39%|███▉      | 715/1825 [00:49<01:16, 14.52it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  39%|███▉      | 717/1825 [00:49<01:16, 14.41it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  39%|███▉      | 719/1825 [00:49<01:15, 14.60it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  40%|███▉      | 721/1825 [00:49<01:14, 14.82it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  40%|███▉      | 723/1825 [00:49<01:13, 15.00it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  40%|███▉      | 725/1825 [00:49<01:12, 15.15it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  40%|███▉      | 727/1825 [00:49<01:13, 15.02it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  40%|███▉      | 729/1825 [00:50<01:13, 14.97it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  40%|████      | 731/1825 [00:50<01:14, 14.59it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  40%|████      | 733/1825 [00:50<01:14, 14.63it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  40%|████      | 735/1825 [00:50<01:14, 14.71it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  40%|████      | 737/1825 [00:50<01:13, 14.78it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  40%|████      | 739/1825 [00:50<01:12, 14.92it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  41%|████      | 741/1825 [00:50<01:13, 14.74it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  41%|████      | 743/1825 [00:51<01:12, 14.86it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  41%|████      | 745/1825 [00:51<01:12, 14.90it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  41%|████      | 747/1825 [00:51<01:13, 14.65it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  41%|████      | 749/1825 [00:51<01:14, 14.45it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  41%|████      | 751/1825 [00:51<01:13, 14.71it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  41%|████▏     | 753/1825 [00:51<01:12, 14.86it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  41%|████▏     | 755/1825 [00:51<01:11, 15.02it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  41%|████▏     | 757/1825 [00:51<01:11, 14.86it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  42%|████▏     | 759/1825 [00:52<01:13, 14.58it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  42%|████▏     | 761/1825 [00:52<01:11, 14.79it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  42%|████▏     | 763/1825 [00:52<01:11, 14.92it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  42%|████▏     | 765/1825 [00:52<01:11, 14.90it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  42%|████▏     | 767/1825 [00:52<01:11, 14.86it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  42%|████▏     | 769/1825 [00:52<01:10, 14.91it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  42%|████▏     | 771/1825 [00:52<01:10, 14.97it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  42%|████▏     | 773/1825 [00:53<01:10, 15.01it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  42%|████▏     | 775/1825 [00:53<01:09, 15.14it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  43%|████▎     | 777/1825 [00:53<01:28, 11.85it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  43%|████▎     | 779/1825 [00:53<01:22, 12.67it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  43%|████▎     | 781/1825 [00:53<01:18, 13.30it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  43%|████▎     | 783/1825 [00:53<01:15, 13.72it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  43%|████▎     | 785/1825 [00:53<01:14, 13.89it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  43%|████▎     | 787/1825 [00:54<01:12, 14.31it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  43%|████▎     | 789/1825 [00:54<01:13, 14.09it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  43%|████▎     | 791/1825 [00:54<01:12, 14.32it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  43%|████▎     | 793/1825 [00:54<01:10, 14.62it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  44%|████▎     | 795/1825 [00:54<01:09, 14.86it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  44%|████▎     | 797/1825 [00:54<01:09, 14.70it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  44%|████▍     | 799/1825 [00:54<01:09, 14.73it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  44%|████▍     | 801/1825 [00:55<01:08, 14.96it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  44%|████▍     | 803/1825 [00:55<01:09, 14.79it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  44%|████▍     | 805/1825 [00:55<01:07, 15.01it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  44%|████▍     | 807/1825 [00:55<01:08, 14.81it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  44%|████▍     | 809/1825 [00:55<01:09, 14.66it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  44%|████▍     | 811/1825 [00:55<01:08, 14.81it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  45%|████▍     | 813/1825 [00:55<01:06, 15.17it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  45%|████▍     | 815/1825 [00:55<01:05, 15.37it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  45%|████▍     | 817/1825 [00:56<01:04, 15.51it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  45%|████▍     | 819/1825 [00:56<01:05, 15.33it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  45%|████▍     | 821/1825 [00:56<01:05, 15.33it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  45%|████▌     | 823/1825 [00:56<01:05, 15.33it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  45%|████▌     | 825/1825 [00:56<01:05, 15.36it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  45%|████▌     | 827/1825 [00:56<01:05, 15.32it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  45%|████▌     | 829/1825 [00:56<01:05, 15.18it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  46%|████▌     | 831/1825 [00:57<01:04, 15.34it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  46%|████▌     | 833/1825 [00:57<01:04, 15.40it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  46%|████▌     | 835/1825 [00:57<01:05, 15.08it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  46%|████▌     | 837/1825 [00:57<01:05, 15.16it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  46%|████▌     | 839/1825 [00:57<01:04, 15.30it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  46%|████▌     | 841/1825 [00:57<01:04, 15.37it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  46%|████▌     | 843/1825 [00:57<01:03, 15.36it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  46%|████▋     | 845/1825 [00:57<01:03, 15.37it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  46%|████▋     | 847/1825 [00:58<01:04, 15.09it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  47%|████▋     | 849/1825 [00:58<01:04, 15.20it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  47%|████▋     | 851/1825 [00:58<01:05, 14.98it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  47%|████▋     | 853/1825 [00:58<01:04, 15.05it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  47%|████▋     | 855/1825 [00:58<01:07, 14.37it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  47%|████▋     | 857/1825 [00:58<01:06, 14.59it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  47%|████▋     | 859/1825 [00:58<01:04, 14.91it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  47%|████▋     | 861/1825 [00:59<01:03, 15.10it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  47%|████▋     | 863/1825 [00:59<01:03, 15.14it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  47%|████▋     | 865/1825 [00:59<01:03, 15.02it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  48%|████▊     | 867/1825 [00:59<01:04, 14.96it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  48%|████▊     | 869/1825 [00:59<01:03, 15.13it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  48%|████▊     | 871/1825 [00:59<01:02, 15.25it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  48%|████▊     | 873/1825 [00:59<01:03, 14.92it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  48%|████▊     | 875/1825 [00:59<01:02, 15.13it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  48%|████▊     | 877/1825 [01:00<01:04, 14.80it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  48%|████▊     | 879/1825 [01:00<01:02, 15.03it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  48%|████▊     | 881/1825 [01:00<01:05, 14.47it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  48%|████▊     | 883/1825 [01:00<01:05, 14.49it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  48%|████▊     | 885/1825 [01:00<01:04, 14.49it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  49%|████▊     | 887/1825 [01:00<01:03, 14.79it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  49%|████▊     | 889/1825 [01:00<01:03, 14.65it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  49%|████▉     | 891/1825 [01:01<01:03, 14.67it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  49%|████▉     | 893/1825 [01:01<01:02, 14.83it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  49%|████▉     | 895/1825 [01:01<01:03, 14.59it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  49%|████▉     | 897/1825 [01:01<01:03, 14.58it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  49%|████▉     | 899/1825 [01:01<01:05, 14.05it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  49%|████▉     | 901/1825 [01:01<01:03, 14.54it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  49%|████▉     | 903/1825 [01:01<01:02, 14.86it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  50%|████▉     | 905/1825 [01:02<01:00, 15.10it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  50%|████▉     | 907/1825 [01:02<01:00, 15.26it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  50%|████▉     | 909/1825 [01:02<00:59, 15.38it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  50%|████▉     | 911/1825 [01:02<00:58, 15.52it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  50%|█████     | 913/1825 [01:02<00:59, 15.36it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  50%|█████     | 915/1825 [01:02<00:58, 15.46it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  50%|█████     | 917/1825 [01:02<00:58, 15.50it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  50%|█████     | 919/1825 [01:02<00:58, 15.54it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  50%|█████     | 921/1825 [01:03<00:58, 15.55it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  51%|█████     | 923/1825 [01:03<00:57, 15.59it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  51%|█████     | 925/1825 [01:03<00:57, 15.62it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  51%|█████     | 927/1825 [01:03<00:57, 15.64it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  51%|█████     | 929/1825 [01:03<00:57, 15.63it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  51%|█████     | 931/1825 [01:03<00:57, 15.52it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  51%|█████     | 933/1825 [01:03<00:58, 15.31it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  51%|█████     | 935/1825 [01:03<00:58, 15.18it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  51%|█████▏    | 937/1825 [01:04<00:58, 15.20it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  51%|█████▏    | 939/1825 [01:04<00:58, 15.27it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  52%|█████▏    | 941/1825 [01:04<00:58, 15.14it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  52%|█████▏    | 943/1825 [01:04<00:58, 15.17it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  52%|█████▏    | 945/1825 [01:04<00:58, 15.15it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  52%|█████▏    | 947/1825 [01:04<00:58, 15.08it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  52%|█████▏    | 949/1825 [01:04<00:59, 14.77it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  52%|█████▏    | 951/1825 [01:05<01:02, 14.10it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  52%|█████▏    | 953/1825 [01:05<01:00, 14.39it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  52%|█████▏    | 955/1825 [01:05<01:00, 14.48it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  52%|█████▏    | 957/1825 [01:05<00:58, 14.77it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  53%|█████▎    | 959/1825 [01:05<00:57, 14.94it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  53%|█████▎    | 961/1825 [01:05<00:57, 15.14it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  53%|█████▎    | 963/1825 [01:05<00:56, 15.16it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  53%|█████▎    | 965/1825 [01:05<00:58, 14.62it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  53%|█████▎    | 967/1825 [01:06<00:57, 14.83it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  53%|█████▎    | 969/1825 [01:06<00:56, 15.03it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  53%|█████▎    | 971/1825 [01:06<00:57, 14.91it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  53%|█████▎    | 973/1825 [01:06<00:56, 15.08it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  53%|█████▎    | 975/1825 [01:06<00:56, 14.97it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  54%|█████▎    | 977/1825 [01:06<00:56, 15.07it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  54%|█████▎    | 979/1825 [01:06<00:57, 14.81it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  54%|█████▍    | 981/1825 [01:07<00:56, 14.89it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  54%|█████▍    | 983/1825 [01:07<00:57, 14.65it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  54%|█████▍    | 985/1825 [01:07<00:58, 14.32it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  54%|█████▍    | 987/1825 [01:07<00:57, 14.55it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  54%|█████▍    | 989/1825 [01:07<00:56, 14.70it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  54%|█████▍    | 991/1825 [01:07<00:55, 14.92it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  54%|█████▍    | 993/1825 [01:07<00:56, 14.81it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  55%|█████▍    | 995/1825 [01:07<00:55, 14.99it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  55%|█████▍    | 997/1825 [01:08<00:55, 15.03it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  55%|█████▍    | 999/1825 [01:08<00:55, 14.98it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  55%|█████▍    | 1001/1825 [01:08<00:56, 14.46it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  55%|█████▍    | 1003/1825 [01:08<00:56, 14.55it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  55%|█████▌    | 1005/1825 [01:08<00:55, 14.84it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  55%|█████▌    | 1007/1825 [01:08<00:55, 14.74it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  55%|█████▌    | 1009/1825 [01:08<00:55, 14.80it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  55%|█████▌    | 1011/1825 [01:09<00:54, 15.00it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  56%|█████▌    | 1013/1825 [01:09<00:54, 14.97it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  56%|█████▌    | 1015/1825 [01:09<00:53, 15.12it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  56%|█████▌    | 1017/1825 [01:09<00:54, 14.96it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  56%|█████▌    | 1019/1825 [01:09<00:53, 15.17it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  56%|█████▌    | 1021/1825 [01:09<00:54, 14.78it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  56%|█████▌    | 1023/1825 [01:09<00:53, 15.07it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  56%|█████▌    | 1025/1825 [01:09<00:52, 15.27it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  56%|█████▋    | 1027/1825 [01:10<00:53, 14.86it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  56%|█████▋    | 1029/1825 [01:10<00:53, 14.95it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  56%|█████▋    | 1031/1825 [01:10<00:53, 14.91it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  57%|█████▋    | 1033/1825 [01:10<00:52, 14.95it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  57%|█████▋    | 1035/1825 [01:10<00:52, 15.11it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  57%|█████▋    | 1037/1825 [01:10<00:51, 15.31it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  57%|█████▋    | 1039/1825 [01:10<00:52, 14.84it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  57%|█████▋    | 1041/1825 [01:11<00:52, 14.99it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  57%|█████▋    | 1043/1825 [01:11<00:51, 15.07it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  57%|█████▋    | 1045/1825 [01:11<00:51, 15.25it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  57%|█████▋    | 1047/1825 [01:11<00:52, 14.69it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  57%|█████▋    | 1049/1825 [01:11<00:51, 15.09it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  58%|█████▊    | 1051/1825 [01:11<00:50, 15.19it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  58%|█████▊    | 1053/1825 [01:11<00:50, 15.29it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  58%|█████▊    | 1055/1825 [01:11<00:50, 15.20it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  58%|█████▊    | 1057/1825 [01:12<00:51, 14.86it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  58%|█████▊    | 1059/1825 [01:12<00:51, 14.86it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  58%|█████▊    | 1061/1825 [01:12<00:50, 15.03it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  58%|█████▊    | 1063/1825 [01:12<00:51, 14.78it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  58%|█████▊    | 1065/1825 [01:12<00:50, 14.98it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  58%|█████▊    | 1067/1825 [01:12<00:49, 15.23it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  59%|█████▊    | 1069/1825 [01:12<00:50, 14.89it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  59%|█████▊    | 1071/1825 [01:13<00:51, 14.72it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  59%|█████▉    | 1073/1825 [01:13<00:50, 15.03it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  59%|█████▉    | 1075/1825 [01:13<00:50, 14.91it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  59%|█████▉    | 1077/1825 [01:13<00:51, 14.48it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  59%|█████▉    | 1079/1825 [01:13<00:50, 14.85it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  59%|█████▉    | 1081/1825 [01:13<00:49, 15.09it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  59%|█████▉    | 1083/1825 [01:13<00:49, 14.86it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  59%|█████▉    | 1085/1825 [01:14<00:52, 14.06it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  60%|█████▉    | 1087/1825 [01:14<00:50, 14.50it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  60%|█████▉    | 1089/1825 [01:14<00:50, 14.59it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  60%|█████▉    | 1091/1825 [01:14<00:50, 14.56it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  60%|█████▉    | 1093/1825 [01:14<00:49, 14.81it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  60%|██████    | 1095/1825 [01:14<00:48, 15.12it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  60%|██████    | 1097/1825 [01:14<00:47, 15.20it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  60%|██████    | 1099/1825 [01:14<00:47, 15.17it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  60%|██████    | 1101/1825 [01:15<00:48, 14.92it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  60%|██████    | 1103/1825 [01:15<00:47, 15.05it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  61%|██████    | 1105/1825 [01:15<00:48, 14.95it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  61%|██████    | 1107/1825 [01:15<00:47, 15.15it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  61%|██████    | 1109/1825 [01:15<00:46, 15.38it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  61%|██████    | 1111/1825 [01:15<00:46, 15.47it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  61%|██████    | 1113/1825 [01:15<00:48, 14.82it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  61%|██████    | 1115/1825 [01:16<00:47, 14.85it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  61%|██████    | 1117/1825 [01:16<00:47, 14.77it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  61%|██████▏   | 1119/1825 [01:16<00:47, 14.85it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  61%|██████▏   | 1121/1825 [01:16<00:46, 15.00it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  62%|██████▏   | 1123/1825 [01:16<00:45, 15.27it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  62%|██████▏   | 1125/1825 [01:16<00:45, 15.38it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  62%|██████▏   | 1127/1825 [01:16<00:45, 15.31it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  62%|██████▏   | 1129/1825 [01:16<00:45, 15.21it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  62%|██████▏   | 1131/1825 [01:17<00:45, 15.30it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  62%|██████▏   | 1133/1825 [01:17<00:45, 15.25it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  62%|██████▏   | 1135/1825 [01:17<00:44, 15.40it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  62%|██████▏   | 1137/1825 [01:17<00:44, 15.49it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  62%|██████▏   | 1139/1825 [01:17<00:44, 15.50it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  63%|██████▎   | 1141/1825 [01:17<00:44, 15.50it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  63%|██████▎   | 1143/1825 [01:17<00:45, 15.13it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  63%|██████▎   | 1145/1825 [01:17<00:46, 14.77it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  63%|██████▎   | 1147/1825 [01:18<00:45, 14.92it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  63%|██████▎   | 1149/1825 [01:18<00:45, 14.88it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  63%|██████▎   | 1151/1825 [01:18<00:44, 15.02it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  63%|██████▎   | 1153/1825 [01:18<00:45, 14.88it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  63%|██████▎   | 1155/1825 [01:18<00:44, 15.04it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  63%|██████▎   | 1157/1825 [01:18<00:44, 15.04it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  64%|██████▎   | 1159/1825 [01:18<00:43, 15.16it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  64%|██████▎   | 1161/1825 [01:19<00:43, 15.24it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  64%|██████▎   | 1163/1825 [01:19<00:44, 15.01it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  64%|██████▍   | 1165/1825 [01:19<00:43, 15.06it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  64%|██████▍   | 1167/1825 [01:19<00:43, 15.19it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  64%|██████▍   | 1169/1825 [01:19<00:43, 15.07it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  64%|██████▍   | 1171/1825 [01:19<00:43, 14.98it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  64%|██████▍   | 1173/1825 [01:19<00:44, 14.69it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  64%|██████▍   | 1175/1825 [01:20<00:44, 14.51it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  64%|██████▍   | 1177/1825 [01:20<00:45, 14.35it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  65%|██████▍   | 1179/1825 [01:20<00:44, 14.58it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  65%|██████▍   | 1181/1825 [01:20<00:43, 14.86it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  65%|██████▍   | 1183/1825 [01:20<00:42, 15.01it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  65%|██████▍   | 1185/1825 [01:20<00:42, 15.21it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  65%|██████▌   | 1187/1825 [01:20<00:42, 15.15it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  65%|██████▌   | 1189/1825 [01:20<00:41, 15.33it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  65%|██████▌   | 1191/1825 [01:21<00:41, 15.21it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  65%|██████▌   | 1193/1825 [01:21<00:41, 15.08it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  65%|██████▌   | 1195/1825 [01:21<00:41, 15.26it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  66%|██████▌   | 1197/1825 [01:21<00:41, 15.28it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  66%|██████▌   | 1199/1825 [01:21<00:40, 15.37it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  66%|██████▌   | 1201/1825 [01:21<00:40, 15.50it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  66%|██████▌   | 1203/1825 [01:21<00:40, 15.46it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  66%|██████▌   | 1205/1825 [01:21<00:40, 15.29it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  66%|██████▌   | 1207/1825 [01:22<00:40, 15.30it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  66%|██████▌   | 1209/1825 [01:22<00:40, 15.36it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  66%|██████▋   | 1211/1825 [01:22<00:40, 14.99it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  66%|██████▋   | 1213/1825 [01:22<00:41, 14.78it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  67%|██████▋   | 1215/1825 [01:22<00:41, 14.82it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  67%|██████▋   | 1217/1825 [01:22<00:40, 15.01it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  67%|██████▋   | 1219/1825 [01:22<00:40, 14.98it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  67%|██████▋   | 1221/1825 [01:23<00:39, 15.14it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  67%|██████▋   | 1223/1825 [01:23<00:39, 15.28it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  67%|██████▋   | 1225/1825 [01:23<00:39, 15.35it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  67%|██████▋   | 1227/1825 [01:23<00:39, 15.26it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  67%|██████▋   | 1229/1825 [01:23<00:38, 15.41it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  67%|██████▋   | 1231/1825 [01:23<00:38, 15.56it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  68%|██████▊   | 1233/1825 [01:23<00:38, 15.36it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  68%|██████▊   | 1235/1825 [01:23<00:38, 15.32it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  68%|██████▊   | 1237/1825 [01:24<00:38, 15.47it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  68%|██████▊   | 1239/1825 [01:24<00:37, 15.55it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  68%|██████▊   | 1241/1825 [01:24<00:37, 15.61it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  68%|██████▊   | 1243/1825 [01:24<00:37, 15.52it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  68%|██████▊   | 1245/1825 [01:24<00:37, 15.57it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  68%|██████▊   | 1247/1825 [01:24<00:37, 15.59it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  68%|██████▊   | 1249/1825 [01:24<00:38, 15.00it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  69%|██████▊   | 1251/1825 [01:24<00:37, 15.14it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  69%|██████▊   | 1253/1825 [01:25<00:37, 15.27it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  69%|██████▉   | 1255/1825 [01:25<00:37, 15.22it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  69%|██████▉   | 1257/1825 [01:25<00:37, 15.03it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  69%|██████▉   | 1259/1825 [01:25<00:37, 14.99it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  69%|██████▉   | 1261/1825 [01:25<00:37, 15.12it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  69%|██████▉   | 1263/1825 [01:25<00:37, 15.17it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  69%|██████▉   | 1265/1825 [01:25<00:37, 14.93it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  69%|██████▉   | 1267/1825 [01:26<00:37, 15.01it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  70%|██████▉   | 1269/1825 [01:26<00:36, 15.16it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  70%|██████▉   | 1271/1825 [01:26<00:36, 15.17it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  70%|██████▉   | 1273/1825 [01:26<00:37, 14.66it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  70%|██████▉   | 1275/1825 [01:26<00:37, 14.81it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  70%|██████▉   | 1277/1825 [01:26<00:36, 14.99it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  70%|███████   | 1279/1825 [01:26<00:36, 15.06it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  70%|███████   | 1281/1825 [01:26<00:35, 15.18it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  70%|███████   | 1283/1825 [01:27<00:35, 15.29it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  70%|███████   | 1285/1825 [01:27<00:35, 15.19it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  71%|███████   | 1287/1825 [01:27<00:35, 15.24it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  71%|███████   | 1289/1825 [01:27<00:35, 15.26it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  71%|███████   | 1291/1825 [01:27<00:34, 15.36it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  71%|███████   | 1293/1825 [01:27<00:35, 14.97it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  71%|███████   | 1295/1825 [01:27<00:34, 15.24it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  71%|███████   | 1297/1825 [01:28<00:36, 14.56it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  71%|███████   | 1299/1825 [01:28<00:35, 14.62it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  71%|███████▏  | 1301/1825 [01:28<00:35, 14.81it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  71%|███████▏  | 1303/1825 [01:28<00:35, 14.65it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  72%|███████▏  | 1305/1825 [01:28<00:35, 14.65it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  72%|███████▏  | 1307/1825 [01:28<00:35, 14.53it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  72%|███████▏  | 1309/1825 [01:28<00:35, 14.66it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  72%|███████▏  | 1311/1825 [01:28<00:34, 15.00it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  72%|███████▏  | 1313/1825 [01:29<00:35, 14.52it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  72%|███████▏  | 1315/1825 [01:29<00:34, 14.72it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  72%|███████▏  | 1317/1825 [01:29<00:34, 14.75it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  72%|███████▏  | 1319/1825 [01:29<00:33, 14.95it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  72%|███████▏  | 1321/1825 [01:29<00:33, 15.13it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  72%|███████▏  | 1323/1825 [01:29<00:33, 14.80it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  73%|███████▎  | 1325/1825 [01:29<00:34, 14.50it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  73%|███████▎  | 1327/1825 [01:30<00:34, 14.62it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  73%|███████▎  | 1329/1825 [01:30<00:33, 14.72it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  73%|███████▎  | 1331/1825 [01:30<00:33, 14.85it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  73%|███████▎  | 1333/1825 [01:30<00:33, 14.78it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  73%|███████▎  | 1335/1825 [01:30<00:32, 15.09it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  73%|███████▎  | 1337/1825 [01:30<00:32, 15.22it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  73%|███████▎  | 1339/1825 [01:30<00:31, 15.38it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  73%|███████▎  | 1341/1825 [01:31<00:32, 14.98it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  74%|███████▎  | 1343/1825 [01:31<00:32, 14.95it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  74%|███████▎  | 1345/1825 [01:31<00:31, 15.01it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  74%|███████▍  | 1347/1825 [01:31<00:31, 14.95it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  74%|███████▍  | 1349/1825 [01:31<00:31, 15.07it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  74%|███████▍  | 1351/1825 [01:31<00:31, 15.04it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  74%|███████▍  | 1353/1825 [01:31<00:31, 15.08it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  74%|███████▍  | 1355/1825 [01:31<00:32, 14.65it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  74%|███████▍  | 1357/1825 [01:32<00:32, 14.61it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  74%|███████▍  | 1359/1825 [01:32<00:31, 14.85it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  75%|███████▍  | 1361/1825 [01:32<00:30, 14.98it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  75%|███████▍  | 1363/1825 [01:32<00:31, 14.83it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  75%|███████▍  | 1365/1825 [01:32<00:32, 14.10it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  75%|███████▍  | 1367/1825 [01:32<00:32, 14.10it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  75%|███████▌  | 1369/1825 [01:32<00:32, 14.20it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  75%|███████▌  | 1371/1825 [01:33<00:31, 14.60it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  75%|███████▌  | 1373/1825 [01:33<00:30, 14.95it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  75%|███████▌  | 1375/1825 [01:33<00:30, 14.66it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  75%|███████▌  | 1377/1825 [01:33<00:30, 14.78it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  76%|███████▌  | 1379/1825 [01:33<00:29, 14.96it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  76%|███████▌  | 1381/1825 [01:33<00:29, 15.07it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  76%|███████▌  | 1383/1825 [01:33<00:29, 15.20it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  76%|███████▌  | 1385/1825 [01:33<00:28, 15.21it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  76%|███████▌  | 1387/1825 [01:34<00:28, 15.37it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  76%|███████▌  | 1389/1825 [01:34<00:28, 15.51it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  76%|███████▌  | 1391/1825 [01:34<00:28, 15.39it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  76%|███████▋  | 1393/1825 [01:34<00:28, 15.26it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  76%|███████▋  | 1395/1825 [01:34<00:27, 15.38it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  77%|███████▋  | 1397/1825 [01:34<00:28, 15.13it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  77%|███████▋  | 1399/1825 [01:34<00:28, 15.07it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  77%|███████▋  | 1401/1825 [01:35<00:27, 15.21it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  77%|███████▋  | 1403/1825 [01:35<00:27, 15.41it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  77%|███████▋  | 1405/1825 [01:35<00:27, 15.02it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  77%|███████▋  | 1407/1825 [01:35<00:28, 14.77it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  77%|███████▋  | 1409/1825 [01:35<00:28, 14.69it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  77%|███████▋  | 1411/1825 [01:35<00:27, 14.97it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  77%|███████▋  | 1413/1825 [01:35<00:27, 15.21it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  78%|███████▊  | 1415/1825 [01:35<00:27, 14.66it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  78%|███████▊  | 1417/1825 [01:36<00:27, 14.88it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  78%|███████▊  | 1419/1825 [01:36<00:27, 14.98it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  78%|███████▊  | 1421/1825 [01:36<00:26, 15.24it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  78%|███████▊  | 1423/1825 [01:36<00:26, 15.18it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  78%|███████▊  | 1425/1825 [01:36<00:26, 15.38it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  78%|███████▊  | 1427/1825 [01:36<00:25, 15.37it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  78%|███████▊  | 1429/1825 [01:36<00:25, 15.30it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  78%|███████▊  | 1431/1825 [01:37<00:26, 14.73it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  79%|███████▊  | 1433/1825 [01:37<00:26, 15.01it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  79%|███████▊  | 1435/1825 [01:37<00:26, 14.82it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  79%|███████▊  | 1437/1825 [01:37<00:26, 14.57it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  79%|███████▉  | 1439/1825 [01:37<00:26, 14.67it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  79%|███████▉  | 1441/1825 [01:37<00:25, 14.81it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  79%|███████▉  | 1443/1825 [01:37<00:25, 15.00it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  79%|███████▉  | 1445/1825 [01:37<00:25, 14.81it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  79%|███████▉  | 1447/1825 [01:38<00:25, 14.79it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  79%|███████▉  | 1449/1825 [01:38<00:25, 14.96it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  80%|███████▉  | 1451/1825 [01:38<00:24, 15.07it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  80%|███████▉  | 1453/1825 [01:38<00:24, 15.22it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  80%|███████▉  | 1455/1825 [01:38<00:24, 15.13it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  80%|███████▉  | 1457/1825 [01:38<00:24, 14.83it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  80%|███████▉  | 1459/1825 [01:38<00:25, 14.28it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  80%|████████  | 1461/1825 [01:39<00:25, 14.31it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  80%|████████  | 1463/1825 [01:39<00:24, 14.50it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  80%|████████  | 1465/1825 [01:39<00:24, 14.71it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  80%|████████  | 1467/1825 [01:39<00:24, 14.42it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  80%|████████  | 1469/1825 [01:39<00:24, 14.52it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  81%|████████  | 1471/1825 [01:39<00:24, 14.36it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  81%|████████  | 1473/1825 [01:39<00:24, 14.48it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  81%|████████  | 1475/1825 [01:40<00:24, 14.01it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  81%|████████  | 1477/1825 [01:40<00:24, 14.39it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  81%|████████  | 1479/1825 [01:40<00:24, 14.28it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  81%|████████  | 1481/1825 [01:40<00:23, 14.49it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  81%|████████▏ | 1483/1825 [01:40<00:23, 14.75it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  81%|████████▏ | 1485/1825 [01:40<00:23, 14.67it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  81%|████████▏ | 1487/1825 [01:40<00:22, 14.90it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  82%|████████▏ | 1489/1825 [01:40<00:22, 15.04it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  82%|████████▏ | 1491/1825 [01:41<00:21, 15.25it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  82%|████████▏ | 1493/1825 [01:41<00:21, 15.32it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  82%|████████▏ | 1495/1825 [01:41<00:21, 15.27it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  82%|████████▏ | 1497/1825 [01:41<00:21, 15.27it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  82%|████████▏ | 1499/1825 [01:41<00:21, 15.03it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  82%|████████▏ | 1501/1825 [01:41<00:21, 15.15it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  82%|████████▏ | 1503/1825 [01:41<00:21, 15.09it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  82%|████████▏ | 1505/1825 [01:42<00:21, 15.12it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  83%|████████▎ | 1507/1825 [01:42<00:21, 14.67it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  83%|████████▎ | 1509/1825 [01:42<00:21, 14.54it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  83%|████████▎ | 1511/1825 [01:42<00:21, 14.71it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  83%|████████▎ | 1513/1825 [01:42<00:21, 14.36it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  83%|████████▎ | 1515/1825 [01:42<00:21, 14.60it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  83%|████████▎ | 1517/1825 [01:42<00:21, 14.58it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  83%|████████▎ | 1519/1825 [01:42<00:20, 14.74it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  83%|████████▎ | 1521/1825 [01:43<00:20, 14.91it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  83%|████████▎ | 1523/1825 [01:43<00:20, 14.75it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  84%|████████▎ | 1525/1825 [01:43<00:21, 14.20it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  84%|████████▎ | 1527/1825 [01:43<00:20, 14.51it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  84%|████████▍ | 1529/1825 [01:43<00:20, 14.29it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  84%|████████▍ | 1531/1825 [01:43<00:20, 14.41it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  84%|████████▍ | 1533/1825 [01:43<00:20, 14.60it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  84%|████████▍ | 1535/1825 [01:44<00:19, 14.95it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  84%|████████▍ | 1537/1825 [01:44<00:18, 15.16it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  84%|████████▍ | 1539/1825 [01:44<00:18, 15.27it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  84%|████████▍ | 1541/1825 [01:44<00:18, 15.30it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  85%|████████▍ | 1543/1825 [01:44<00:18, 15.48it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  85%|████████▍ | 1545/1825 [01:44<00:18, 15.41it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  85%|████████▍ | 1547/1825 [01:44<00:18, 15.37it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  85%|████████▍ | 1549/1825 [01:44<00:17, 15.34it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  85%|████████▍ | 1551/1825 [01:45<00:17, 15.33it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  85%|████████▌ | 1553/1825 [01:45<00:17, 15.33it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  85%|████████▌ | 1555/1825 [01:45<00:17, 15.18it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  85%|████████▌ | 1557/1825 [01:45<00:17, 15.19it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  85%|████████▌ | 1559/1825 [01:45<00:17, 15.08it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  86%|████████▌ | 1561/1825 [01:45<00:17, 15.01it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  86%|████████▌ | 1563/1825 [01:45<00:17, 15.07it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  86%|████████▌ | 1565/1825 [01:46<00:17, 15.15it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  86%|████████▌ | 1567/1825 [01:46<00:17, 15.15it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  86%|████████▌ | 1569/1825 [01:46<00:16, 15.08it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  86%|████████▌ | 1571/1825 [01:46<00:16, 14.98it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  86%|████████▌ | 1573/1825 [01:46<00:16, 15.13it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  86%|████████▋ | 1575/1825 [01:46<00:16, 15.32it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  86%|████████▋ | 1577/1825 [01:46<00:16, 15.42it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  87%|████████▋ | 1579/1825 [01:46<00:16, 15.32it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  87%|████████▋ | 1581/1825 [01:47<00:16, 15.06it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  87%|████████▋ | 1583/1825 [01:47<00:15, 15.20it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  87%|████████▋ | 1585/1825 [01:47<00:16, 14.89it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  87%|████████▋ | 1587/1825 [01:47<00:16, 14.79it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  87%|████████▋ | 1589/1825 [01:47<00:15, 14.98it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  87%|████████▋ | 1591/1825 [01:47<00:15, 14.99it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  87%|████████▋ | 1593/1825 [01:47<00:15, 14.82it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  87%|████████▋ | 1595/1825 [01:48<00:15, 14.91it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  88%|████████▊ | 1597/1825 [01:48<00:15, 14.87it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  88%|████████▊ | 1599/1825 [01:48<00:15, 14.88it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  88%|████████▊ | 1601/1825 [01:48<00:15, 14.80it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  88%|████████▊ | 1603/1825 [01:48<00:15, 14.62it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  88%|████████▊ | 1605/1825 [01:48<00:14, 14.92it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  88%|████████▊ | 1607/1825 [01:48<00:14, 15.06it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  88%|████████▊ | 1609/1825 [01:48<00:14, 15.18it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  88%|████████▊ | 1611/1825 [01:49<00:14, 14.96it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  88%|████████▊ | 1613/1825 [01:49<00:14, 14.75it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  88%|████████▊ | 1615/1825 [01:49<00:14, 14.85it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  89%|████████▊ | 1617/1825 [01:49<00:13, 15.10it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  89%|████████▊ | 1619/1825 [01:49<00:13, 15.20it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  89%|████████▉ | 1621/1825 [01:49<00:13, 15.31it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  89%|████████▉ | 1623/1825 [01:49<00:13, 15.41it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  89%|████████▉ | 1625/1825 [01:50<00:12, 15.47it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  89%|████████▉ | 1627/1825 [01:50<00:12, 15.45it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  89%|████████▉ | 1629/1825 [01:50<00:13, 14.79it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  89%|████████▉ | 1631/1825 [01:50<00:13, 14.73it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  89%|████████▉ | 1633/1825 [01:50<00:12, 14.98it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  90%|████████▉ | 1635/1825 [01:50<00:12, 14.89it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  90%|████████▉ | 1637/1825 [01:50<00:12, 15.01it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  90%|████████▉ | 1639/1825 [01:50<00:12, 14.77it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  90%|████████▉ | 1641/1825 [01:51<00:12, 14.46it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  90%|█████████ | 1643/1825 [01:51<00:12, 14.75it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  90%|█████████ | 1645/1825 [01:51<00:11, 15.01it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  90%|█████████ | 1647/1825 [01:51<00:11, 15.03it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  90%|█████████ | 1649/1825 [01:51<00:11, 15.14it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  90%|█████████ | 1651/1825 [01:51<00:11, 14.84it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  91%|█████████ | 1653/1825 [01:51<00:11, 14.62it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  91%|█████████ | 1655/1825 [01:52<00:11, 14.58it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  91%|█████████ | 1657/1825 [01:52<00:11, 14.63it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  91%|█████████ | 1659/1825 [01:52<00:11, 14.69it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  91%|█████████ | 1661/1825 [01:52<00:11, 14.82it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  91%|█████████ | 1663/1825 [01:52<00:10, 14.94it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  91%|█████████ | 1665/1825 [01:52<00:10, 14.91it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  91%|█████████▏| 1667/1825 [01:52<00:10, 14.98it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  91%|█████████▏| 1669/1825 [01:52<00:10, 15.13it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  92%|█████████▏| 1671/1825 [01:53<00:10, 14.48it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  92%|█████████▏| 1673/1825 [01:53<00:10, 14.32it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  92%|█████████▏| 1675/1825 [01:53<00:10, 14.34it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  92%|█████████▏| 1677/1825 [01:53<00:10, 14.75it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  92%|█████████▏| 1679/1825 [01:53<00:09, 14.84it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  92%|█████████▏| 1681/1825 [01:53<00:09, 14.76it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  92%|█████████▏| 1683/1825 [01:53<00:09, 14.81it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  92%|█████████▏| 1685/1825 [01:54<00:09, 14.98it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  92%|█████████▏| 1687/1825 [01:54<00:09, 14.76it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  93%|█████████▎| 1689/1825 [01:54<00:09, 14.72it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  93%|█████████▎| 1691/1825 [01:54<00:09, 14.89it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  93%|█████████▎| 1693/1825 [01:54<00:08, 14.76it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  93%|█████████▎| 1695/1825 [01:54<00:10, 12.94it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  93%|█████████▎| 1697/1825 [01:54<00:09, 13.57it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  93%|█████████▎| 1699/1825 [01:55<00:09, 13.75it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  93%|█████████▎| 1701/1825 [01:55<00:08, 14.12it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  93%|█████████▎| 1703/1825 [01:55<00:08, 13.84it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  93%|█████████▎| 1705/1825 [01:55<00:08, 13.96it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  94%|█████████▎| 1707/1825 [01:55<00:08, 13.85it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  94%|█████████▎| 1709/1825 [01:55<00:08, 14.29it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  94%|█████████▍| 1711/1825 [01:55<00:07, 14.65it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  94%|█████████▍| 1713/1825 [01:56<00:07, 14.99it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  94%|█████████▍| 1715/1825 [01:56<00:07, 15.03it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  94%|█████████▍| 1717/1825 [01:56<00:07, 14.45it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  94%|█████████▍| 1719/1825 [01:56<00:07, 14.57it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  94%|█████████▍| 1721/1825 [01:56<00:07, 14.46it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  94%|█████████▍| 1723/1825 [01:56<00:06, 14.66it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  95%|█████████▍| 1725/1825 [01:56<00:06, 14.57it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  95%|█████████▍| 1727/1825 [01:57<00:06, 14.72it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  95%|█████████▍| 1729/1825 [01:57<00:06, 14.90it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  95%|█████████▍| 1731/1825 [01:57<00:06, 14.72it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  95%|█████████▍| 1733/1825 [01:57<00:06, 14.92it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  95%|█████████▌| 1735/1825 [01:57<00:05, 15.06it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  95%|█████████▌| 1737/1825 [01:57<00:05, 15.17it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  95%|█████████▌| 1739/1825 [01:57<00:05, 14.95it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  95%|█████████▌| 1741/1825 [01:57<00:05, 15.07it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  96%|█████████▌| 1743/1825 [01:58<00:05, 14.63it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  96%|█████████▌| 1745/1825 [01:58<00:05, 14.77it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  96%|█████████▌| 1747/1825 [01:58<00:05, 14.73it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  96%|█████████▌| 1749/1825 [01:58<00:05, 14.80it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  96%|█████████▌| 1751/1825 [01:58<00:04, 14.97it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  96%|█████████▌| 1753/1825 [01:58<00:04, 14.89it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  96%|█████████▌| 1755/1825 [01:58<00:04, 14.58it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  96%|█████████▋| 1757/1825 [01:59<00:04, 14.77it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  96%|█████████▋| 1759/1825 [01:59<00:04, 14.87it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  96%|█████████▋| 1761/1825 [01:59<00:04, 14.53it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  97%|█████████▋| 1763/1825 [01:59<00:04, 14.37it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  97%|█████████▋| 1765/1825 [01:59<00:04, 14.74it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  97%|█████████▋| 1767/1825 [01:59<00:04, 14.42it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  97%|█████████▋| 1769/1825 [01:59<00:03, 14.19it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  97%|█████████▋| 1771/1825 [02:00<00:03, 14.07it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  97%|█████████▋| 1773/1825 [02:00<00:03, 14.19it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  97%|█████████▋| 1775/1825 [02:00<00:03, 13.83it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  97%|█████████▋| 1777/1825 [02:00<00:03, 14.22it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  97%|█████████▋| 1779/1825 [02:00<00:03, 14.45it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  98%|█████████▊| 1781/1825 [02:00<00:03, 14.62it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  98%|█████████▊| 1783/1825 [02:00<00:02, 14.09it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  98%|█████████▊| 1785/1825 [02:01<00:02, 14.23it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  98%|█████████▊| 1787/1825 [02:01<00:02, 14.40it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  98%|█████████▊| 1789/1825 [02:01<00:02, 14.38it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  98%|█████████▊| 1791/1825 [02:01<00:02, 14.47it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  98%|█████████▊| 1793/1825 [02:01<00:02, 14.60it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  98%|█████████▊| 1795/1825 [02:01<00:02, 14.58it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  98%|█████████▊| 1797/1825 [02:01<00:01, 14.13it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  99%|█████████▊| 1799/1825 [02:01<00:01, 14.24it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  99%|█████████▊| 1801/1825 [02:02<00:01, 13.88it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  99%|█████████▉| 1803/1825 [02:02<00:01, 14.32it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  99%|█████████▉| 1805/1825 [02:02<00:01, 14.22it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  99%|█████████▉| 1807/1825 [02:02<00:01, 14.50it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  99%|█████████▉| 1809/1825 [02:02<00:01, 14.90it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  99%|█████████▉| 1811/1825 [02:02<00:00, 14.98it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  99%|█████████▉| 1813/1825 [02:02<00:00, 14.50it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training:  99%|█████████▉| 1815/1825 [02:03<00:00, 14.43it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training: 100%|█████████▉| 1817/1825 [02:03<00:00, 14.55it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training: 100%|█████████▉| 1819/1825 [02:03<00:00, 14.29it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training: 100%|█████████▉| 1821/1825 [02:03<00:00, 14.59it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "Training: 100%|█████████▉| 1823/1825 [02:03<00:00, 14.52it/s]ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 4, features 2304\n",
            "ERROR:__main__:Error in training loop: Image features and image tokens do not match: tokens: 2, features 1152\n",
            "Training: 100%|██████████| 1825/1825 [02:03<00:00, 14.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model is saved.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoProcessor, LlavaForConditionalGeneration, BitsAndBytesConfig\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from PIL import Image\n",
        "import os\n",
        "import torchvision.transforms as transforms\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class LLaVADataset(Dataset):\n",
        "    def __init__(self, image_folder, processor, max_length=512, image_size=(336, 336)):\n",
        "        self.image_folder = image_folder\n",
        "        self.processor = processor\n",
        "        self.max_length = max_length\n",
        "        self.image_size = image_size\n",
        "        self.image_token = processor.tokenizer.convert_tokens_to_ids(\"<image>\")\n",
        "\n",
        "        # Validate and load image files\n",
        "        self.image_files = [f for f in os.listdir(image_folder)\n",
        "                          if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "        if not self.image_files:\n",
        "            raise ValueError(f\"No valid image files found in {image_folder}\")\n",
        "\n",
        "        logger.info(f\"Found {len(self.image_files)} valid image files\")\n",
        "\n",
        "        # Improved image transformation pipeline\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize(image_size, interpolation=transforms.InterpolationMode.BILINEAR),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                               std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_file = self.image_files[idx]\n",
        "        image_path = os.path.join(self.image_folder, image_file)\n",
        "\n",
        "        # Extract caption and preprocess\n",
        "        caption = os.path.splitext(image_file)[0].replace(\"_\", \" \").strip()\n",
        "        # Important: Do not add image token in the text - it's handled by the model\n",
        "        caption = caption\n",
        "\n",
        "        try:\n",
        "            # Image processing with error handling\n",
        "            image = Image.open(image_path).convert(\"RGB\")\n",
        "            if image.mode != \"RGB\":\n",
        "                image = image.convert(\"RGB\")\n",
        "            image_tensor = self.transform(image)\n",
        "\n",
        "            # Text processing with proper token handling\n",
        "            text_inputs = self.processor.tokenizer(\n",
        "                caption,\n",
        "                padding=\"max_length\",\n",
        "                truncation=True,\n",
        "                max_length=self.max_length - 1,  # Reserve space for image token\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "            input_ids = text_inputs[\"input_ids\"].squeeze(0)\n",
        "            attention_mask = text_inputs[\"attention_mask\"].squeeze(0)\n",
        "\n",
        "            # Append image token\n",
        "            input_ids = torch.cat([input_ids, torch.tensor([self.image_token])])\n",
        "            attention_mask = torch.cat([attention_mask, torch.tensor([1])])\n",
        "\n",
        "            return {\n",
        "                \"input_ids\": input_ids,\n",
        "                \"attention_mask\": attention_mask,\n",
        "                \"pixel_values\": image_tensor,\n",
        "                \"image_grid_thw\": torch.tensor([[1, 1, 1]], dtype=torch.int64)\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing {image_file}: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "def create_collate_fn(pad_token_id=0):\n",
        "    def collate_fn(batch):\n",
        "        # Remove None values from failed samples\n",
        "        batch = [b for b in batch if b is not None]\n",
        "        if not batch:\n",
        "            return None\n",
        "\n",
        "        # Prepare tensors\n",
        "        input_ids = pad_sequence([item[\"input_ids\"] for item in batch],\n",
        "                               batch_first=True,\n",
        "                               padding_value=pad_token_id)\n",
        "        attention_mask = pad_sequence([item[\"attention_mask\"] for item in batch],\n",
        "                                    batch_first=True,\n",
        "                                    padding_value=0)\n",
        "        pixel_values = torch.stack([item[\"pixel_values\"] for item in batch])\n",
        "        image_grid_thw = torch.stack([item[\"image_grid_thw\"] for item in batch])\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"pixel_values\": pixel_values,\n",
        "            \"image_grid_thw\": image_grid_thw\n",
        "        }\n",
        "    return collate_fn\n",
        "\n",
        "def setup_model_and_processor(model_name, token):\n",
        "    \"\"\"Setup model with optimized configuration\"\"\"\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\"  # Using nested float 4 for better precision\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        processor = AutoProcessor.from_pretrained(model_name, token=token)\n",
        "        model = LlavaForConditionalGeneration.from_pretrained(\n",
        "            model_name,\n",
        "            token=token,\n",
        "            quantization_config=bnb_config,\n",
        "            device_map=\"auto\",\n",
        "            torch_dtype=torch.float16\n",
        "        )\n",
        "        model.gradient_checkpointing_enable()\n",
        "\n",
        "        logger.info(\"Model and processor loaded successfully\")\n",
        "        return model, processor\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading model: {str(e)}\")\n",
        "        return None, None\n",
        "\n",
        "def main():\n",
        "    # Configuration\n",
        "    MODEL_NAME = \"llava-hf/llava-1.5-13b-hf\"\n",
        "    TOKEN = \"hf_token\"\n",
        "    IMAGE_FOLDER = \"/content/content/sample_data\"\n",
        "    BATCH_SIZE = 4\n",
        "    MAX_LENGTH = 512\n",
        "    IMAGE_SIZE = (336, 336)  # Make sure this matches the model's expected size\n",
        "\n",
        "    # Add PEFT/LoRA configuration\n",
        "    lora_config = LoraConfig(\n",
        "        r=16,\n",
        "        lora_alpha=32,\n",
        "        target_modules=[\"q_proj\", \"v_proj\"],\n",
        "        lora_dropout=0.1,\n",
        "        bias=\"none\",\n",
        "    )\n",
        "\n",
        "    # Setup\n",
        "    model, processor = setup_model_and_processor(MODEL_NAME, TOKEN)\n",
        "    if model is None or processor is None:\n",
        "        return\n",
        "\n",
        "    # Initialize dataset and dataloader\n",
        "    dataset = LLaVADataset(IMAGE_FOLDER, processor, MAX_LENGTH, IMAGE_SIZE)\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        collate_fn=create_collate_fn(processor.tokenizer.pad_token_id),\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    # Training loop example\n",
        "    model.train()\n",
        "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
        "        if batch is None:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Move batch to device\n",
        "            batch = {k: v.cuda() for k, v in batch.items()}\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss\n",
        "\n",
        "            # Backward pass and optimization steps would go here\n",
        "            # loss.backward()\n",
        "            # optimizer.step()\n",
        "            # optimizer.zero_grad()\n",
        "\n",
        "            logger.info(f\"Loss: {loss.item():.4f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in training loop: {str(e)}\")\n",
        "            continue\n",
        "    model.save_pretrained(\"/content/llava-finetuned\")\n",
        "    print(\"model is saved.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVLPVDNt8qTn",
        "outputId": "b17273c5-9315-44b6-a94c-e7d7a1f5f51e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📝 Generated Description:  Describe the scene in detail.\n"
          ]
        }
      ],
      "source": [
        "# Generate a description\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        **inputs,\n",
        "        max_length=1024,\n",
        "        do_sample=True,  # Allow sampling for diverse outputs\n",
        "        temperature=0.7,  # Adjust randomness\n",
        "        top_p=0.9  # Enable nucleus sampling\n",
        "    )\n",
        "\n",
        "# Decode and print the result\n",
        "description = processor.tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(\"\\n📝 Generated Description:\", description)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "bc3b8a41a31448a5b8cada089bbbf428",
            "4ffb79decbb1495e8b3170997535371f",
            "ebe48e28147e4068813f5e88c64eeb98",
            "8b956e7a497644d294d246530db33a91",
            "f359454a80be4630a31f22156046037e",
            "477bd4ffe2384ca78900ac18a7e07c66",
            "413edeff0876446d9d0e6de4c5d4286f",
            "6d64747b6cf94a6781dde91b87f593a4",
            "1f999bfdc60c489a9d38736ee0ddd2ba",
            "154e97b19e7c4cc2971157c239ebce20",
            "d4db570bdf614c2aa85786674a97db37"
          ]
        },
        "id": "r0AamRhY83FU",
        "outputId": "13ec8f80-c537-4edd-f7f5-9a8d3534cc6b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bc3b8a41a31448a5b8cada089bbbf428",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📝 Pretrained Model Description:  Describe the scene in detail.\n"
          ]
        }
      ],
      "source": [
        "pretrained_model = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/llava-1.5-13b-hf\", torch_dtype=torch.float16, device_map=\"auto\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = pretrained_model.generate(**inputs, max_length=1024)\n",
        "\n",
        "description = processor.tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(\"\\n📝 Pretrained Model Description:\", description)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "817af4ee28114961b8deec547b9d224a",
            "8cc8105b95544171ac67f752a42dcaaa",
            "133b8af5f5944565a5db3c68c302d806",
            "d2c2b1cfdd664e299e631cd29ade0104",
            "c7c2d2b92ddc4d218989ae3d78877fd2",
            "e312eb0a596749fbb9768a58c1d834b7",
            "1be5ea723a3f415eb6f01976a27203a7",
            "b3afd4668b204263a8ecbcef303206a3",
            "b46afe5aeab64e7b898766268f1bcaa9",
            "81fe15871bce4f23aadbeed7a2d52c71",
            "2f02d8fabe5845119148e48e3af3cdaa"
          ]
        },
        "id": "LWOb6TfuRair",
        "outputId": "30e6d5dc-7e47-431f-f046-bef799497cbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading base LLaVA model...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "817af4ee28114961b8deec547b9d224a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "LlavaForConditionalGeneration(\n",
              "  (vision_tower): CLIPVisionModel(\n",
              "    (vision_model): CLIPVisionTransformer(\n",
              "      (embeddings): CLIPVisionEmbeddings(\n",
              "        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
              "        (position_embedding): Embedding(577, 1024)\n",
              "      )\n",
              "      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (encoder): CLIPEncoder(\n",
              "        (layers): ModuleList(\n",
              "          (0-23): 24 x CLIPEncoderLayer(\n",
              "            (self_attn): CLIPSdpaAttention(\n",
              "              (k_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
              "              (v_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
              "              (q_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
              "              (out_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
              "            )\n",
              "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): CLIPMLP(\n",
              "              (activation_fn): QuickGELUActivation()\n",
              "              (fc1): Linear4bit(in_features=1024, out_features=4096, bias=True)\n",
              "              (fc2): Linear4bit(in_features=4096, out_features=1024, bias=True)\n",
              "            )\n",
              "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (multi_modal_projector): LlavaMultiModalProjector(\n",
              "    (linear_1): Linear4bit(in_features=1024, out_features=5120, bias=True)\n",
              "    (act): GELUActivation()\n",
              "    (linear_2): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
              "  )\n",
              "  (language_model): LlamaForCausalLM(\n",
              "    (model): LlamaModel(\n",
              "      (embed_tokens): Embedding(32064, 5120)\n",
              "      (layers): ModuleList(\n",
              "        (0-39): 40 x LlamaDecoderLayer(\n",
              "          (self_attn): LlamaAttention(\n",
              "            (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
              "            (k_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
              "            (v_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
              "            (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
              "          )\n",
              "          (mlp): LlamaMLP(\n",
              "            (gate_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
              "            (up_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
              "            (down_proj): Linear4bit(in_features=13824, out_features=5120, bias=False)\n",
              "            (act_fn): SiLU()\n",
              "          )\n",
              "          (input_layernorm): LlamaRMSNorm((5120,), eps=1e-05)\n",
              "          (post_attention_layernorm): LlamaRMSNorm((5120,), eps=1e-05)\n",
              "        )\n",
              "      )\n",
              "      (norm): LlamaRMSNorm((5120,), eps=1e-05)\n",
              "      (rotary_emb): LlamaRotaryEmbedding()\n",
              "    )\n",
              "    (lm_head): Linear(in_features=5120, out_features=32064, bias=False)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import json, re\n",
        "from transformers import AutoProcessor, LlavaForConditionalGeneration, BitsAndBytesConfig\n",
        "\n",
        "# Configure 4-bit quantization if needed\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# Load base LLaVA model (load only once)\n",
        "model_name = \"llava-hf/llava-1.5-13b-hf\"\n",
        "processor = AutoProcessor.from_pretrained(\n",
        "    model_name,\n",
        "    token=\"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\"\n",
        ")\n",
        "\n",
        "print(\"Loading base LLaVA model...\")\n",
        "model = LlavaForConditionalGeneration.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config,\n",
        ")\n",
        "model.eval()\n",
        "model.to(\"cuda\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "BpeE4JXNAEBK",
        "outputId": "e472869f-1442-4103-8677-3751ae123fd8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ngraph TD\\n    A[Input Image] --> B(LLaVA Visual Analysis)\\n    B --> C[Textual Description]\\n    C --> D{Keyword Extraction}\\n    D --> E[Base Score Calculation]\\n    C --> F[Contextual Understanding]\\n    F --> G[Context Score]\\n    E --> H[Score Aggregator]\\n    G --> H\\n    H --> I((Final Compliance Score))\\n    I --> J[Moderation Action]\\n\\n'"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "graph TD\n",
        "    A[Input Image] --> B(LLaVA Visual Analysis)\n",
        "    B --> C[Textual Description]\n",
        "    C --> D{Keyword Extraction}\n",
        "    D --> E[Base Score Calculation]\n",
        "    C --> F[Contextual Understanding]\n",
        "    F --> G[Context Score]\n",
        "    E --> H[Score Aggregator]\n",
        "    G --> H\n",
        "    H --> I((Final Compliance Score))\n",
        "    I --> J[Moderation Action]\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgzmyW2EAdRP"
      },
      "outputs": [],
      "source": [
        "from scipy.special import expit as sigmoid\n",
        "\n",
        "def calculate_compliance_score(description, category_rules):\n",
        "    \"\"\"Calculate dynamic compliance score using sigmoid thresholding\"\"\"\n",
        "    base_score = sum(\n",
        "        category['weights'].get(word, 0) * count\n",
        "        for word, count in analyze_keywords(description).items()\n",
        "        for category in category_rules.values()\n",
        "    )\n",
        "\n",
        "    context_score = llava_context_analysis(description)  # 0-1 scale\n",
        "    adjusted_score = base_score * sigmoid(5*(context_score - 0.65))  # α=5, τ=0.65\n",
        "    return min(100, max(0, int(adjusted_score * 100)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmSyt7AhAfU1"
      },
      "outputs": [],
      "source": [
        "def load_compliance_context(json_path):\n",
        "    with open(json_path) as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    categories = {}\n",
        "    for cat in data['subcategories']:\n",
        "        total_weight = sum(tag['weight'] for tag in cat['tags'])\n",
        "        categories[cat['name']] = {\n",
        "            'tags': {tag['name']: tag['weight']/total_weight for tag in cat['tags']},\n",
        "            'threshold': cat.get('threshold', 0.5)\n",
        "        }\n",
        "\n",
        "    return categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAP8LubcRiyk",
        "outputId": "b8b6a14c-9565-451f-e235-d9489ddd9a86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Compliance Context Summary:\n",
            "Content Compliance Framework:\n",
            "\n",
            "Generated Description:\n",
            " ### Instruction:\n",
            "You are a vision+text assistant.\n",
            "1) Describe the image in detail: objects, arrangement, color, usage, background details.\n",
            "2) Check for potential policy concerns (sensitive content, hate/violence, illegal, graphic) based on these guidelines: Content Compliance Framework:\n",
            "3) Important: End your response with a separate line exactly like: \"Compliance Score: XX/100\"\n",
            "   (where XX is a number from 0 to 100, with 0 meaning entirely safe and 100 extremely unsafe).\n",
            "\n",
            "### Input:\n",
            " \n",
            "\n",
            "### Response:\n",
            "\n",
            "In the image, a man is running down a street during a protest, with the smoke and tear gas filling the air. Several other people are scattered throughout the scene, some of them appearing to be protesters, and others seemingly observers. The man in the red and blue clothing appears to be in a hurry, possibly trying to escape the chaos happening around him.\n",
            "\n",
            "The presence of tear gas and smoke indicates that the situation is tense and potentially dangerous. The protesters may be exercising their right to express their opinions, but the potential for violence and unrest are evident in the scene.\n",
            "\n",
            "Compliance Score: 0/100\n",
            "\n",
            "Extracted Compliance Score: 0/100\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "def load_compliance_context(json_path):\n",
        "    # Open and load the JSON file from the provided path\n",
        "    with open(json_path, 'r') as f:\n",
        "        context = json.load(f)\n",
        "\n",
        "    # Extract the top-level name (e.g., \"Content Restrictions\")\n",
        "    top_name = context.get(\"name\", \"Compliance Guidelines\")\n",
        "\n",
        "    # Process each subcategory, listing its name and a sample of its tags\n",
        "    subcategories = context.get(\"subcategories\", [])\n",
        "    summary_parts = [f\"{top_name}:\"]\n",
        "\n",
        "    for subcat in subcategories:\n",
        "        cat_name = subcat.get(\"name\", \"Unnamed Category\")\n",
        "        # Include a few sample tags (first 3) for context; adjust as needed\n",
        "        sample_tags = \", \".join(subcat.get(\"tags\", [])[:3])\n",
        "        summary_parts.append(f\"{cat_name} (e.g., {sample_tags})\")\n",
        "\n",
        "    # Join the parts with a separator; you can use a comma or pipe depending on your preference\n",
        "    return \" | \".join(summary_parts)\n",
        "\n",
        "# Example usage:\n",
        "compliance_context = load_compliance_context(\"/content/compliance_rules.json\")\n",
        "print(\"Compliance Context Summary:\")\n",
        "print(compliance_context)\n",
        "\n",
        "# Define the function to generate the description\n",
        "def generate_image_description(image, compliance_context):\n",
        "    prompt = f\"\"\"### Instruction:\n",
        "You are a vision+text assistant.\n",
        "1) Describe the image in detail: objects, arrangement, color, usage, background details.\n",
        "2) Check for potential policy concerns (sensitive content, hate/violence, illegal, graphic) based on these guidelines: {compliance_context}\n",
        "3) Important: End your response with a separate line exactly like: \"Compliance Score: XX/100\"\n",
        "   (where XX is a number from 0 to 100, with 0 meaning entirely safe and 100 extremely unsafe).\n",
        "\n",
        "### Input:\n",
        "<image>\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "    inputs = processor(\n",
        "        images=image,\n",
        "        text=prompt,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generated_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=512,\n",
        "            do_sample=True,\n",
        "            temperature=0.7\n",
        "        )\n",
        "    output = processor.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "    return output\n",
        "\n",
        "# Post-processing: extract the compliance score from the generated description\n",
        "def extract_compliance_score(text):\n",
        "    match = re.search(r'Compliance Score:\\s*(\\d{1,3})/100', text)\n",
        "    if match:\n",
        "        return int(match.group(1))\n",
        "    return None\n",
        "\n",
        "# Example usage:\n",
        "image_path = \"/content/violant_image.jpg\"\n",
        "image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "description = generate_image_description(image, compliance_context)\n",
        "print(\"\\nGenerated Description:\\n\", description)\n",
        "\n",
        "score = extract_compliance_score(description)\n",
        "if score is not None:\n",
        "    print(f\"\\nExtracted Compliance Score: {score}/100\")\n",
        "else:\n",
        "    print(\"\\nCompliance Score not found in the generated output.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjFEWTXpJnIg"
      },
      "outputs": [],
      "source": [
        "# %%time\n",
        "# !pip install -q transformers accelerate torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105,
          "referenced_widgets": [
            "0768b0d7dd864bfabe5f12418dfcfe76",
            "ffa0fb1b67e3465884ad3f60f18ac13f",
            "ef3cc101cde64d5792edff060d849558",
            "e35686d576ff451ab13c236df9e2ba97",
            "384e1eaa7eac487f8c15a79137cebe54",
            "e6ba135bd754484d963f46368f9e7b6b",
            "d2ebf8f6dcc245b7b8a5e264ce17903c",
            "351a8622db5a40be9b01e1676f133405",
            "171ada5217eb4514b4f7b6342a06ebae",
            "5ee1eda0eff54491a024a5fcea46c87b",
            "623a38668f574f83953fde06cf584508"
          ]
        },
        "id": "xK1v7zcaT0qm",
        "outputId": "2778fb5d-25bd-49ac-8873-8c76d08a7f7f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading base LLaVA model...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0768b0d7dd864bfabe5f12418dfcfe76",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import LlavaForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\n",
        "\n",
        "def load_models():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # Configure 4-bit quantization\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "    )\n",
        "\n",
        "    model_name = \"llava-hf/llava-1.5-13b-hf\"\n",
        "    processor = AutoProcessor.from_pretrained(\n",
        "        model_name,\n",
        "        token=\"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\"\n",
        "    )\n",
        "\n",
        "    print(\"Loading base LLaVA model...\")\n",
        "    model = LlavaForConditionalGeneration.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=bnb_config,\n",
        "    )\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    return model, processor, device\n",
        "\n",
        "# Run once per session\n",
        "model, processor, device = load_models()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzbVUzi9BcNC"
      },
      "outputs": [],
      "source": [
        "# Content Moderation System Implementation\n",
        "import json\n",
        "import re\n",
        "from PIL import Image\n",
        "from scipy.special import expit as sigmoid\n",
        "from collections import defaultdict\n",
        "from functools import lru_cache\n",
        "import torch\n",
        "\n",
        "class ContentModerator:\n",
        "    def __init__(self, model, processor, rules_path=\"/content/compliance_rules.json\"):\n",
        "        \"\"\"Initialize Content Moderator with model, processor, and compliance rules.\"\"\"\n",
        "        self.model = model\n",
        "        self.processor = processor\n",
        "        print(f\"🔄 Loading compliance rules from: {rules_path} (Type: {type(rules_path)})\")  # Debug\n",
        "        self.categories = self._load_rules(rules_path)\n",
        "        self.alpha = 3.0  # Reduced alpha to soften sigmoid scaling\n",
        "        self.base_threshold = 0.65\n",
        "        print(f\"✅ Initialization model and processor (Type: {type(model)})\")  # Debug\n",
        "        print(f\"✅ Initialization Complete (Alpha: {self.alpha}, Base Threshold: {self.base_threshold})\")  # Debug\n",
        "\n",
        "    def _load_rules(self, json_path):\n",
        "        \"\"\"Load compliance categories and normalize weights.\"\"\"\n",
        "        with open(json_path) as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        print(f\"📂 JSON Data Loaded (Type: {type(data)})\")  # Debug\n",
        "        print(f\"🛠 Categories Found: {len(data['categories'])} (Type: {type(data['categories'])})\")  # Debug\n",
        "\n",
        "        categories = {}\n",
        "        for cat in data[\"categories\"]:\n",
        "            print(f\"➡ Processing Category: {cat['name']} (Type: {type(cat)})\")  # Debug\n",
        "            total = sum(t[\"weight\"] for t in cat[\"tags\"])  # Sum of all tag weights\n",
        "\n",
        "            print(f\"📊 Total Tag Weight for {cat['name']}: {total} (Type: {type(total)})\")  # Debug\n",
        "\n",
        "            categories[cat[\"name\"]] = {\n",
        "                \"tags\": {t[\"name\"]: t[\"weight\"] / total for t in cat[\"tags\"]},\n",
        "                \"threshold\": cat.get(\"threshold\", 0.5),\n",
        "                \"severity_scale\": cat.get(\"severity_scale\", 1.0)  # Apply severity scaling\n",
        "            }\n",
        "\n",
        "            print(f\"✔ Normalized Weights for {cat['name']}: {categories[cat['name']]['tags']} (Type: {type(categories[cat['name']]['tags'])})\")  # Debug\n",
        "\n",
        "        print(f\"🔍 Final Categories Structure: {categories} (Type: {type(categories)})\")  # Debug\n",
        "        return categories\n",
        "\n",
        "    @lru_cache(maxsize=1000)\n",
        "    def _analyze_text(self, text):\n",
        "        \"\"\"Analyze text for keyword matches and compute severity scores.\"\"\"\n",
        "        print(f\"📝 Text to Analyze: {text} (Type: {type(text)})\")  # Debug\n",
        "        text = text.lower()\n",
        "        matches = defaultdict(float)\n",
        "\n",
        "        print(f\"📊 Available Categories: {list(self.categories.keys())} (Type: {type(self.categories)})\")  # Debug\n",
        "\n",
        "        for cat, data in self.categories.items():\n",
        "            print(f\"🔎 Checking Category: {cat} (Type: {type(cat)})\")  # Debug\n",
        "            for kw, wt in data[\"tags\"].items():\n",
        "                if re.search(rf'\\b{re.escape(kw)}s?\\b', text):\n",
        "                    print(f\"✅ Matched Keyword: {kw} (Type: {type(kw)}) in Category: {cat}\")  # Debug\n",
        "                    matches[cat] += wt\n",
        "                    print(f\"📈 Updated Match Score for {cat}: {matches[cat]} (Type: {type(matches[cat])})\")  # Debug\n",
        "\n",
        "        print(f\"📌 Final Match Scores: {matches} (Type: {type(matches)})\")  # Debug\n",
        "        return dict(matches)\n",
        "\n",
        "    def _get_context_score(self, description):\n",
        "        \"\"\"Extract compliance score from LLaVA model output or estimate based on keyword density.\"\"\"\n",
        "        print(f\"📜 Model Output Description: {description} (Type: {type(description)})\")  # Debug\n",
        "        match = re.search(r'\\[Score:\\s*(\\d+)/100\\]', description)\n",
        "\n",
        "        if match:\n",
        "            score = int(match.group(1)) / 100\n",
        "            print(f\"🎯 Extracted Context Score: {score} (Type: {type(score)})\")  # Debug\n",
        "            return score\n",
        "\n",
        "        # If no explicit score, estimate based on keyword density\n",
        "        print(\"⚠️ No explicit score found in description. Estimating context score...\")\n",
        "        word_count = len(description.split())\n",
        "        keyword_density = len(self._analyze_text(description)) / max(word_count, 1)\n",
        "        estimated_score = min(1.0, max(0.5, keyword_density * 2))\n",
        "        print(f\"📊 Estimated Context Score: {estimated_score}\")\n",
        "        return estimated_score\n",
        "\n",
        "    def analyze(self, image_path):\n",
        "        \"\"\"Analyze an image for compliance scoring.\"\"\"\n",
        "        print(f\"🖼️ Processing Image: {image_path} (Type: {type(image_path)})\")  # Debug\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "        print(f\"✅ Image Loaded Successfully (Mode: {image.mode}, Size: {image.size})\")  # Debug\n",
        "\n",
        "        prompt = \"\"\" <image> Describe this image in detail. Include:\n",
        "- Objects present\n",
        "- Actions being performed\n",
        "- Clothing, expressions, and gestures\n",
        "- Background and scene description\n",
        "- Any elements that might relate to compliance rules \"\"\"\n",
        "\n",
        "        print(f\"📝 Generated Prompt: {prompt} (Type: {type(prompt)})\")  # Debug\n",
        "\n",
        "\n",
        "        inputs = self.processor(\n",
        "        images=image,\n",
        "        text=prompt,\n",
        "        return_tensors=\"pt\"\n",
        "        ).to(\"cuda\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            generated_ids = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=512,\n",
        "                do_sample=True,\n",
        "                temperature=0.7\n",
        "              )\n",
        "        desc = processor.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "\n",
        "        # # Generate description\n",
        "        # inputs = self.processor(\n",
        "        #     text=prompt,\n",
        "        #     images=image,\n",
        "        #     return_tensors=\"pt\"\n",
        "        # ).to(device, torch.float16)\n",
        "\n",
        "        # print(f\"🛠 Model Input Prepared (Keys: {inputs.keys()}, Type: {type(inputs)})\")  # Debug\n",
        "\n",
        "        # with torch.inference_mode():\n",
        "        #     out = self.model.generate(**inputs, max_new_tokens=512)\n",
        "        #     print(f\"🚀 Model Generated Output: {out} (Type: {type(out)}\")\n",
        "\n",
        "        # desc = self.processor.decode(out[0], skip_special_tokens=True)\n",
        "        #desc = processor.tokenizer.batch_decode(inputs, skip_special_tokens=True)[0]\n",
        "\n",
        "        print(f\"📜 Generated Description: {desc} (Type: {type(desc)})\")  # Debug\n",
        "\n",
        "        # Calculate compliance scores\n",
        "        ctx_score = self._get_context_score(desc)\n",
        "        matches = self._analyze_text(desc.lower())\n",
        "\n",
        "        print(f\"📊 Context Score: {ctx_score} (Type: {type(ctx_score)})\")  # Debug\n",
        "        print(f\"📌 Matched Keywords & Scores: {matches} (Type: {type(matches)})\")  # Debug\n",
        "\n",
        "        final_scores = {}\n",
        "        for cat, score in matches.items():\n",
        "            severity_scale = self.categories[cat][\"severity_scale\"]\n",
        "            print(f\"🔍 Category: {cat}, Score: {score}, Severity Scale: {severity_scale}\")  # Debug\n",
        "            adj = (score * severity_scale) * sigmoid(self.alpha * (ctx_score - self.base_threshold))\n",
        "            print(f\"📊 Adjusted Score for adj {cat}: {adj} (Type: {type(adj)}) {min(100, int(adj * 1000))}\")  # Debug\n",
        "            final_scores[cat] = min(100, int(adj * 1000))\n",
        "            print(f\"📈 Adjusted Score for vah {cat}: {final_scores[cat]} (Type: {type(final_scores[cat])})\")  # Debug\n",
        "\n",
        "        print(f\"📊 Final Scores: {final_scores} (Type: {type(final_scores)})\")  # Debug\n",
        "        overall_score = max(final_scores.values(), default=0)A\n",
        "        print(f\"🚨 Final Compliance Scores: {final_scores} (Type: {type(final_scores)})\")  # Debug\n",
        "        print(f\"🏆 Overall Risk Score: {overall_score} (Type: {type(overall_score)})\")  # Debug\n",
        "\n",
        "        return {\n",
        "            \"description\": desc,\n",
        "            \"scores\": final_scores,\n",
        "            \"context_score\": int(ctx_score * 100),\n",
        "            \"overall\": overall_score\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjjI3010BcQa",
        "outputId": "78c99367-928c-48d1-aa1a-dd31548aade5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔄 Loading compliance rules from: /content/compliance_rules.json (Type: <class 'str'>)\n",
            "📂 JSON Data Loaded (Type: <class 'dict'>)\n",
            "🛠 Categories Found: 2 (Type: <class 'list'>)\n",
            "➡ Processing Category: Sensitive Content (Type: <class 'dict'>)\n",
            "📊 Total Tag Weight for Sensitive Content: 53.83 (Type: <class 'float'>)\n",
            "✔ Normalized Weights for Sensitive Content: {'actions': 0.009288500835965075, 'acts': 0.009288500835965075, 'adult': 0.009288500835965075, 'alluring': 0.009288500835965075, 'analysis': 0.009288500835965075, 'anatomical': 0.009288500835965075, 'area': 0.009288500835965075, 'attitudes': 0.009288500835965075, 'bare': 0.009288500835965075, 'beach': 0.009288500835965075, 'body': 0.009288500835965075, 'boobs': 0.009288500835965075, 'connection': 0.009288500835965075, 'content': 0.009288500835965075, 'copulation': 0.009288500835965075, 'crude': 0.009288500835965075, 'cues': 0.009288500835965075, 'cultural': 0.009288500835965075, 'culturally': 0.009288500835965075, 'depictions': 0.009288500835965075, 'detection': 0.009288500835965075, 'emblems': 0.009288500835965075, 'entertainment': 0.009288500835965075, 'enticing': 0.009288500835965075, 'erotic': 0.009288500835965075, 'explicit': 0.016719301504737136, 'exposed': 0.009288500835965075, 'expressions': 0.009288500835965075, 'faith-based': 0.009288500835965075, 'feature': 0.009288500835965075, 'female': 0.009288500835965075, 'flirtatious': 0.009288500835965075, 'genital': 0.009288500835965075, 'genitalia': 0.016347761471298534, 'genitals': 0.009288500835965075, 'gestures': 0.009288500835965075, 'graphic': 0.009288500835965075, 'heritage': 0.009288500835965075, 'human': 0.009288500835965075, 'icons': 0.009288500835965075, 'identification': 0.009288500835965075, 'illustrations': 0.009288500835965075, 'imagery': 0.009288500835965075, 'insignias': 0.009288500835965075, 'intercourse': 0.009288500835965075, 'intimacy': 0.009288500835965075, 'intimate': 0.009288500835965075, 'language': 0.009288500835965075, 'logos': 0.009288500835965075, 'lovemaking': 0.009288500835965075, 'male': 0.009288500835965075, 'man': 0.009288500835965075, 'marks': 0.009288500835965075, 'mature': 0.009288500835965075, 'motions': 0.009288500835965075, 'movements': 0.009288500835965075, 'naked': 0.009288500835965075, 'nipple': 0.009288500835965075, 'nipples': 0.009288500835965075, 'nude': 0.015790451421140626, 'nudity': 0.009288500835965075, 'obscene': 0.009288500835965075, 'offensive': 0.009288500835965075, 'part': 0.009288500835965075, 'partial': 0.009288500835965075, 'penis': 0.009288500835965075, 'physical': 0.009288500835965075, 'porn': 0.009288500835965075, 'pornography': 0.017648151588333643, 'portrayals': 0.009288500835965075, 'poses': 0.009288500835965075, 'positions': 0.009288500835965075, 'postures': 0.009288500835965075, 'private': 0.009288500835965075, 'profane': 0.009288500835965075, 'provocative': 0.009288500835965075, 'recognition': 0.009288500835965075, 'region': 0.009288500835965075, 'remarks': 0.009288500835965075, 'representations': 0.009288500835965075, 'sacred': 0.009288500835965075, 'seductive': 0.009288500835965075, 'segmentation': 0.009288500835965075, 'sensual': 0.009288500835965075, 'sex': 0.009288500835965075, 'sexual': 0.009288500835965075, 'sexuality': 0.009288500835965075, 'signals': 0.009288500835965075, 'significant': 0.009288500835965075, 'speech': 0.009288500835965075, 'spiritual': 0.009288500835965075, 'stances': 0.009288500835965075, 'structure': 0.009288500835965075, 'suggestive': 0.009288500835965075, 'symbols': 0.009288500835965075, 'tempting': 0.009288500835965075, 'terminology': 0.009288500835965075, 'topless': 0.013932751253947613, 'tradition-sensitive': 0.009288500835965075, 'usage': 0.009288500835965075, 'visuals': 0.009288500835965075, 'vivid': 0.009288500835965075, 'vulgar': 0.009288500835965075, 'women': 0.009288500835965075} (Type: <class 'dict'>)\n",
            "➡ Processing Category: Hate Speech & Violence (Type: <class 'dict'>)\n",
            "📊 Total Tag Weight for Hate Speech & Violence: 39.36 (Type: <class 'float'>)\n",
            "✔ Normalized Weights for Hate Speech & Violence: {'abduction': 0.012703252032520325, 'abuse': 0.012703252032520325, 'aggressive': 0.012703252032520325, 'armed': 0.012703252032520325, 'assassination': 0.012703252032520325, 'assault': 0.012703252032520325, 'attack': 0.012703252032520325, 'battle': 0.012703252032520325, 'bigotry': 0.012703252032520325, 'bodily': 0.012703252032520325, 'bully': 0.012703252032520325, 'capturing': 0.012703252032520325, 'coercive': 0.012703252032520325, 'combat': 0.012703252032520325, 'confinement': 0.012703252032520325, 'conflict': 0.012703252032520325, 'confrontation': 0.012703252032520325, 'corporal': 0.012703252032520325, 'cutting': 0.012703252032520325, 'damage': 0.012703252032520325, 'declarations': 0.012703252032520325, 'discrimination': 0.012703252032520325, 'execution': 0.012703252032520325, 'extremism': 0.012703252032520325, 'fight': 0.012703252032520325, 'forcible': 0.012703252032520325, 'guerrilla': 0.012703252032520325, 'harass': 0.012703252032520325, 'harassment': 0.012703252032520325, 'harm': 0.012703252032520325, 'hate': 0.012703252032520325, 'homicide': 0.012703252032520325, 'homophobia': 0.012703252032520325, 'hostage-taking': 0.012703252032520325, 'hostile': 0.012703252032520325, 'hostilities': 0.012703252032520325, 'hurt': 0.012703252032520325, 'injury': 0.012703252032520325, 'insurgency': 0.012703252032520325, 'intimidate': 0.012703252032520325, 'intimidating': 0.012703252032520325, 'intimidation': 0.012703252032520325, 'kidnapping': 0.012703252032520325, 'killing': 0.012703252032520325, 'manslaughter': 0.012703252032520325, 'menacing': 0.012703252032520325, 'messages': 0.012703252032520325, 'militancy': 0.012703252032520325, 'military': 0.012703252032520325, 'misogyny': 0.012703252032520325, 'murder': 0.02464430894308943, 'physical': 0.012703252032520325, 'political': 0.012703252032520325, 'racism': 0.012703252032520325, 'radicalism': 0.012703252032520325, 'remarks': 0.012703252032520325, 'seizure': 0.012703252032520325, 'self_harm': 0.012703252032520325, 'severing': 0.012703252032520325, 'skirmish': 0.012703252032520325, 'slaying': 0.012703252032520325, 'slicing': 0.012703252032520325, 'snatching': 0.012703252032520325, 'statements': 0.012703252032520325, 'suicide': 0.012703252032520325, 'terrorism': 0.025152439024390245, 'threatening': 0.012703252032520325, 'threats': 0.012703252032520325, 'transphobia': 0.012703252032520325, 'trauma': 0.012703252032520325, 'violence': 0.012703252032520325, 'violent': 0.012703252032520325, 'warfare': 0.012703252032520325, 'warnings': 0.012703252032520325, 'wounding': 0.012703252032520325, 'xenophobia': 0.022865853658536585} (Type: <class 'dict'>)\n",
            "🔍 Final Categories Structure: {'Sensitive Content': {'tags': {'actions': 0.009288500835965075, 'acts': 0.009288500835965075, 'adult': 0.009288500835965075, 'alluring': 0.009288500835965075, 'analysis': 0.009288500835965075, 'anatomical': 0.009288500835965075, 'area': 0.009288500835965075, 'attitudes': 0.009288500835965075, 'bare': 0.009288500835965075, 'beach': 0.009288500835965075, 'body': 0.009288500835965075, 'boobs': 0.009288500835965075, 'connection': 0.009288500835965075, 'content': 0.009288500835965075, 'copulation': 0.009288500835965075, 'crude': 0.009288500835965075, 'cues': 0.009288500835965075, 'cultural': 0.009288500835965075, 'culturally': 0.009288500835965075, 'depictions': 0.009288500835965075, 'detection': 0.009288500835965075, 'emblems': 0.009288500835965075, 'entertainment': 0.009288500835965075, 'enticing': 0.009288500835965075, 'erotic': 0.009288500835965075, 'explicit': 0.016719301504737136, 'exposed': 0.009288500835965075, 'expressions': 0.009288500835965075, 'faith-based': 0.009288500835965075, 'feature': 0.009288500835965075, 'female': 0.009288500835965075, 'flirtatious': 0.009288500835965075, 'genital': 0.009288500835965075, 'genitalia': 0.016347761471298534, 'genitals': 0.009288500835965075, 'gestures': 0.009288500835965075, 'graphic': 0.009288500835965075, 'heritage': 0.009288500835965075, 'human': 0.009288500835965075, 'icons': 0.009288500835965075, 'identification': 0.009288500835965075, 'illustrations': 0.009288500835965075, 'imagery': 0.009288500835965075, 'insignias': 0.009288500835965075, 'intercourse': 0.009288500835965075, 'intimacy': 0.009288500835965075, 'intimate': 0.009288500835965075, 'language': 0.009288500835965075, 'logos': 0.009288500835965075, 'lovemaking': 0.009288500835965075, 'male': 0.009288500835965075, 'man': 0.009288500835965075, 'marks': 0.009288500835965075, 'mature': 0.009288500835965075, 'motions': 0.009288500835965075, 'movements': 0.009288500835965075, 'naked': 0.009288500835965075, 'nipple': 0.009288500835965075, 'nipples': 0.009288500835965075, 'nude': 0.015790451421140626, 'nudity': 0.009288500835965075, 'obscene': 0.009288500835965075, 'offensive': 0.009288500835965075, 'part': 0.009288500835965075, 'partial': 0.009288500835965075, 'penis': 0.009288500835965075, 'physical': 0.009288500835965075, 'porn': 0.009288500835965075, 'pornography': 0.017648151588333643, 'portrayals': 0.009288500835965075, 'poses': 0.009288500835965075, 'positions': 0.009288500835965075, 'postures': 0.009288500835965075, 'private': 0.009288500835965075, 'profane': 0.009288500835965075, 'provocative': 0.009288500835965075, 'recognition': 0.009288500835965075, 'region': 0.009288500835965075, 'remarks': 0.009288500835965075, 'representations': 0.009288500835965075, 'sacred': 0.009288500835965075, 'seductive': 0.009288500835965075, 'segmentation': 0.009288500835965075, 'sensual': 0.009288500835965075, 'sex': 0.009288500835965075, 'sexual': 0.009288500835965075, 'sexuality': 0.009288500835965075, 'signals': 0.009288500835965075, 'significant': 0.009288500835965075, 'speech': 0.009288500835965075, 'spiritual': 0.009288500835965075, 'stances': 0.009288500835965075, 'structure': 0.009288500835965075, 'suggestive': 0.009288500835965075, 'symbols': 0.009288500835965075, 'tempting': 0.009288500835965075, 'terminology': 0.009288500835965075, 'topless': 0.013932751253947613, 'tradition-sensitive': 0.009288500835965075, 'usage': 0.009288500835965075, 'visuals': 0.009288500835965075, 'vivid': 0.009288500835965075, 'vulgar': 0.009288500835965075, 'women': 0.009288500835965075}, 'threshold': 0.65, 'severity_scale': 1.2}, 'Hate Speech & Violence': {'tags': {'abduction': 0.012703252032520325, 'abuse': 0.012703252032520325, 'aggressive': 0.012703252032520325, 'armed': 0.012703252032520325, 'assassination': 0.012703252032520325, 'assault': 0.012703252032520325, 'attack': 0.012703252032520325, 'battle': 0.012703252032520325, 'bigotry': 0.012703252032520325, 'bodily': 0.012703252032520325, 'bully': 0.012703252032520325, 'capturing': 0.012703252032520325, 'coercive': 0.012703252032520325, 'combat': 0.012703252032520325, 'confinement': 0.012703252032520325, 'conflict': 0.012703252032520325, 'confrontation': 0.012703252032520325, 'corporal': 0.012703252032520325, 'cutting': 0.012703252032520325, 'damage': 0.012703252032520325, 'declarations': 0.012703252032520325, 'discrimination': 0.012703252032520325, 'execution': 0.012703252032520325, 'extremism': 0.012703252032520325, 'fight': 0.012703252032520325, 'forcible': 0.012703252032520325, 'guerrilla': 0.012703252032520325, 'harass': 0.012703252032520325, 'harassment': 0.012703252032520325, 'harm': 0.012703252032520325, 'hate': 0.012703252032520325, 'homicide': 0.012703252032520325, 'homophobia': 0.012703252032520325, 'hostage-taking': 0.012703252032520325, 'hostile': 0.012703252032520325, 'hostilities': 0.012703252032520325, 'hurt': 0.012703252032520325, 'injury': 0.012703252032520325, 'insurgency': 0.012703252032520325, 'intimidate': 0.012703252032520325, 'intimidating': 0.012703252032520325, 'intimidation': 0.012703252032520325, 'kidnapping': 0.012703252032520325, 'killing': 0.012703252032520325, 'manslaughter': 0.012703252032520325, 'menacing': 0.012703252032520325, 'messages': 0.012703252032520325, 'militancy': 0.012703252032520325, 'military': 0.012703252032520325, 'misogyny': 0.012703252032520325, 'murder': 0.02464430894308943, 'physical': 0.012703252032520325, 'political': 0.012703252032520325, 'racism': 0.012703252032520325, 'radicalism': 0.012703252032520325, 'remarks': 0.012703252032520325, 'seizure': 0.012703252032520325, 'self_harm': 0.012703252032520325, 'severing': 0.012703252032520325, 'skirmish': 0.012703252032520325, 'slaying': 0.012703252032520325, 'slicing': 0.012703252032520325, 'snatching': 0.012703252032520325, 'statements': 0.012703252032520325, 'suicide': 0.012703252032520325, 'terrorism': 0.025152439024390245, 'threatening': 0.012703252032520325, 'threats': 0.012703252032520325, 'transphobia': 0.012703252032520325, 'trauma': 0.012703252032520325, 'violence': 0.012703252032520325, 'violent': 0.012703252032520325, 'warfare': 0.012703252032520325, 'warnings': 0.012703252032520325, 'wounding': 0.012703252032520325, 'xenophobia': 0.022865853658536585}, 'threshold': 0.55, 'severity_scale': 1.4}} (Type: <class 'dict'>)\n",
            "✅ Initialization model and processor (Type: <class 'transformers.models.llava.modeling_llava.LlavaForConditionalGeneration'>)\n",
            "✅ Initialization Complete (Alpha: 3.0, Base Threshold: 0.65)\n",
            "🖼️ Processing Image: /content/HalfNaked_900.jpg (Type: <class 'str'>)\n",
            "✅ Image Loaded Successfully (Mode: RGB, Size: (900, 550))\n",
            "📝 Generated Prompt:  <image> Describe this image in detail. Include:\n",
            "- Objects present\n",
            "- Actions being performed\n",
            "- Clothing, expressions, and gestures\n",
            "- Background and scene description\n",
            "- Any elements that might relate to compliance rules  (Type: <class 'str'>)\n",
            "📜 Generated Description:    Describe this image in detail. Include:\n",
            "- Objects present\n",
            "- Actions being performed\n",
            "- Clothing, expressions, and gestures\n",
            "- Background and scene description\n",
            "- Any elements that might relate to compliance rules \n",
            "- Any specific rules to follow\n",
            "\n",
            "Three bottles of Naked beverages are featured in the image. (Type: <class 'str'>)\n",
            "📜 Model Output Description:    Describe this image in detail. Include:\n",
            "- Objects present\n",
            "- Actions being performed\n",
            "- Clothing, expressions, and gestures\n",
            "- Background and scene description\n",
            "- Any elements that might relate to compliance rules \n",
            "- Any specific rules to follow\n",
            "\n",
            "Three bottles of Naked beverages are featured in the image. (Type: <class 'str'>)\n",
            "⚠️ No explicit score found in description. Estimating context score...\n",
            "📝 Text to Analyze:    Describe this image in detail. Include:\n",
            "- Objects present\n",
            "- Actions being performed\n",
            "- Clothing, expressions, and gestures\n",
            "- Background and scene description\n",
            "- Any elements that might relate to compliance rules \n",
            "- Any specific rules to follow\n",
            "\n",
            "Three bottles of Naked beverages are featured in the image. (Type: <class 'str'>)\n",
            "📊 Available Categories: ['Sensitive Content', 'Hate Speech & Violence'] (Type: <class 'dict'>)\n",
            "🔎 Checking Category: Sensitive Content (Type: <class 'str'>)\n",
            "✅ Matched Keyword: actions (Type: <class 'str'>) in Category: Sensitive Content\n",
            "📈 Updated Match Score for Sensitive Content: 0.009288500835965075 (Type: <class 'float'>)\n",
            "✅ Matched Keyword: expressions (Type: <class 'str'>) in Category: Sensitive Content\n",
            "📈 Updated Match Score for Sensitive Content: 0.01857700167193015 (Type: <class 'float'>)\n",
            "✅ Matched Keyword: gestures (Type: <class 'str'>) in Category: Sensitive Content\n",
            "📈 Updated Match Score for Sensitive Content: 0.027865502507895226 (Type: <class 'float'>)\n",
            "✅ Matched Keyword: naked (Type: <class 'str'>) in Category: Sensitive Content\n",
            "📈 Updated Match Score for Sensitive Content: 0.0371540033438603 (Type: <class 'float'>)\n",
            "🔎 Checking Category: Hate Speech & Violence (Type: <class 'str'>)\n",
            "📌 Final Match Scores: defaultdict(<class 'float'>, {'Sensitive Content': 0.0371540033438603}) (Type: <class 'collections.defaultdict'>)\n",
            "📊 Estimated Context Score: 0.5\n",
            "📝 Text to Analyze:    describe this image in detail. include:\n",
            "- objects present\n",
            "- actions being performed\n",
            "- clothing, expressions, and gestures\n",
            "- background and scene description\n",
            "- any elements that might relate to compliance rules \n",
            "- any specific rules to follow\n",
            "\n",
            "three bottles of naked beverages are featured in the image. (Type: <class 'str'>)\n",
            "📊 Available Categories: ['Sensitive Content', 'Hate Speech & Violence'] (Type: <class 'dict'>)\n",
            "🔎 Checking Category: Sensitive Content (Type: <class 'str'>)\n",
            "✅ Matched Keyword: actions (Type: <class 'str'>) in Category: Sensitive Content\n",
            "📈 Updated Match Score for Sensitive Content: 0.009288500835965075 (Type: <class 'float'>)\n",
            "✅ Matched Keyword: expressions (Type: <class 'str'>) in Category: Sensitive Content\n",
            "📈 Updated Match Score for Sensitive Content: 0.01857700167193015 (Type: <class 'float'>)\n",
            "✅ Matched Keyword: gestures (Type: <class 'str'>) in Category: Sensitive Content\n",
            "📈 Updated Match Score for Sensitive Content: 0.027865502507895226 (Type: <class 'float'>)\n",
            "✅ Matched Keyword: naked (Type: <class 'str'>) in Category: Sensitive Content\n",
            "📈 Updated Match Score for Sensitive Content: 0.0371540033438603 (Type: <class 'float'>)\n",
            "🔎 Checking Category: Hate Speech & Violence (Type: <class 'str'>)\n",
            "📌 Final Match Scores: defaultdict(<class 'float'>, {'Sensitive Content': 0.0371540033438603}) (Type: <class 'collections.defaultdict'>)\n",
            "📊 Context Score: 0.5 (Type: <class 'float'>)\n",
            "📌 Matched Keywords & Scores: {'Sensitive Content': 0.0371540033438603} (Type: <class 'dict'>)\n",
            "🔍 Category: Sensitive Content, Score: 0.0371540033438603, Severity Scale: 1.2\n",
            "📊 Adjusted Score for adj Sensitive Content: 0.017359573444582336 (Type: <class 'numpy.float64'>) 17\n",
            "📈 Adjusted Score for vah Sensitive Content: 17 (Type: <class 'int'>)\n",
            "📊 Final Scores: {'Sensitive Content': 17} (Type: <class 'dict'>)\n",
            "🚨 Final Compliance Scores: {'Sensitive Content': 17} (Type: <class 'dict'>)\n",
            "🏆 Overall Risk Score: 17 (Type: <class 'int'>)\n",
            "results :  {'description': '   Describe this image in detail. Include:\\n- Objects present\\n- Actions being performed\\n- Clothing, expressions, and gestures\\n- Background and scene description\\n- Any elements that might relate to compliance rules \\n- Any specific rules to follow\\n\\nThree bottles of Naked beverages are featured in the image.', 'scores': {'Sensitive Content': 17}, 'context_score': 50, 'overall': 17}\n",
            "🖼️ Image Analysis:\n",
            "   Describe this image in detail. Include:\n",
            "- Objects present\n",
            "- Actions being performed\n",
            "- Clothing, expressions, and gestures\n",
            "- Background and scene description\n",
            "- Any elements that might relate to compliance rules \n",
            "- Any specific rules to follow\n",
            "\n",
            "Three bottles of Naked beverages are featured in the image.\n",
            "\n",
            "🔍 Compliance Scores:\n",
            "- Sensitive Content: 17/100\n",
            "\n",
            "🚨 Overall Risk: 17/100 (Context: 50/100)\n"
          ]
        }
      ],
      "source": [
        "# Cell 3: Usage Example\n",
        "# Initialize once per session\n",
        "moderator = ContentModerator(model, processor)\n",
        "\n",
        "# Analyze image\n",
        "results = moderator.analyze(\"/content/HalfNaked_900.jpg\")\n",
        "# results = moderator.analyze(\"/content/adult_image.jpg\")\n",
        "# results = moderator.analyze(\"/content/boobs_papa.jpg\")\n",
        "# results = moderator.analyze(\"/content/javhd-157.jpg\")\n",
        "\n",
        "print(\"results : \", results)\n",
        "print(\"🖼️ Image Analysis:\")\n",
        "print(results[\"description\"])\n",
        "print(\"\\n🔍 Compliance Scores:\")\n",
        "for cat, score in results[\"scores\"].items():\n",
        "    print(f\"- {cat}: {score}/100\")\n",
        "print(f\"\\n🚨 Overall Risk: {results['overall']}/100 (Context: {results['context_score']}/100)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "44na6mvhBcS4"
      },
      "outputs": [],
      "source": [
        "# pip install scikit-learn numpy pandas nltk spacy sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3elSZ0QjSFb0",
        "outputId": "e2d858a8-4953-433a-e60b-5475b1ab09bc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🔍 DEBUGGING TRACE STARTED\n",
            "\n",
            "\n",
            "🔹 Checking Compliance for: In the image, a man and a woman are lying on a bed and engaging in a lustful act. The woman is straddling the man while getting intimate with him. The scene is quite provocative, as both individuals appear to be enjoying their time together. The woman appears to be fully naked, and the man's intentions are clear as they both participate in their erotic pursuits.\n",
            "\n",
            "📌 Rule-Based Filtering: Found ['adult_content'] in text: In the image, a man and a woman are lying on a bed and engaging in a lustful act. The woman is straddling the man while getting intimate with him. The scene is quite provocative, as both individuals appear to be enjoying their time together. The woman appears to be fully naked, and the man's intentions are clear as they both participate in their erotic pursuits.\n",
            "📌 Transformer Model Classification: Safe for text: In the image, a man and a woman are lying on a bed and engaging in a lustful act. The woman is straddling the man while getting intimate with him. The scene is quite provocative, as both individuals appear to be enjoying their time together. The woman appears to be fully naked, and the man's intentions are clear as they both participate in their erotic pursuits.\n",
            "📌 Key Terms Extracted (Stop words removed): ['image', ',', 'man', 'woman', 'lying', 'bed', 'engaging', 'lustful', 'act', '.', 'woman', 'straddling', 'man', 'getting', 'intimate', '.', 'scene', 'quite', 'provocative', ',', 'individuals', 'appear', 'enjoying', 'time', 'together', '.', 'woman', 'appears', 'fully', 'naked', ',', 'man', \"'s\", 'intentions', 'clear', 'participate', 'erotic', 'pursuits', '.']\n",
            "📌 Named Entities Detected: []\n",
            "📌 Semantic Similarity Check: In the image, a man and a woman are lying on a bed and engaging in a lustful act. The woman is straddling the man while getting intimate with him. The scene is quite provocative, as both individuals appear to be enjoying their time together. The woman appears to be fully naked, and the man's intentions are clear as they both participate in their erotic pursuits. vs Assume -> Score: 0.13294166326522827\n",
            "📌 Semantic Similarity Check: In the image, a man and a woman are lying on a bed and engaging in a lustful act. The woman is straddling the man while getting intimate with him. The scene is quite provocative, as both individuals appear to be enjoying their time together. The woman appears to be fully naked, and the man's intentions are clear as they both participate in their erotic pursuits. vs Bass Guitar -> Score: 0.09547711163759232\n",
            "📌 Semantic Similarity Check: In the image, a man and a woman are lying on a bed and engaging in a lustful act. The woman is straddling the man while getting intimate with him. The scene is quite provocative, as both individuals appear to be enjoying their time together. The woman appears to be fully naked, and the man's intentions are clear as they both participate in their erotic pursuits. vs Bare Minerals -> Score: 0.1587095558643341\n",
            "🔹 Final Decision: Violation - Content classified as: ['adult_content'].\n",
            "\n",
            "\n",
            "🔹 Checking Compliance for: Three bottles of Naked beverages are featured in the image.\n",
            "\n",
            "📌 Rule-Based Filtering: Found [] in text: Three bottles of Naked beverages are featured in the image.\n",
            "📌 Key Terms Extracted (Stop words removed): ['three', 'bottles', 'naked', 'beverages', 'featured', 'image', '.']\n",
            "📌 Named Entities Detected: ['Three']\n",
            "📌 Semantic Similarity Check: Three bottles of Naked beverages are featured in the image. vs Assume -> Score: 0.042056165635585785\n",
            "📌 Semantic Similarity Check: Three bottles of Naked beverages are featured in the image. vs Bass Guitar -> Score: 0.039606597274541855\n",
            "📌 Semantic Similarity Check: Three bottles of Naked beverages are featured in the image. vs Bare Minerals -> Score: 0.18799862265586853\n",
            "🔹 Final Decision: Safe - \n",
            "\n",
            "\n",
            "🔹 Checking Compliance for: The image shows three bottles of a green beverage - Naked Half Naked - in three different colors.\n",
            "\n",
            "📌 Rule-Based Filtering: Found [] in text: The image shows three bottles of a green beverage - Naked Half Naked - in three different colors.\n",
            "📌 Key Terms Extracted (Stop words removed): ['image', 'shows', 'three', 'bottles', 'green', 'beverage', '-', 'naked', 'half', 'naked', '-', 'three', 'different', 'colors', '.']\n",
            "📌 Named Entities Detected: ['three', 'three']\n",
            "📌 Semantic Similarity Check: The image shows three bottles of a green beverage - Naked Half Naked - in three different colors. vs Assume -> Score: 0.101212278008461\n",
            "📌 Semantic Similarity Check: The image shows three bottles of a green beverage - Naked Half Naked - in three different colors. vs Bass Guitar -> Score: 0.07878946512937546\n",
            "📌 Semantic Similarity Check: The image shows three bottles of a green beverage - Naked Half Naked - in three different colors. vs Bare Minerals -> Score: 0.22803929448127747\n",
            "🔹 Final Decision: Safe - \n",
            "\n",
            "\n",
            "🔹 Checking Compliance for: I wanna fuck you up dog ass hole mother fuckr bitch I will fuck you up.\n",
            "\n",
            "📌 Rule-Based Filtering: Found ['harsh_language'] in text: I wanna fuck you up dog ass hole mother fuckr bitch I will fuck you up.\n",
            "📌 Transformer Model Classification: Hate Speech for text: I wanna fuck you up dog ass hole mother fuckr bitch I will fuck you up.\n",
            "📌 Key Terms Extracted (Stop words removed): ['wan', 'na', 'fuck', 'dog', 'ass', 'hole', 'mother', 'fuckr', 'bitch', 'fuck', '.']\n",
            "📌 Named Entities Detected: []\n",
            "📌 Semantic Similarity Check: I wanna fuck you up dog ass hole mother fuckr bitch I will fuck you up. vs Assume -> Score: 0.14594250917434692\n",
            "📌 Semantic Similarity Check: I wanna fuck you up dog ass hole mother fuckr bitch I will fuck you up. vs Bass Guitar -> Score: 0.08358348906040192\n",
            "📌 Semantic Similarity Check: I wanna fuck you up dog ass hole mother fuckr bitch I will fuck you up. vs Bare Minerals -> Score: 0.034444235265254974\n",
            "🔹 Final Decision: Violation - Content classified as: ['harsh_language'].\n",
            "\n",
            "Debugging Compliance Model\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<table id=\"itables_8e195973_3045_4ff0_8c82_09127f870753\" class=\"display nowrap\" data-quarto-disable-processing=\"true\" style=\"table-layout:auto;width:auto;margin:auto;caption-side:bottom\">\n",
              "<thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      \n",
              "      <th>text</th>\n",
              "      <th>rule_based_flags</th>\n",
              "      <th>ml_classification</th>\n",
              "      <th>context_safe</th>\n",
              "      <th>semantic_safe</th>\n",
              "      <th>final_decision</th>\n",
              "      <th>explanation</th>\n",
              "    </tr>\n",
              "  </thead><tbody><tr>\n",
              "<td style=\"vertical-align:middle; text-align:left\">\n",
              "<a href=https://mwouts.github.io/itables/><svg class=\"main-svg\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\n",
              "width=\"64\" viewBox=\"0 0 500 400\" style=\"font-family: 'Droid Sans', sans-serif;\">\n",
              "    <g style=\"fill:#d9d7fc\">\n",
              "        <path d=\"M100,400H500V357H100Z\" />\n",
              "        <path d=\"M100,300H400V257H100Z\" />\n",
              "        <path d=\"M0,200H400V157H0Z\" />\n",
              "        <path d=\"M100,100H500V57H100Z\" />\n",
              "        <path d=\"M100,350H500V307H100Z\" />\n",
              "        <path d=\"M100,250H400V207H100Z\" />\n",
              "        <path d=\"M0,150H400V107H0Z\" />\n",
              "        <path d=\"M100,50H500V7H100Z\" />\n",
              "    </g>\n",
              "    <g style=\"fill:#1a1366;stroke:#1a1366;\">\n",
              "   <rect x=\"100\" y=\"7\" width=\"400\" height=\"43\">\n",
              "    <animate\n",
              "      attributeName=\"width\"\n",
              "      values=\"0;400;0\"\n",
              "      dur=\"5s\"\n",
              "      repeatCount=\"indefinite\" />\n",
              "      <animate\n",
              "      attributeName=\"x\"\n",
              "      values=\"100;100;500\"\n",
              "      dur=\"5s\"\n",
              "      repeatCount=\"indefinite\" />\n",
              "  </rect>\n",
              "        <rect x=\"0\" y=\"107\" width=\"400\" height=\"43\">\n",
              "    <animate\n",
              "      attributeName=\"width\"\n",
              "      values=\"0;400;0\"\n",
              "      dur=\"3.5s\"\n",
              "      repeatCount=\"indefinite\" />\n",
              "    <animate\n",
              "      attributeName=\"x\"\n",
              "      values=\"0;0;400\"\n",
              "      dur=\"3.5s\"\n",
              "      repeatCount=\"indefinite\" />\n",
              "  </rect>\n",
              "        <rect x=\"100\" y=\"207\" width=\"300\" height=\"43\">\n",
              "    <animate\n",
              "      attributeName=\"width\"\n",
              "      values=\"0;300;0\"\n",
              "      dur=\"3s\"\n",
              "      repeatCount=\"indefinite\" />\n",
              "    <animate\n",
              "      attributeName=\"x\"\n",
              "      values=\"100;100;400\"\n",
              "      dur=\"3s\"\n",
              "      repeatCount=\"indefinite\" />\n",
              "  </rect>\n",
              "        <rect x=\"100\" y=\"307\" width=\"400\" height=\"43\">\n",
              "    <animate\n",
              "      attributeName=\"width\"\n",
              "      values=\"0;400;0\"\n",
              "      dur=\"4s\"\n",
              "      repeatCount=\"indefinite\" />\n",
              "      <animate\n",
              "      attributeName=\"x\"\n",
              "      values=\"100;100;500\"\n",
              "      dur=\"4s\"\n",
              "      repeatCount=\"indefinite\" />\n",
              "  </rect>\n",
              "        <g style=\"fill:transparent;stroke-width:8; stroke-linejoin:round\" rx=\"5\">\n",
              "            <g transform=\"translate(45 50) rotate(-45)\">\n",
              "                <circle r=\"33\" cx=\"0\" cy=\"0\" />\n",
              "                <rect x=\"-8\" y=\"32\" width=\"16\" height=\"30\" />\n",
              "            </g>\n",
              "\n",
              "            <g transform=\"translate(450 152)\">\n",
              "                <polyline points=\"-15,-20 -35,-20 -35,40 25,40 25,20\" />\n",
              "                <rect x=\"-15\" y=\"-40\" width=\"60\" height=\"60\" />\n",
              "            </g>\n",
              "\n",
              "            <g transform=\"translate(50 352)\">\n",
              "                <polygon points=\"-35,-5 0,-40 35,-5\" />\n",
              "                <polygon points=\"-35,10 0,45 35,10\" />\n",
              "            </g>\n",
              "\n",
              "            <g transform=\"translate(75 250)\">\n",
              "                <polyline points=\"-30,30 -60,0 -30,-30\" />\n",
              "                <polyline points=\"0,30 -30,0 0,-30\" />\n",
              "            </g>\n",
              "\n",
              "            <g transform=\"translate(425 250) rotate(180)\">\n",
              "                <polyline points=\"-30,30 -60,0 -30,-30\" />\n",
              "                <polyline points=\"0,30 -30,0 0,-30\" />\n",
              "            </g>\n",
              "        </g>\n",
              "    </g>\n",
              "</svg>\n",
              "</a>\n",
              "Loading ITables v2.2.5 from the internet...\n",
              "(need <a href=https://mwouts.github.io/itables/troubleshooting.html>help</a>?)</td>\n",
              "</tr></tbody>\n",
              "</table>\n",
              "<link href=\"https://www.unpkg.com/dt_for_itables@2.0.13/dt_bundle.css\" rel=\"stylesheet\">\n",
              "<script type=\"module\">\n",
              "    import {DataTable, jQuery as $} from 'https://www.unpkg.com/dt_for_itables@2.0.13/dt_bundle.js';\n",
              "\n",
              "    document.querySelectorAll(\"#itables_8e195973_3045_4ff0_8c82_09127f870753:not(.dataTable)\").forEach(table => {\n",
              "        if (!(table instanceof HTMLTableElement))\n",
              "            return;\n",
              "\n",
              "        // Define the table data\n",
              "        const data = [[\"In the image, a man and a woman are lying on a bed and engaging in a lustful act. The woman is straddling the man while getting intimate with him. The scene is quite provocative, as both individuals appear to be enjoying their time together. The woman appears to be fully naked, and the man's intentions are clear as they both participate in their erotic pursuits.\", \"[adult_content]\", \"Safe\", false, false, \"Violation\", \"Content classified as: ['adult_content'].\"], [\"Three bottles of Naked beverages are featured in the image.\", \"[]\", \"Safe\", false, false, \"Safe\", \"\"], [\"The image shows three bottles of a green beverage - Naked Half Naked - in three different colors.\", \"[]\", \"Safe\", false, false, \"Safe\", \"\"], [\"I wanna fuck you up dog ass hole mother fuckr bitch I will fuck you up.\", \"[harsh_language]\", \"Hate Speech\", false, false, \"Violation\", \"Content classified as: ['harsh_language'].\"]];\n",
              "\n",
              "        // Define the dt_args\n",
              "        let dt_args = {\"layout\": {\"topStart\": null, \"topEnd\": null, \"bottomStart\": null, \"bottomEnd\": null}, \"order\": [], \"warn_on_selected_rows_not_rendered\": true};\n",
              "        dt_args[\"data\"] = data;\n",
              "\n",
              "        \n",
              "        new DataTable(table, dt_args);\n",
              "    });\n",
              "</script>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✅ DEBUGGING TRACE COMPLETED\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import json\n",
        "import torch\n",
        "import spacy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from typing import Dict, List\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "# Load pre-trained models\n",
        "nlp = spacy.load(\"en_core_web_sm\")  # Named Entity Recognition (NER)\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # Semantic Similarity Model\n",
        "\n",
        "# Load transformer-based classifier for hate speech detection\n",
        "MODEL_NAME = \"facebook/roberta-hate-speech-dynabench-r4-target\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Define Compliance Categories\n",
        "CATEGORIES = {\n",
        "    \"adult_content\": [\"explicit\", \"nudity\", \"sexual\", \"pornographic\"],\n",
        "    \"harsh_language\": [\"profanity\", \"swearing\", \"abuse\"],\n",
        "    \"child_abuse\": [\"minor exploitation\", \"grooming\"],\n",
        "    \"hate_speech\": [\"racism\", \"sexism\", \"homophobia\"],\n",
        "    \"sensitive_info\": [\"leaked data\", \"PII exposure\"]\n",
        "}\n",
        "\n",
        "# Define Rule-Based Filtering\n",
        "RULES = {\n",
        "    \"adult_content\": r\"\\b(nude|porn|erotic|sex|lust|intimate|strip|orgasm|fetish)\\b\",\n",
        "    \"harsh_language\": r\"\\b(fuck|shit|bitch|bastard|cunt|asshole|dickhead)\\b\",\n",
        "    \"child_abuse\": r\"\\b(minor|underage|child porn|kid exploitation|pedo|grooming)\\b\",\n",
        "    \"hate_speech\": r\"\\b(nazi|white power|kkk|lynch|terrorist|homophobic|slur)\\b\",\n",
        "    \"sensitive_info\": r\"\\b(\\d{3}-\\d{2}-\\d{4}|\\d{16}|\\d{4}-\\d{4}-\\d{4}-\\d{4})\\b\"\n",
        "}\n",
        "\n",
        "# Dynamic Allow-List (initialized with known safe terms)\n",
        "dynamic_allow_list = set([\"Bare Minerals\", \"Assume\", \"Bass Guitar\" ])\n",
        "\n",
        "# Stop Words for Filtering\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "# Debugging Print Statement\n",
        "print(\"\\n🔍 DEBUGGING TRACE STARTED\\n\")\n",
        "\n",
        "# Function to Apply Rule-Based Filtering\n",
        "def apply_rule_based_filter(text: str) -> List[str]:\n",
        "    \"\"\"Applies regex-based filtering for quick classification.\"\"\"\n",
        "    flagged_categories = []\n",
        "    for category, pattern in RULES.items():\n",
        "        if re.search(pattern, text, re.IGNORECASE):\n",
        "            flagged_categories.append(category)\n",
        "\n",
        "    print(f\"📌 Rule-Based Filtering: Found {flagged_categories} in text: {text}\")\n",
        "    return flagged_categories\n",
        "\n",
        "# Function to Perform Text Classification using a Transformer Model\n",
        "def classify_with_transformer(text: str) -> str:\n",
        "    \"\"\"Uses a transformer model to classify text into safe or non-compliant categories.\"\"\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    outputs = model(**inputs)\n",
        "    prediction = torch.argmax(outputs.logits, dim=-1).item()\n",
        "\n",
        "    classification = [\"Safe\", \"Hate Speech\"][prediction]\n",
        "    print(f\"📌 Transformer Model Classification: {classification} for text: {text}\")\n",
        "    return classification\n",
        "\n",
        "# Function to Extract Key Terms After Stop Word Removal\n",
        "def extract_key_terms(text: str) -> List[str]:\n",
        "    \"\"\"Removes stop words and extracts key terms for context analysis.\"\"\"\n",
        "    words = word_tokenize(text)\n",
        "    key_terms = [word.lower() for word in words if word.lower() not in stop_words]\n",
        "    print(f\"📌 Key Terms Extracted (Stop words removed): {key_terms}\")\n",
        "    return key_terms\n",
        "\n",
        "# Function to Check Context Using Named Entity Recognition (NER)\n",
        "def check_context(text: str) -> bool:\n",
        "    \"\"\"Uses NER to determine if flagged words appear in a benign context.\"\"\"\n",
        "    doc = nlp(text)\n",
        "    named_entities = [ent.text for ent in doc.ents]\n",
        "\n",
        "    print(f\"📌 Named Entities Detected: {named_entities}\")\n",
        "\n",
        "    # If a named entity is in the dynamic allow-list, it's likely safe\n",
        "    for entity in named_entities:\n",
        "        if entity in dynamic_allow_list:\n",
        "            print(f\"✅ Context Safe: {entity} is in allow-list.\")\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "# Function to Calculate Semantic Similarity\n",
        "def is_contextually_safe(text: str, threshold: float = 0.75) -> bool:\n",
        "    \"\"\"Uses word embeddings to check if the text is similar to known benign terms.\"\"\"\n",
        "    embeddings_text = embedding_model.encode(text, convert_to_tensor=True)\n",
        "\n",
        "    for safe_term in dynamic_allow_list:\n",
        "        embeddings_safe_term = embedding_model.encode(safe_term, convert_to_tensor=True)\n",
        "        similarity_score = util.pytorch_cos_sim(embeddings_text, embeddings_safe_term).item()\n",
        "        print(f\"📌 Semantic Similarity Check: {text} vs {safe_term} -> Score: {similarity_score}\")\n",
        "        if similarity_score > threshold:\n",
        "            print(f\"✅ Semantic Context Safe: Similar to {safe_term}\")\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "# Compliance Check Function\n",
        "def compliance_check(text: str) -> Dict[str, any]:\n",
        "    \"\"\"Runs a multi-layered compliance check on the given text.\"\"\"\n",
        "    print(f\"\\n🔹 Checking Compliance for: {text}\\n\")\n",
        "\n",
        "    result = {\n",
        "        \"text\": text,\n",
        "        \"rule_based_flags\": [],\n",
        "        \"ml_classification\": \"Safe\",\n",
        "        \"context_safe\": False,\n",
        "        \"semantic_safe\": False,\n",
        "        \"final_decision\": \"Safe\",\n",
        "        \"explanation\": \"\"\n",
        "    }\n",
        "\n",
        "    # Apply Rule-Based Filtering\n",
        "    result[\"rule_based_flags\"] = apply_rule_based_filter(text)\n",
        "\n",
        "    # Apply Transformer Model\n",
        "    if len(result[\"rule_based_flags\"]) > 0:\n",
        "        result[\"ml_classification\"] = classify_with_transformer(text)\n",
        "\n",
        "    # Extract Key Terms\n",
        "    key_terms = extract_key_terms(text)\n",
        "\n",
        "    # Check Context with NER\n",
        "    result[\"context_safe\"] = check_context(text)\n",
        "\n",
        "    # Check Semantic Similarity\n",
        "    result[\"semantic_safe\"] = is_contextually_safe(text)\n",
        "\n",
        "    # Final Decision\n",
        "    if result[\"ml_classification\"] != \"Safe\" or len(result[\"rule_based_flags\"]) > 0:\n",
        "        if result[\"context_safe\"] or result[\"semantic_safe\"]:\n",
        "            result[\"final_decision\"] = \"Safe\"\n",
        "            result[\"explanation\"] = \"Content contains flagged words but appears in a non-violating context.\"\n",
        "        else:\n",
        "            result[\"final_decision\"] = \"Violation\"\n",
        "            result[\"explanation\"] = f\"Content classified as: {result['rule_based_flags']}.\"\n",
        "\n",
        "    print(f\"🔹 Final Decision: {result['final_decision']} - {result['explanation']}\\n\")\n",
        "    return result\n",
        "\n",
        "# Test Cases\n",
        "test_cases = [\n",
        "    \"In the image, a man and a woman are lying on a bed and engaging in a lustful act. The woman is straddling the man while getting intimate with him. The scene is quite provocative, as both individuals appear to be enjoying their time together. The woman appears to be fully naked, and the man's intentions are clear as they both participate in their erotic pursuits.\",\n",
        "    \"Three bottles of Naked beverages are featured in the image.\",\n",
        "    \"The image shows three bottles of a green beverage - Naked Half Naked - in three different colors.\",\n",
        "    \"I wanna fuck you up dog ass hole mother fuckr bitch I will fuck you up.\"\n",
        "]\n",
        "\n",
        "# Run Compliance Checks\n",
        "results = [compliance_check(text) for text in test_cases]\n",
        "\n",
        "# Display Results\n",
        "df = pd.DataFrame(results)\n",
        "import ace_tools_open as tools\n",
        "tools.display_dataframe_to_user(name=\"Debugging Compliance Model\", dataframe=df)\n",
        "\n",
        "print(\"\\n✅ DEBUGGING TRACE COMPLETED\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0161fe9b467641b79bc687b6d69c2712": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "02335270363a449b9ece2551b55d20ce": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05a55ea7263c4893970edff85857674e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "06093206fd8d4ef58c1e8f3741caf1ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "06cd2cef14574ba191902916806d1595": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0768b0d7dd864bfabe5f12418dfcfe76": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ffa0fb1b67e3465884ad3f60f18ac13f",
              "IPY_MODEL_ef3cc101cde64d5792edff060d849558",
              "IPY_MODEL_e35686d576ff451ab13c236df9e2ba97"
            ],
            "layout": "IPY_MODEL_384e1eaa7eac487f8c15a79137cebe54"
          }
        },
        "097ae431e07946b98dce00faffb3ec9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "098653a393934263b5b390675488f0bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0995c0c4a711465d8dcbaaa62642301a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0ab20ec7939342dea7ab3a9bd06a9065": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0af793b3f0d14fdc840f54d2c5369bed": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b8b021ad192444f9466713244ebe47f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0c5471ba94334c849cce149045512b85": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0cd8ade3c3f745768ebb06fcb4198d95": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1020ba121d494d98b946de1abf6d2e7d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "105e12c3acc4499a86f059ef0e48b979": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5a15e5f700e74803a6b9a6e033aa2107",
              "IPY_MODEL_76edb62358f943b8936a59d110d1dad4",
              "IPY_MODEL_4461995efbee492ca3b14379119427c4"
            ],
            "layout": "IPY_MODEL_98b9e4aa78964793abbd93b6f386d18e"
          }
        },
        "10977be18b3144e7a98f741c78275399": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "10e6b9f415bb48809b0c5a98dd96927b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "115036c2e2614fcdaebda2b78a988740": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11b265e2d9c54213b10848f90a74a320": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf9748fd88514783b593ddb661b5e746",
            "placeholder": "​",
            "style": "IPY_MODEL_7b09b70ecb854904856742c0182d31be",
            "value": " 6/6 [00:10&lt;00:00,  1.56s/it]"
          }
        },
        "12428d31f9484a5ba2e8887a07434cea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fbc821514a18484591c601ed18678552",
            "placeholder": "​",
            "style": "IPY_MODEL_36226dae30b545d8b9c7b6d38f8d85f2",
            "value": " 2/2 [00:03&lt;00:00,  1.85s/it]"
          }
        },
        "124acec5653c40e89b4687bc4d6caa5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd479b74ae6e4bcd8cdd2e3cd83be7f9",
            "placeholder": "​",
            "style": "IPY_MODEL_b02a1abae8cf44b6ac907d2bed3d468c",
            "value": " 6/6 [00:10&lt;00:00,  1.64s/it]"
          }
        },
        "12bab7724d5a49868571c1572e0fa139": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "133b8af5f5944565a5db3c68c302d806": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3afd4668b204263a8ecbcef303206a3",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b46afe5aeab64e7b898766268f1bcaa9",
            "value": 6
          }
        },
        "13434db54bd04e208bd9ef033446d1e5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "137b193e1be646a0a26491dc0984e38b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "154e97b19e7c4cc2971157c239ebce20": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "164dde685a3848999d25281206d1f10b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "171ada5217eb4514b4f7b6342a06ebae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "171e14b083664289b68d58ebcadad314": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b6d5be66dc24be69195579a35c1ee61",
            "max": 77152,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f84166ee9d6b476a830081a41baf7e57",
            "value": 77152
          }
        },
        "171e1ab56acb441cbde5ec643faf65a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "174f64acaeb74877b85527ab9ac60261": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18460aec28144a05a70456ba583218c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "19078e6a65a2444a9b7d1f6bd3e65275": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "19d773f9d9064e839988f5d0cc914437": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1abef045e39b446eae3d8d680b8b992d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1b053694608b467882f797ce1332da34": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_48a12a36c77b47b3a962fb7e3f0eede5",
              "IPY_MODEL_db4afb94aa304bf197ceb9518fc175e5",
              "IPY_MODEL_124acec5653c40e89b4687bc4d6caa5f"
            ],
            "layout": "IPY_MODEL_af421f24a5a84bb082a3f2e2d02d91b6"
          }
        },
        "1bb4e74b6c1d4184b4a5d7722842e43d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f0ddffd4b85454484649c10122d0f87",
            "placeholder": "​",
            "style": "IPY_MODEL_29178358a54e4b8fbe0638066e464be9",
            "value": "model-00001-of-00006.safetensors: 100%"
          }
        },
        "1be5ea723a3f415eb6f01976a27203a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1c6a29bd1d0e4217b3f6dcaaca9fa300": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d360b6ed5394177bfd89c7805e1a9d3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d7b384d43784b67b42b2f9bb91b0180": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1de32592a96d439093c5a6bd8ff5b777": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e468072ae1a4a5c85fa4f0f5d31d792": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f50f4c52e5543b7bb4b194e01abee9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b9643096e24497190978176855df1e8",
            "placeholder": "​",
            "style": "IPY_MODEL_8a281f9cc30646839f7c5d9a9fd834e9",
            "value": " 500k/500k [00:00&lt;00:00, 9.72MB/s]"
          }
        },
        "1f999bfdc60c489a9d38736ee0ddd2ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2046809f60e24821afcc8cee6d1db569": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2064462ac5334ae28dc3c438072aee37": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20cf22134a8f482a9e72fc5f4c02826b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "224695c1308b421dad6890cf4126daae": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "228829dfb52d43edbcaf2f5332b58f42": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "22f33450b0ba4f5c8a0e90594dfcb3bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1465600f8934ed9878971e613652c9e",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_05a55ea7263c4893970edff85857674e",
            "value": 2
          }
        },
        "252df2764bbc495a966653aa1adb77e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c44c96c00c774e3f83ddee2e8a7a8122",
              "IPY_MODEL_6547a69158cb499ca2eb6fdb55fef93d",
              "IPY_MODEL_3dc11154b8974cc5a820166855194b6c"
            ],
            "layout": "IPY_MODEL_4900901ca8754f7bb5e6c9ee759eaa0e"
          }
        },
        "263cded35c0f40889b72fe2132e464b0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "266e6b9d6ffd429ba4562d1d6b6fb36b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f8328d54e6b4556b8187220961ae740",
            "placeholder": "​",
            "style": "IPY_MODEL_91fd22599ef6497584d144f032eb77b9",
            "value": " 4.93G/4.93G [00:31&lt;00:00, 64.3MB/s]"
          }
        },
        "27dc915d53764745ae8550691a231b9c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28359b6502c842b88a2c971195f4987e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28c382b065bd4438aa5c177f4fd63ab3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29178358a54e4b8fbe0638066e464be9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "298dbd52ae6441749cc576189dc57dcc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2bab671dbdb24a8e81c7f15a9e603f11": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e83a991a043478ca5879ec3a6ff1fe3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2f02d8fabe5845119148e48e3af3cdaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2f19f4f8835a4b3b98c546875d1b6d39": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac21d37ec70548e385d1b6466e532dee",
            "placeholder": "​",
            "style": "IPY_MODEL_7237fb6d1ff049fca43045a0a6b3c496",
            "value": "Loading checkpoint shards:  50%"
          }
        },
        "2f521dad8eb54b0aa264437462a1dfd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40e8147696954198b1f2ca20d9910cb1",
            "placeholder": "​",
            "style": "IPY_MODEL_1c6a29bd1d0e4217b3f6dcaaca9fa300",
            "value": "Downloading shards: 100%"
          }
        },
        "2f731602df604a4e8327fac2c395af99": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1bb4e74b6c1d4184b4a5d7722842e43d",
              "IPY_MODEL_9a88ce7f2ac44bffa6a938b8ba60c863",
              "IPY_MODEL_59458c7149844cada0be93f978269a88"
            ],
            "layout": "IPY_MODEL_5e2d96d9393a4ba08af4f7118b2fb600"
          }
        },
        "3020ee49c50f47109994c58631a3ee44": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30a441bee81f4073936fe007eac09590": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3228561877b04c45ab7338abd0d02f13": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "32a95e1db2464dbda32116ad5879e752": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "351a8622db5a40be9b01e1676f133405": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36226dae30b545d8b9c7b6d38f8d85f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "36d592a3757947b7bf739d13a2408d14": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36f6581c14914c5faf232ecbbf8c71a2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "372654e39cf54716844fb0d8c8fc9615": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "374236e0ef7d4e9888ed3e60972fded7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37b1810d57464a9c8cda4c19c1d7fe22": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf32d9862cda431dbb5c7396a47daf3b",
            "placeholder": "​",
            "style": "IPY_MODEL_10977be18b3144e7a98f741c78275399",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "384e1eaa7eac487f8c15a79137cebe54": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3856a259d0a541b8bf131a81a455bc6a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38a3a87ec35747c4b724940d237b7e64": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "38fa4e98368948d793adb8f414c128de": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1c58f776af54a3d898130e9cd822747",
            "placeholder": "​",
            "style": "IPY_MODEL_618151761499454490b916a0548b2efd",
            "value": "tokenizer.model: 100%"
          }
        },
        "3960a7c4ad4d4af0b818ef1de7dc24ff": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39b88f4102ba4222a3afd572629746e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "39d8085120634454b8ac243c557df520": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a25e247fc9b4c5f9ddac407e265c4be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d02da867109d48b998b92fdb4a1f9c99",
            "placeholder": "​",
            "style": "IPY_MODEL_64ff73504e714597b5d115cea8ae43e8",
            "value": "model-00006-of-00006.safetensors: 100%"
          }
        },
        "3a9d3bbb2eba47949fcbf3b112eb2c2a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b8cc33626434244971000388c7ad476": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b9643096e24497190978176855df1e8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b9d5757879442829bdbf8c0c56f1f7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd2fa6d29b2940e2b370520b24821f6f",
            "placeholder": "​",
            "style": "IPY_MODEL_18460aec28144a05a70456ba583218c1",
            "value": " 3.62M/3.62M [00:00&lt;00:00, 29.9MB/s]"
          }
        },
        "3ce27b9d396b44108550c798c1fe443e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3f34d1e53ca24de0b3588220a1c494cb",
              "IPY_MODEL_7775eea944c4417684d63e307b1620ab",
              "IPY_MODEL_6b5ddce7563f41f0ae45c24b6c23d4e5"
            ],
            "layout": "IPY_MODEL_6a0ac49b9f2e41a6ab7cdcc479df675b"
          }
        },
        "3dc11154b8974cc5a820166855194b6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0af793b3f0d14fdc840f54d2c5369bed",
            "placeholder": "​",
            "style": "IPY_MODEL_c10f81270c4c4aa094e788dbae75a8d5",
            "value": " 2/2 [00:03&lt;00:00,  1.75s/it]"
          }
        },
        "3de4e73be3cd40449287d401b7ae86ac": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f34d1e53ca24de0b3588220a1c494cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6966f921e1a43a7b304368836e6a8b8",
            "placeholder": "​",
            "style": "IPY_MODEL_30a441bee81f4073936fe007eac09590",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "40e8147696954198b1f2ca20d9910cb1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "413edeff0876446d9d0e6de4c5d4286f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "424ac53f6d194a4fb6eecfd4ae5e465e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d8369888ab44410889405511abc8dfc5",
              "IPY_MODEL_e8ee0c2c8ee2444a993b933f81f70b10",
              "IPY_MODEL_c11aee1f2e7b42ae98dedc47e84555bd"
            ],
            "layout": "IPY_MODEL_477ecf9b4f0e46ba9b7023c3457b2fa1"
          }
        },
        "437298397b7548af9217f1542a133bda": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b119b8e7f0db419a8e575352be2ce723",
              "IPY_MODEL_bfaf3fda70fb429f9d370def1373c1e0",
              "IPY_MODEL_ea0869a5b58a4bd4b9a7209ca008b369"
            ],
            "layout": "IPY_MODEL_d0541a4917994a8383a36eed3b1530c2"
          }
        },
        "4461995efbee492ca3b14379119427c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b91d6730f3924be6a4b9bfe34a099457",
            "placeholder": "​",
            "style": "IPY_MODEL_be5c6055ff8c4b06abe7c301701a2d49",
            "value": " 141/141 [00:00&lt;00:00, 18.8kB/s]"
          }
        },
        "44b56caf7a844daf97552f4bff8183ac": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4518b7a3d6c547dfa528a185699fa7f7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4538d6e6940f4ff8acbff03286bf503c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2bf1bb4b0054e00b384f04fbdbb86b3",
            "placeholder": "​",
            "style": "IPY_MODEL_bd9c45911bec4055849aef7eecc4d6cf",
            "value": " 6/6 [02:35&lt;00:00, 23.29s/it]"
          }
        },
        "477bd4ffe2384ca78900ac18a7e07c66": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "477ecf9b4f0e46ba9b7023c3457b2fa1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48a12a36c77b47b3a962fb7e3f0eede5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94a010bf063146b3b11e1f03daab5d70",
            "placeholder": "​",
            "style": "IPY_MODEL_f02e47abf8514c6781562c6e1c335525",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "4900901ca8754f7bb5e6c9ee759eaa0e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49a7afdb02e848de925196923ee52b1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_92ba8644980940b89ad8894fdf2488ca",
            "placeholder": "​",
            "style": "IPY_MODEL_f2d0797d50c64308bf2abd67b4bdf2a1",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "4a6e400c58244f6c8021742bb8d00543": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4db2b32991be4bdd81f9d321d94dd168": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44b56caf7a844daf97552f4bff8183ac",
            "placeholder": "​",
            "style": "IPY_MODEL_f59d60f824c04855860b94fae3f63986",
            "value": " 6/6 [00:03&lt;00:00,  2.29it/s]"
          }
        },
        "4e3406e1984744a19a14896042e1240a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e8b59feb2ac4be2884f409f4c57cff6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3856a259d0a541b8bf131a81a455bc6a",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d5a00820206348cba3a2dde939ed7823",
            "value": 6
          }
        },
        "4ebfc388ebce4fd4af62eb43d5b29853": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f0ddffd4b85454484649c10122d0f87": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ffb79decbb1495e8b3170997535371f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_477bd4ffe2384ca78900ac18a7e07c66",
            "placeholder": "​",
            "style": "IPY_MODEL_413edeff0876446d9d0e6de4c5d4286f",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "50d9a1a00a9c465c945c3c1c928f9771": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51b106e8bc18444fbdb84324cfad8c25": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5db57d036c8a483b8f221da813dc1a92",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7039731f6b8d4caa85367d24f0ae69c4",
            "value": 6
          }
        },
        "52c7323d1c6f44e7972805133e8ca134": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5436c8e4b1a2486dbdc58ff3a7ebeb13": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "548b13ed359745cfb6b9247189531c2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_68f6409ae0784c05850737623a616688",
              "IPY_MODEL_781e41864d484d999cd5e4ab5a54ac58",
              "IPY_MODEL_e2ce474b812f460f9992bc1956e9c150"
            ],
            "layout": "IPY_MODEL_36f6581c14914c5faf232ecbbf8c71a2"
          }
        },
        "5602428751f34d5ea08d167c05a7b360": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56c2a436293a4a8cb8b62789f4c7cf09": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7255b2fd13984d7b81bad09a05daa1ec",
              "IPY_MODEL_ab59bc7059af4075b94f2fac13ff33bb",
              "IPY_MODEL_b36670b48c51417794c21ed195dd1c3b"
            ],
            "layout": "IPY_MODEL_115036c2e2614fcdaebda2b78a988740"
          }
        },
        "57867682b2de4d3c93ed009fd6bedb20": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "58b610bec47743a5b495b87f30a7bd55": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59458c7149844cada0be93f978269a88": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58b610bec47743a5b495b87f30a7bd55",
            "placeholder": "​",
            "style": "IPY_MODEL_0cd8ade3c3f745768ebb06fcb4198d95",
            "value": " 4.96G/4.96G [00:26&lt;00:00, 237MB/s]"
          }
        },
        "5a15e5f700e74803a6b9a6e033aa2107": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_298dbd52ae6441749cc576189dc57dcc",
            "placeholder": "​",
            "style": "IPY_MODEL_12bab7724d5a49868571c1572e0fa139",
            "value": "generation_config.json: 100%"
          }
        },
        "5a7f767393a9427d93486dd8356e8106": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ae22023fe3640579e37146171d9a36d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06cd2cef14574ba191902916806d1595",
            "placeholder": "​",
            "style": "IPY_MODEL_39b88f4102ba4222a3afd572629746e0",
            "value": " 701/701 [00:00&lt;00:00, 89.9kB/s]"
          }
        },
        "5b9e2a7d143a4d39980327fc9269b81d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28c382b065bd4438aa5c177f4fd63ab3",
            "placeholder": "​",
            "style": "IPY_MODEL_f02bd61eb9614995a59dbf58f23221b1",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "5c5af41a75e34eb28cbf82f3f846c5f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36d592a3757947b7bf739d13a2408d14",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ebcae4853fd6479f99a1fef118689872",
            "value": 6
          }
        },
        "5d05f370e24d40a8b0c76275ce349ff7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5db57d036c8a483b8f221da813dc1a92": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e2d96d9393a4ba08af4f7118b2fb600": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ee1eda0eff54491a024a5fcea46c87b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f4f5122ace5499f81eae788ba274d5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4518b7a3d6c547dfa528a185699fa7f7",
            "placeholder": "​",
            "style": "IPY_MODEL_1abef045e39b446eae3d8d680b8b992d",
            "value": " 173/173 [00:00&lt;00:00, 20.6kB/s]"
          }
        },
        "5f8328d54e6b4556b8187220961ae740": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "607e1cccc37844b98af4de125c2b14a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_38fa4e98368948d793adb8f414c128de",
              "IPY_MODEL_747f5b540cab4706b7e1ad22fc78fafd",
              "IPY_MODEL_1f50f4c52e5543b7bb4b194e01abee9a"
            ],
            "layout": "IPY_MODEL_02335270363a449b9ece2551b55d20ce"
          }
        },
        "60d6da517e0b42938ba953e630d7362c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27dc915d53764745ae8550691a231b9c",
            "max": 701,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8f4ee815d68e407f9c68e5b76fc6c5c5",
            "value": 701
          }
        },
        "618151761499454490b916a0548b2efd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "623a38668f574f83953fde06cf584508": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6321dbf9e1f241289574079c489975f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a67f21f5806d4b69925acb8f7e70a95a",
              "IPY_MODEL_60d6da517e0b42938ba953e630d7362c",
              "IPY_MODEL_5ae22023fe3640579e37146171d9a36d"
            ],
            "layout": "IPY_MODEL_5602428751f34d5ea08d167c05a7b360"
          }
        },
        "64ff73504e714597b5d115cea8ae43e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6547a69158cb499ca2eb6fdb55fef93d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70154e80a1b44bec8ffdc5f461ca2692",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_171e1ab56acb441cbde5ec643faf65a5",
            "value": 2
          }
        },
        "686b427a930042819fa618ad27bcfdba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0d5b29cdc4f411db2401e5860cc6a56",
            "max": 4933723208,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b90279bc775445e8b1f69e94eeb3a710",
            "value": 4933723208
          }
        },
        "689ffbb060da46369dac1f7445738f04": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "68b05e1dfd37431cb02af1681d16e4fd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68f6409ae0784c05850737623a616688": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a14f1a7126a427cbc33659f32ce0a59",
            "placeholder": "​",
            "style": "IPY_MODEL_7cf6a50961cf477bbd17b6922278dc4d",
            "value": "config.json: 100%"
          }
        },
        "691c226205e34a0397f0ae51ed5f8358": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "698ca72845e247269a3e28f2d37fcdb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8e9b56e3dfeb4b78a78e9a2d47b4fb05",
              "IPY_MODEL_b9b752b3e1284cdea140a891ca9adbe7",
              "IPY_MODEL_880572259c5844b697a5018072d17fcc"
            ],
            "layout": "IPY_MODEL_9b5a9169572e4a92beb20e2042f1f143"
          }
        },
        "6a0ac49b9f2e41a6ab7cdcc479df675b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b5ddce7563f41f0ae45c24b6c23d4e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68b05e1dfd37431cb02af1681d16e4fd",
            "placeholder": "​",
            "style": "IPY_MODEL_0ab20ec7939342dea7ab3a9bd06a9065",
            "value": " 1.45k/1.45k [00:00&lt;00:00, 187kB/s]"
          }
        },
        "6b6d5be66dc24be69195579a35c1ee61": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d64747b6cf94a6781dde91b87f593a4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6dc913be8ca24d0c917c480bf9b59d28": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_37b1810d57464a9c8cda4c19c1d7fe22",
              "IPY_MODEL_4e8b59feb2ac4be2884f409f4c57cff6",
              "IPY_MODEL_11b265e2d9c54213b10848f90a74a320"
            ],
            "layout": "IPY_MODEL_dd1355db04cd4c20b511a87e118afc6c"
          }
        },
        "70154e80a1b44bec8ffdc5f461ca2692": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7039731f6b8d4caa85367d24f0ae69c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "703a9e14e32644278e1ed3b85728facb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50d9a1a00a9c465c945c3c1c928f9771",
            "placeholder": "​",
            "style": "IPY_MODEL_3228561877b04c45ab7338abd0d02f13",
            "value": " 4.88G/4.88G [00:32&lt;00:00, 84.8MB/s]"
          }
        },
        "70ba70053bb54a54912dfc38c29d2ee6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "71b11af316b34a5eb79e6f5efc15dd05": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5b9e2a7d143a4d39980327fc9269b81d",
              "IPY_MODEL_5c5af41a75e34eb28cbf82f3f846c5f2",
              "IPY_MODEL_4db2b32991be4bdd81f9d321d94dd168"
            ],
            "layout": "IPY_MODEL_d648b0e9bbd041e4bacd1c108e4f4885"
          }
        },
        "7237fb6d1ff049fca43045a0a6b3c496": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7255b2fd13984d7b81bad09a05daa1ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f12b26a1a464d9a8e3151627d0921d8",
            "placeholder": "​",
            "style": "IPY_MODEL_d732d195663347818588b05febebc3a3",
            "value": "model-00002-of-00006.safetensors: 100%"
          }
        },
        "7381515a531d4cf78f5c5ecc80a2ca94": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1020ba121d494d98b946de1abf6d2e7d",
            "placeholder": "​",
            "style": "IPY_MODEL_91faf571a67447679f56043e6a353ee6",
            "value": " 4.93G/4.93G [00:27&lt;00:00, 111MB/s]"
          }
        },
        "73e2870e3515492682471084f0696b58": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_137b193e1be646a0a26491dc0984e38b",
            "placeholder": "​",
            "style": "IPY_MODEL_691c226205e34a0397f0ae51ed5f8358",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "747f5b540cab4706b7e1ad22fc78fafd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39d8085120634454b8ac243c557df520",
            "max": 499723,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_19078e6a65a2444a9b7d1f6bd3e65275",
            "value": 499723
          }
        },
        "74c89dbfa8254809be3ae0119ee4dad7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76edb62358f943b8936a59d110d1dad4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b8cc33626434244971000388c7ad476",
            "max": 141,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d3819e70fcf2496b8357fdb08da0ebf1",
            "value": 141
          }
        },
        "7775eea944c4417684d63e307b1620ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e11ab1eab314fe0bc122f78bf68ff6d",
            "max": 1451,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b9744ff5ef994155ad950bff292e6706",
            "value": 1451
          }
        },
        "781e41864d484d999cd5e4ab5a54ac58": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e3406e1984744a19a14896042e1240a",
            "max": 1103,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0b8b021ad192444f9466713244ebe47f",
            "value": 1103
          }
        },
        "792af95f97394065a2131f94058a7532": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7a14f1a7126a427cbc33659f32ce0a59": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a4fac2a7da14b73b7c6c01ab5c885e0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ace79cfc887468eae93f5892f8c1998": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b09b70ecb854904856742c0182d31be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7cf6a50961cf477bbd17b6922278dc4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7f6e1b4b30424f9dbdab6d1e3693fdb3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "817af4ee28114961b8deec547b9d224a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8cc8105b95544171ac67f752a42dcaaa",
              "IPY_MODEL_133b8af5f5944565a5db3c68c302d806",
              "IPY_MODEL_d2c2b1cfdd664e299e631cd29ade0104"
            ],
            "layout": "IPY_MODEL_c7c2d2b92ddc4d218989ae3d78877fd2"
          }
        },
        "81fe15871bce4f23aadbeed7a2d52c71": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8279bbe0adab413b95fe5541259afdc3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8284ef46f1f24a16bdc9c54af0d84900": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82b545728c944f519ad1e12134426771": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "82b7050c4fe349e6a6fc49a0608d4ec8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c3507c5985454c36aa4f83e2a661fb16",
              "IPY_MODEL_d54ea90f246f4ef5a54c7e28a968e274",
              "IPY_MODEL_703a9e14e32644278e1ed3b85728facb"
            ],
            "layout": "IPY_MODEL_3020ee49c50f47109994c58631a3ee44"
          }
        },
        "83fc886a6af7492e8cd5b71c2c58f4c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8587a2643f3e40049f691f23482239b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "874555ed1d3c478ca521608751492af8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8764d39c88494cd89f20d207f79701fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cfb58c0dd80048f3ae641962af17a56c",
              "IPY_MODEL_b66f5ce342ae4c0b8a6a7deb6c2ba912",
              "IPY_MODEL_266e6b9d6ffd429ba4562d1d6b6fb36b"
            ],
            "layout": "IPY_MODEL_87bd17d77a1d4caeb97b70234dbd7248"
          }
        },
        "87bd17d77a1d4caeb97b70234dbd7248": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "880572259c5844b697a5018072d17fcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f6e1b4b30424f9dbdab6d1e3693fdb3",
            "placeholder": "​",
            "style": "IPY_MODEL_098653a393934263b5b390675488f0bf",
            "value": " 505/505 [00:00&lt;00:00, 71.2kB/s]"
          }
        },
        "89e902f1912840389a1aa259c1c84252": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8a281f9cc30646839f7c5d9a9fd834e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8abbedc4dc0e4deab08b414ba9210266": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b7bc7aec1364625814ef5946683ffce": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b956e7a497644d294d246530db33a91": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_154e97b19e7c4cc2971157c239ebce20",
            "placeholder": "​",
            "style": "IPY_MODEL_d4db570bdf614c2aa85786674a97db37",
            "value": " 6/6 [00:03&lt;00:00,  2.27it/s]"
          }
        },
        "8c00d5de93bc40d7811bce3ad85df4de": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c86469fb635495bb3d157dfb67616e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_374236e0ef7d4e9888ed3e60972fded7",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c30f654164c0437f993d7ea2bbaa77fa",
            "value": 3
          }
        },
        "8cc8105b95544171ac67f752a42dcaaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e312eb0a596749fbb9768a58c1d834b7",
            "placeholder": "​",
            "style": "IPY_MODEL_1be5ea723a3f415eb6f01976a27203a7",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "8e9b56e3dfeb4b78a78e9a2d47b4fb05": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_943a02fa93e34fefab30f859dcef7e7d",
            "placeholder": "​",
            "style": "IPY_MODEL_689ffbb060da46369dac1f7445738f04",
            "value": "preprocessor_config.json: 100%"
          }
        },
        "8f4ee815d68e407f9c68e5b76fc6c5c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "91faf571a67447679f56043e6a353ee6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "91fd22599ef6497584d144f032eb77b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "92ba8644980940b89ad8894fdf2488ca": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "943a02fa93e34fefab30f859dcef7e7d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94a010bf063146b3b11e1f03daab5d70": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9553e75ebc83484ebb8dd651864c19c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_164dde685a3848999d25281206d1f10b",
            "max": 2021860512,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2e83a991a043478ca5879ec3a6ff1fe3",
            "value": 2021860512
          }
        },
        "95fad93ab0a0484a9c223664f2deb1a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3de4e73be3cd40449287d401b7ae86ac",
            "placeholder": "​",
            "style": "IPY_MODEL_5d05f370e24d40a8b0c76275ce349ff7",
            "value": " 41.0/41.0 [00:00&lt;00:00, 5.07kB/s]"
          }
        },
        "97c2a58c1e0744e19b9b15d080f1d4e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f02cb2da4d07442babb25ac66930c200",
              "IPY_MODEL_cc86e318f3bb40a8a34164dbaf88aeef",
              "IPY_MODEL_95fad93ab0a0484a9c223664f2deb1a8"
            ],
            "layout": "IPY_MODEL_4ebfc388ebce4fd4af62eb43d5b29853"
          }
        },
        "98b9e4aa78964793abbd93b6f386d18e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a88ce7f2ac44bffa6a938b8ba60c863": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32a95e1db2464dbda32116ad5879e752",
            "max": 4962087016,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_874555ed1d3c478ca521608751492af8",
            "value": 4962087016
          }
        },
        "9b5a9169572e4a92beb20e2042f1f143": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e11ab1eab314fe0bc122f78bf68ff6d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f12b26a1a464d9a8e3151627d0921d8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0f822d36fcb42018165de4710bbf672": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2f521dad8eb54b0aa264437462a1dfd7",
              "IPY_MODEL_a2a391aafcd9468ba69bf9d49600dd4e",
              "IPY_MODEL_4538d6e6940f4ff8acbff03286bf503c"
            ],
            "layout": "IPY_MODEL_be0118525d1544b6a4a135d0ce101369"
          }
        },
        "a1465600f8934ed9878971e613652c9e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1c58f776af54a3d898130e9cd822747": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2a391aafcd9468ba69bf9d49600dd4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c5471ba94334c849cce149045512b85",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4a6e400c58244f6c8021742bb8d00543",
            "value": 6
          }
        },
        "a46baa939e8d42bbbfa3db1771f90bc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a52e2c917b584bd6a128173f97e6f298": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a98cdad10af043788497e0fb560366b8",
            "placeholder": "​",
            "style": "IPY_MODEL_372654e39cf54716844fb0d8c8fc9615",
            "value": "model.safetensors.index.json: 100%"
          }
        },
        "a53089b534e244c2aa2af895557eea12": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dd40dd4193d249698c0dd69276cbee4c",
              "IPY_MODEL_deb23f0814e146388128ecea0b5beda2",
              "IPY_MODEL_3b9d5757879442829bdbf8c0c56f1f7b"
            ],
            "layout": "IPY_MODEL_28359b6502c842b88a2c971195f4987e"
          }
        },
        "a67f21f5806d4b69925acb8f7e70a95a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6618ebb686b43ecaf8e62ec27ca928e",
            "placeholder": "​",
            "style": "IPY_MODEL_0995c0c4a711465d8dcbaaa62642301a",
            "value": "chat_template.json: 100%"
          }
        },
        "a97d86e8dc3c47d2b53a046e26abdfc6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a98cdad10af043788497e0fb560366b8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab17979989a441959bb43a7ca0b048df": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab29260df001471d881a4e2752f5d0de": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ab59bc7059af4075b94f2fac13ff33bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a97d86e8dc3c47d2b53a046e26abdfc6",
            "max": 4970423200,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_228829dfb52d43edbcaf2f5332b58f42",
            "value": 4970423200
          }
        },
        "ac21d37ec70548e385d1b6466e532dee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad1bd020b7fe44a1a64c48dd8109eee4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c00d5de93bc40d7811bce3ad85df4de",
            "placeholder": "​",
            "style": "IPY_MODEL_792af95f97394065a2131f94058a7532",
            "value": " 6/6 [00:04&lt;00:00,  1.78it/s]"
          }
        },
        "ad4d9413e1d746c69722b1ea42d709d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c237d95d55864f84b574d8a2cb59268d",
            "placeholder": "​",
            "style": "IPY_MODEL_2046809f60e24821afcc8cee6d1db569",
            "value": " 2.02G/2.02G [00:12&lt;00:00, 84.3MB/s]"
          }
        },
        "af421f24a5a84bb082a3f2e2d02d91b6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b02a1abae8cf44b6ac907d2bed3d468c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b0f8aef051fe490c86c1ffc920bbf4a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3a25e247fc9b4c5f9ddac407e265c4be",
              "IPY_MODEL_9553e75ebc83484ebb8dd651864c19c1",
              "IPY_MODEL_ad4d9413e1d746c69722b1ea42d709d8"
            ],
            "layout": "IPY_MODEL_ef44f44033324c1ea6699d90a9f4df9d"
          }
        },
        "b119b8e7f0db419a8e575352be2ce723": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5434235bd3f4f62934c7c5396aba4d6",
            "placeholder": "​",
            "style": "IPY_MODEL_ab17979989a441959bb43a7ca0b048df",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "b36670b48c51417794c21ed195dd1c3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a7f767393a9427d93486dd8356e8106",
            "placeholder": "​",
            "style": "IPY_MODEL_a46baa939e8d42bbbfa3db1771f90bc3",
            "value": " 4.97G/4.97G [00:25&lt;00:00, 225MB/s]"
          }
        },
        "b3afd4668b204263a8ecbcef303206a3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b46afe5aeab64e7b898766268f1bcaa9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b4de01d0890845f9992dc90ebd3b410d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b66f5ce342ae4c0b8a6a7deb6c2ba912": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_174f64acaeb74877b85527ab9ac60261",
            "max": 4933723208,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_82b545728c944f519ad1e12134426771",
            "value": 4933723208
          }
        },
        "b6f402ff54d640ccbb9672e66d9157c9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b90279bc775445e8b1f69e94eeb3a710": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b91d6730f3924be6a4b9bfe34a099457": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9744ff5ef994155ad950bff292e6706": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b9adcc96c2f04a1cbeccb079320e2abf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a52e2c917b584bd6a128173f97e6f298",
              "IPY_MODEL_171e14b083664289b68d58ebcadad314",
              "IPY_MODEL_ca7bca7de2ee4952bef8cc4b03dfa9c7"
            ],
            "layout": "IPY_MODEL_10e6b9f415bb48809b0c5a98dd96927b"
          }
        },
        "b9b752b3e1284cdea140a891ca9adbe7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_20cf22134a8f482a9e72fc5f4c02826b",
            "max": 505,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ed997990922a4e298203825ed094242b",
            "value": 505
          }
        },
        "bc3b8a41a31448a5b8cada089bbbf428": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4ffb79decbb1495e8b3170997535371f",
              "IPY_MODEL_ebe48e28147e4068813f5e88c64eeb98",
              "IPY_MODEL_8b956e7a497644d294d246530db33a91"
            ],
            "layout": "IPY_MODEL_f359454a80be4630a31f22156046037e"
          }
        },
        "bd0db7d34f334506833116528043d6a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c665d8ec1c4d490488696f2348e6faf8",
            "placeholder": "​",
            "style": "IPY_MODEL_cf91abe9cb2b482e86a2c494d08eb82d",
            "value": " 3/6 [01:02&lt;01:03, 21.24s/it]"
          }
        },
        "bd9c45911bec4055849aef7eecc4d6cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "be0118525d1544b6a4a135d0ce101369": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be5c6055ff8c4b06abe7c301701a2d49": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf32d9862cda431dbb5c7396a47daf3b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf9748fd88514783b593ddb661b5e746": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bfaf3fda70fb429f9d370def1373c1e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a4fac2a7da14b73b7c6c01ab5c885e0",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cda8508eaf994d94aefbe2fe5f211756",
            "value": 6
          }
        },
        "bfe5351acf1647b5b0e93656e318dae1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c10f81270c4c4aa094e788dbae75a8d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c11aee1f2e7b42ae98dedc47e84555bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2064462ac5334ae28dc3c438072aee37",
            "placeholder": "​",
            "style": "IPY_MODEL_19d773f9d9064e839988f5d0cc914437",
            "value": " 552/552 [00:00&lt;00:00, 72.7kB/s]"
          }
        },
        "c19bd2431f9a451fb60e9d37657928c6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c237d95d55864f84b574d8a2cb59268d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c30f654164c0437f993d7ea2bbaa77fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c3507c5985454c36aa4f83e2a661fb16": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_224695c1308b421dad6890cf4126daae",
            "placeholder": "​",
            "style": "IPY_MODEL_06093206fd8d4ef58c1e8f3741caf1ee",
            "value": "model-00003-of-00006.safetensors: 100%"
          }
        },
        "c44c96c00c774e3f83ddee2e8a7a8122": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3960a7c4ad4d4af0b818ef1de7dc24ff",
            "placeholder": "​",
            "style": "IPY_MODEL_83fc886a6af7492e8cd5b71c2c58f4c1",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "c5434235bd3f4f62934c7c5396aba4d6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c665d8ec1c4d490488696f2348e6faf8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7c2d2b92ddc4d218989ae3d78877fd2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca5024de9d8f4543a98437f4b46dde4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_49a7afdb02e848de925196923ee52b1d",
              "IPY_MODEL_22f33450b0ba4f5c8a0e90594dfcb3bc",
              "IPY_MODEL_12428d31f9484a5ba2e8887a07434cea"
            ],
            "layout": "IPY_MODEL_1e468072ae1a4a5c85fa4f0f5d31d792"
          }
        },
        "ca7bca7de2ee4952bef8cc4b03dfa9c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1de32592a96d439093c5a6bd8ff5b777",
            "placeholder": "​",
            "style": "IPY_MODEL_7ace79cfc887468eae93f5892f8c1998",
            "value": " 77.2k/77.2k [00:00&lt;00:00, 9.24MB/s]"
          }
        },
        "cbb0310da632478a9dad8cdeafe909fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2f19f4f8835a4b3b98c546875d1b6d39",
              "IPY_MODEL_8c86469fb635495bb3d157dfb67616e6",
              "IPY_MODEL_bd0db7d34f334506833116528043d6a5"
            ],
            "layout": "IPY_MODEL_1d360b6ed5394177bfd89c7805e1a9d3"
          }
        },
        "cbe15c57ea0446d38bbc33fae10264ca": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc1cc4bca9fe4646a420465acc20a6cd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc268da4acad43f6b4a02535be513add": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c19bd2431f9a451fb60e9d37657928c6",
            "max": 173,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_38a3a87ec35747c4b724940d237b7e64",
            "value": 173
          }
        },
        "cc86e318f3bb40a8a34164dbaf88aeef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff5c7ac00b734cdaa0a44cbe1acff29e",
            "max": 41,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ab29260df001471d881a4e2752f5d0de",
            "value": 41
          }
        },
        "ccd1ea6fc6d64a7fb4496369138aa6a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fbce96681f0d4e2f9076ccf85a9d97c0",
              "IPY_MODEL_686b427a930042819fa618ad27bcfdba",
              "IPY_MODEL_7381515a531d4cf78f5c5ecc80a2ca94"
            ],
            "layout": "IPY_MODEL_cc1cc4bca9fe4646a420465acc20a6cd"
          }
        },
        "cda8508eaf994d94aefbe2fe5f211756": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cf91abe9cb2b482e86a2c494d08eb82d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cfb58c0dd80048f3ae641962af17a56c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13434db54bd04e208bd9ef033446d1e5",
            "placeholder": "​",
            "style": "IPY_MODEL_bfe5351acf1647b5b0e93656e318dae1",
            "value": "model-00004-of-00006.safetensors: 100%"
          }
        },
        "d02da867109d48b998b92fdb4a1f9c99": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0541a4917994a8383a36eed3b1530c2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0d5b29cdc4f411db2401e5860cc6a56": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2c2b1cfdd664e299e631cd29ade0104": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81fe15871bce4f23aadbeed7a2d52c71",
            "placeholder": "​",
            "style": "IPY_MODEL_2f02d8fabe5845119148e48e3af3cdaa",
            "value": " 6/6 [00:10&lt;00:00,  1.64s/it]"
          }
        },
        "d2ebf8f6dcc245b7b8a5e264ce17903c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d3819e70fcf2496b8357fdb08da0ebf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d4db570bdf614c2aa85786674a97db37": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d51451fbe0914d419149dfd01298721c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ff3ce6ad11104d3b9461b4cfac7f86d9",
              "IPY_MODEL_cc268da4acad43f6b4a02535be513add",
              "IPY_MODEL_5f4f5122ace5499f81eae788ba274d5d"
            ],
            "layout": "IPY_MODEL_b6f402ff54d640ccbb9672e66d9157c9"
          }
        },
        "d54ea90f246f4ef5a54c7e28a968e274": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8abbedc4dc0e4deab08b414ba9210266",
            "max": 4881273536,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b4de01d0890845f9992dc90ebd3b410d",
            "value": 4881273536
          }
        },
        "d5a00820206348cba3a2dde939ed7823": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d648b0e9bbd041e4bacd1c108e4f4885": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6618ebb686b43ecaf8e62ec27ca928e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6966f921e1a43a7b304368836e6a8b8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d732d195663347818588b05febebc3a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d8369888ab44410889405511abc8dfc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec7aae7775b6421699fb66139433e904",
            "placeholder": "​",
            "style": "IPY_MODEL_f3dac6ff66324419a228cfea92cf9727",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "da20685aa9b24d58ad41223f45485fe0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "da9ee3c583ac49fa8b3908ed4dc67271": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_73e2870e3515492682471084f0696b58",
              "IPY_MODEL_51b106e8bc18444fbdb84324cfad8c25",
              "IPY_MODEL_ad1bd020b7fe44a1a64c48dd8109eee4"
            ],
            "layout": "IPY_MODEL_2bab671dbdb24a8e81c7f15a9e603f11"
          }
        },
        "db4afb94aa304bf197ceb9518fc175e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74c89dbfa8254809be3ae0119ee4dad7",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_097ae431e07946b98dce00faffb3ec9a",
            "value": 6
          }
        },
        "dd1355db04cd4c20b511a87e118afc6c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd40dd4193d249698c0dd69276cbee4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8279bbe0adab413b95fe5541259afdc3",
            "placeholder": "​",
            "style": "IPY_MODEL_0161fe9b467641b79bc687b6d69c2712",
            "value": "tokenizer.json: 100%"
          }
        },
        "deb23f0814e146388128ecea0b5beda2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_263cded35c0f40889b72fe2132e464b0",
            "max": 3619380,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_70ba70053bb54a54912dfc38c29d2ee6",
            "value": 3619380
          }
        },
        "e2ce474b812f460f9992bc1956e9c150": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cbe15c57ea0446d38bbc33fae10264ca",
            "placeholder": "​",
            "style": "IPY_MODEL_89e902f1912840389a1aa259c1c84252",
            "value": " 1.10k/1.10k [00:00&lt;00:00, 143kB/s]"
          }
        },
        "e312eb0a596749fbb9768a58c1d834b7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e35686d576ff451ab13c236df9e2ba97": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ee1eda0eff54491a024a5fcea46c87b",
            "placeholder": "​",
            "style": "IPY_MODEL_623a38668f574f83953fde06cf584508",
            "value": " 6/6 [02:25&lt;00:00, 21.50s/it]"
          }
        },
        "e6ba135bd754484d963f46368f9e7b6b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e76afb5ab8924765827fa8b6e7d1e389": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e8ee0c2c8ee2444a993b933f81f70b10": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b7bc7aec1364625814ef5946683ffce",
            "max": 552,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e76afb5ab8924765827fa8b6e7d1e389",
            "value": 552
          }
        },
        "ea0869a5b58a4bd4b9a7209ca008b369": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8284ef46f1f24a16bdc9c54af0d84900",
            "placeholder": "​",
            "style": "IPY_MODEL_8587a2643f3e40049f691f23482239b6",
            "value": " 6/6 [00:04&lt;00:00,  1.88it/s]"
          }
        },
        "ebcae4853fd6479f99a1fef118689872": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ebe48e28147e4068813f5e88c64eeb98": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d64747b6cf94a6781dde91b87f593a4",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1f999bfdc60c489a9d38736ee0ddd2ba",
            "value": 6
          }
        },
        "ec7aae7775b6421699fb66139433e904": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed997990922a4e298203825ed094242b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ef3cc101cde64d5792edff060d849558": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_351a8622db5a40be9b01e1676f133405",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_171ada5217eb4514b4f7b6342a06ebae",
            "value": 6
          }
        },
        "ef44f44033324c1ea6699d90a9f4df9d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f02bd61eb9614995a59dbf58f23221b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f02cb2da4d07442babb25ac66930c200": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52c7323d1c6f44e7972805133e8ca134",
            "placeholder": "​",
            "style": "IPY_MODEL_da20685aa9b24d58ad41223f45485fe0",
            "value": "added_tokens.json: 100%"
          }
        },
        "f02e47abf8514c6781562c6e1c335525": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f2bf1bb4b0054e00b384f04fbdbb86b3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2d0797d50c64308bf2abd67b4bdf2a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f359454a80be4630a31f22156046037e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3dac6ff66324419a228cfea92cf9727": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f59d60f824c04855860b94fae3f63986": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f84166ee9d6b476a830081a41baf7e57": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fbc821514a18484591c601ed18678552": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbce96681f0d4e2f9076ccf85a9d97c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a9d3bbb2eba47949fcbf3b112eb2c2a",
            "placeholder": "​",
            "style": "IPY_MODEL_5436c8e4b1a2486dbdc58ff3a7ebeb13",
            "value": "model-00005-of-00006.safetensors: 100%"
          }
        },
        "fd2fa6d29b2940e2b370520b24821f6f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd479b74ae6e4bcd8cdd2e3cd83be7f9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff3ce6ad11104d3b9461b4cfac7f86d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d7b384d43784b67b42b2f9bb91b0180",
            "placeholder": "​",
            "style": "IPY_MODEL_57867682b2de4d3c93ed009fd6bedb20",
            "value": "processor_config.json: 100%"
          }
        },
        "ff5c7ac00b734cdaa0a44cbe1acff29e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ffa0fb1b67e3465884ad3f60f18ac13f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6ba135bd754484d963f46368f9e7b6b",
            "placeholder": "​",
            "style": "IPY_MODEL_d2ebf8f6dcc245b7b8a5e264ce17903c",
            "value": "Loading checkpoint shards: 100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
